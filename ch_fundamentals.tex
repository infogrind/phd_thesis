\chapter{Fundamentals of Source-Channel Communication}\label{ch:fundamentals}

Source-channel communication under a delay constraint is a rather marginal topic
in information theory. The main reason is perhaps that two pillars of
information theory, Shannon's source coding and channel coding theorems, are
asymptotic results and the codes they use are not allowed if one is restricted
to operate on sequences of short length. 

This chapter has two goals. The first goal is to introduce the source-channel
communication problem in its generality. The second goal is to quote the
relevant previous work in order to set the stage for the chapters ahead.

\secref{setup} starts the chapter by introducing the elements that make up the
source-channel communication problem and by defining the performance criteria of
interest. The known fundamental limits for these criteria are then reviewed in
\secref{fundamentallimits}.  \secref{optimality} considers the conditions for a
given communication system to achieve the theoretical performance limits.
Furthermore, this section cites results from previous work that characterize an
optimal communication system in terms of a ``match'' between the quantities that
make up the system. Lastly, \secref{twofacets} provides a preview of the
remaining chapters and positions these chapters within the larger framework.

%If coding delay and complexity are unrestricted, one can achieve these
%conditions by performing source and channel coding separately (this is
%sometimes called \emph{tandem coding}). If the delay is restricted, however,
%the optimality of the separation-based approach is no longer guaranteed.
%\secref{delayconstraint} thus looks at the consequences of limited delay and
%shows that characterizing the theoretically achievable performance limits for
%this problem is still a largely unsolved question.  \secref{gaussian} focuses
%on the particular case of a Gaussian source and a Gaussian channel under
%squared error distortion and an average transmit power constraint. In this
%case, if the source and the channel produce and transmit, respectively, symbols
%at the same rate, then uncoded transmission (with minimal delay) performs as
%good as any code of arbitrary complexity and delay, which is explained in
%\secref{gaussuncoded}. If, on the other hand, the rate at which the channel
%accepts inputs is greater or less than the rate at which the source symbols are
%produced, then no such simple transmission scheme exists for this case, as
%\secref{gaussbwex} argues. This situation changes, however, if the transmitter
%has perfect causal feedback from the receiver. In this case, studied in detail
%in \secref{gaussfeedback}, a similarly simple scheme as that of
%\secref{gaussuncoded} is possible. In a short digression,
%\secref{gaussfeedback} shows how this scheme is not, in fact, particular to the
%Gaussian case but is of a more general nature. Lastly,
%\secref{lessonsfromfeedback} derives insight from the feedback example and
%suggests how one could use the properties of the feedback scheme even if no
%feedback is available, thus paving the way for the communication scheme to come
%in \chapref{mindelbwex}. 


\section{Problem Set-Up}\label{sec:setup}

This thesis is about point-to-point communication of a memoryless source across
a memoryless channel. In its most general form, this problem is made up of the
following six elements, displayed schematically in \figref{scgen}.
\begin{itemize}
  \item A discrete-time memoryless \emph{source} with distribution~$\pS$,
    producing a source symbol every $\ts$~seconds.
  \item A discrete-time memoryless \emph{channel} with transition
    distribution~$\pyx$, accepting an input symbol for transmission every
    $\tc$~seconds.
  \item An \emph{encoder} function~$f$ that maps a block of~$k$ source symbols
    $S^k = (S_1, \dots, S_k)$ into $n$~channel input symbols $X^n = (X_1, \dots,
    X_n)$.
  \item A \emph{decoder} function~$g$ that maps a block of $n$~channel output
    symbols $Y^n = (Y_1, \dots, Y_n)$ into $k$~source estimates $\Sh^k = (\Sh_1,
    \dots, \Sh_k)$.
  \item A \emph{cost measure} $\rho(x)$ that assigns a transmission cost to each
    channel input symbol, and a \emph{distortion measure} $d(s,\sh)$ that
    assigns a reconstruction ``badness'' to every pair of source symbol and
    estimate.
\end{itemize}
To match the number of channel inputs produced by the encoder to the
rate at which the channel accepts them, $k$ and $n$ must satisfy $n/k \le \ts /
\tc$. The communication consists thus of identical rounds of transmission of
length $k\ts$ (or $n\tc$).



\begin{figure}
  \begin{center}
    \input{figures/sc_gen.tex_t}
  \end{center}
  \caption{A general memoryless point-to-point communication system.}
  \label{fig:scgen}
\end{figure}

If the discrete-time source and channel represent an underlying continuous
bandlimited source and channel, the sampling theorem relates $\ts$ and $\tc$ to
the respective bandwidths. If $\ts = \tc$, one therefore says that the source
and channel are \emph{bandwidth matched}. Correspondingly, a code is said to be
\emph{bandwidth matched} if $k=n$, to be a \emph{bandwidth expansion} code if $k
< n$, and to be a \emph{bandwidth compression} code if $k > n$. 

The source and the channel, together with the encoder and decoder, imply a joint
distribution of the tuple $(S^k, X^n, Y^n, \Sh^k)$.  Depending on the encoder,
the sequence of channel inputs $X^n$ may not be identically distributed;
similarly, the marginal (joint) distribution of the source/estimate pairs
$(S_i,\Sh_i)$ may not be the same for all~$i$. The average cost and distortion
of a communication system are therefore defined as the empirical average over
a block of channel inputs and source symbols, respectively.

\begin{definition}
  \label{def:avgcost}
  The \emph{average channel input cost} incurred by a memoryless point-to-point
  communication system is
  \begin{equation*}
    P = \sum_{i=1}^n \E[\rho(X_i)],
  \end{equation*}
  where the expectations are taken over the marginal distributions of the~$X_i$.
\end{definition}

\begin{definition}
  \label{def:avgdist}
  The \emph{average distortion} incurred by a memoryless point-to-point
  communication system is
  \begin{equation*}
    D = \sum_{i=1}^k \E[d(S_i, \Sh_i)],
  \end{equation*}
  where the expectation is taken over the joint distribution of $S_i$
  and~$\Sh_i$.
\end{definition}

\begin{definition}
  \label{def:capacity}
  The \emph{capacity-cost function} of a channel $\pyx$ with cost
  function~$\rho(x)$ is
  \begin{equation*}
    C(P) = \max_{\px: \E[\rho(X)] \le P} I(X;Y).
  \end{equation*}
\end{definition}

\begin{definition}
  \label{def:ratedistortion}
  The \emph{rate-distortion function} of a source~$\ps$ with distortion
  measure~$d(s,\sh)$ is 
  \begin{equation*}
    R(D) = \min_{\pshs: \E[d(S,\Sh)] \le D} I(S;\Sh).
  \end{equation*}
\end{definition}


\section{Fundamental Limits of Performance}\label{sec:fundamentallimits}

The goal of source-channel communication is to transmit a source at a low cost
of transmission and with a reconstruction at the receiver that has little
distortion from the original. The region of achievable cost and distortion pairs
is thus fundamental in establishing the best possible performance of a given
communication problem.

\begin{definition}
  \label{def:achievableregion}
  For a given source~$\ps$ producing a symbol every $\ts$~seconds, a
  channel~$\pyx$ accepting an input every $\tc$~seconds, a cost
  measure~$\rho(x)$ and a distortion measure~$d(s,\sh)$, the \emph{achievable
  cost\slash distortion region} is the set of all pairs $(D,P)$ for which there
  exists  a sequence of codes that approach the distortion~$D$ and the cost~$P$
  in the limit.
\end{definition}

The most fundamental bound on the achievable cost/distortion region is given by
the following result~\cite[Theorem~21]{Shannon1948}.

\begin{theorem}
  \label{thm:separationconverse}
  In any memoryless point-to-point source-channel communication system, the
  average cost~$P$ and the average distortion~$D$ are related by
  \begin{equation}
    \label{eq:separation}
    kR(D) \le nC(P).
  \end{equation}
\end{theorem}

\begin{proof}
  See \appref{separationproof}.
\end{proof}

Because $R(D)$ is a decreasing function of~$D$ and $C(P)$ is an increasing
function of~$P$, \thmref{separationconverse} can alternatively be written as $D
\ge R^{-1}((n/k) C(P))$ or $P \ge C^{-1}((k/n) R(D))$.  It thus specifies the
smallest distortion achievable for a given cost constraint or the smallest cost
required to achieve a given distortion, respectively, and so provides an
\emph{outer bound} to the achievable cost/distortion region.  Regardless of how
powerful an encoder and decoder are, the incurred cost and distortion always lie
in the region specified by~\eqref{eq:separation}; this is illustrated in
\figref{achievableregion}. Note also that~\eqref{eq:separation} only depends on
the ratio $k/n$, so that the same bound applies to a code that encodes $1000k$
source symbols into $1000n$ channel inputs. 

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \draw[l] (0,0) -- (4,0) node[right] {$P$};
      \draw[l] (0,0) -- (0,2.5) node[above] {$D$};

      \path[clip] (0,0) rectangle (4,2.5);
      \draw[fill=black!10] (4,2.5) circle [x radius=3.5cm,y radius=2cm];

      \node[font=\scriptsize\sffamily,align=center] at (2.7,1.75) 
      {achievable\\region};
    \end{tikzpicture}
  \end{center}
  \caption{Schematic display of the achievable cost and distortion region that
  follows from \thmref{separationconverse}.}
  \label{fig:achievableregion}
\end{figure}

This theorem is often referred to as the converse part of the separation
theorem, because it establishes that a $(D,P)$ pair which cannot be achieved (or
approached) using separately performed source and channel coding cannot be
achieved at all. However, the theorem should not only be seen in relation to
separation-based coding.  In fact, while the forward part of the
separation theorem (given for reference in \appref{separationforward}) is only
valid if one allows for codes of unrestricted delay (and complexity),
\thmref{separationconverse} applies to \emph{all} codes and is thus in a way
more general than the forward part. In particular, it also applies to codes with
limited delay, which are a central subject of this thesis.


\section{Optimality Conditions}\label{sec:optimality}

For the purpose of this chapter, an optimal communication system is
defined as follows.

\begin{definition}
  \label{def:optimality}
  An \emph{optimal communication system} is a communication system whose
  average cost and distortion satisfy~\eqref{eq:separation} with
  equality.\footnote{This is not the strictest form of
  optimality, and it precludes the existence of optimal communication systems in
  some situations, for example when $\max_D R(D) < \min_P C(P)$. See
  Gastpar~\cite{GastparThesis} for a more general  definition of optimality.}
\end{definition}

By going step by step through the inequalities in the proof of
\thmref{separationconverse}, the following result can be established. 

\begin{theorem}
  \label{thm:optimalityconditions}
  A point-to-point memoryless communication system is optimal according to
  \defref{optimality} if and only if the following conditions are all satisfied.
  \begin{enumerate}
    \item For each $i = 1$, \dots, $k$, the joint distribution $p(s_i,\sh_i)$
      achieves the rate-distortion function of the source such that
      \[ R\left(\frac1k \sum_{i=1}^k \E[d(S_i,\Sh_i)] \right) = \frac1k
      \sum_{i=1}^k R\left( \E[d(S_i,\Sh_i)] \right).\]
%      at the same average
%      distortion~$D$.
    \item The \emph{reverse test channel} $p(s^k|\sh^k)$ factors as
      $\prod_{i=1}^k p(s_i | \sh_i)$. 
    \item The encoder is information lossless in the sense that it satisfies
      \[I(S^k;Y^n) = I(X^k; Y^n).\] 
    \item The estimate sequence $\Sh^k$ is a sufficient statistic for $S^k$
      given $Y^n$. (Equivalently, we can say that the decoder must be memoryless
      in the sense that $I(X^n;Y^n) = I(X^n;\Sh^k)$.)
    \item The channel output sequence $Y^n$ consists of independent random
      variables.
    \item The marginal distributions $p(x_i)$ all achieve the capacity of the
      channel such that
      \[ \frac1n \sum_{i=1}^n C(\E[\rho(X_i)]) = C\left( \frac1n \sum_{i=1}^n
      \E[\rho(X_i)] \right). \]
  \end{enumerate}
\end{theorem}

%The forward part of the separation theorem states that under the
%conditions of the source coding theorem and the channel coding theorem, there
%exist for any source, channel, distortion measure and cost measure, a sequence
%of codes that approaches $k R(D) = nC(P)$ and thus leads to an optimal
%communication system. In the limit, this sequence of codes thus fulfills all the
%conditions of \thmref{optimalityconditions}.

According to the above definition of optimality, a communication system that
uses codes based on the separation principle can only approach, but not achieve
optimality, in the sense that for any $\e > 0$ there exists a separation based
code for which $kR(D) = nC(P) - \e$, but in general not for $\e = 0$. 

Can only codes based on the separation principle achieve (or approach) any point
of the achievable cost\slash distortion region? Not at all. As the next section
shows, one can construct infinitely many examples of very simple joint
source-channel codes that incur minimal delay yet whose average cost and
distortion lie on the boundary of the achievable region characterized
by~\eqref{eq:separation} and are thus optimal. 


\subsection{Optimality through Measure-Matching}

In a bandwidth-matched system, where the number~$n$ of channel inputs per second
equals the number~$k$ of source symbols per second, a communication system is
optimal according to \defref{optimality} if $R(D) = C(P)$. Consider now a
\emph{single-letter code} $(f, g)$ that maps each source symbol~$S$ into a
single channel input~$X$ and each channel output~$Y$ into an estimate~$\Sh$;
\ie, both $f(\cdot)$ and $g(\cdot)$ are functions from~$\R$ to~$\R$. For this
case, \thmref{optimalityconditions} simplifies to the following statement.

\begin{theorem}[Single-letter optimality~\cite{GastparRV2003}]
  \label{thm:sloptimality}
  A bandwidth-matched point-to-point communication system is optimal according
  to \defref{optimality} if and only if the following conditions are all
  satisfied.
  \begin{enumerate}
    \item The joint distribution $p(s,\sh)$ achieves the rate-distortion
      function of the source at average distortion $D = \E[d(S,\Sh)]$, \ie,
      $I(S;\Sh) = R(D)$.

    \item The code is information lossless in the sense that $I(S;\Sh) =
      I(X;Y)$. 

    \item The channel input distribution $p(x)$ achieves the capacity of the
      channel at average cost~$P$, \ie, $I(X;Y) = C(P)$. 
  \end{enumerate}
\end{theorem}

\begin{proof}
  Apply \thmref{optimalityconditions} with $k = n = 1$.
\end{proof}

One way to verify condition~1 of \thmref{sloptimality} is to find the
conditional distribution $p_{\Sh|S}$ that achieves the rate-distortion function
of the source given the distortion measure~$d(s,\sh)$ and then to compare it to
the actual distribution. Similarly, to verify condition~3 one can find the
capacity achieving distribution of the channel $p_{Y|X}$ given the cost
measure~$\rho(x)$ and then check whether it matches the actual input
distribution.

Alternatively, the following results by Gastpar et al.~\cite[Lemmas~2.2
and~2.3]{GastparRV2003} allow to test conditions~1 and~3 of
\thmref{sloptimality} without solving the capacity problem and the
rate-distortion problem.

\begin{lemma}
  \label{lem:gastparrd}
  For a fixed discrete source distribution~$p_S$, a discrete channel conditional
  distribution~$p_{Y|X}$, and a single-letter code~$(f,g)$:
  \begin{enumerate}
    \item If $0 < I(S; \Sh)$, condition~1 of \thmref{sloptimality} is satisfied
      if and only if the distortion measure satisfies
      \begin{equation}
        \label{eq:rdmatch}
        d(s,\sh) = -c_1 \log_2 \frac{p(\sh|s)}{p(\sh)} + d_0(s),
      \end{equation}
      where $c_1 > 0$ and $d_0(\cdot)$ is an arbitrary function. 

    \item If $I(S;\Sh) = 0$, condition~1 of \thmref{sloptimality} is satisfied
      for any distortion measure~$d(s,\sh)$.
  \end{enumerate}
\end{lemma}

\begin{lemma}
  \label{lem:gastparcost}
  For a fixed discrete source distribution~$p_S$, a single-letter encoder~$f$,
  and a discrete channel conditional distribution $p_{Y|X}$ with unconstrained
  capacity $C_0 \deq \max_{p_X} I(X;Y)$:
  \begin{enumerate}
    \item If $I(X;Y) < C_0$, condition~3 of \thmref{sloptimality} is satisfied
      if and only if the input cost measure satisfies
      \begin{equation}
        \label{eq:costmatch}
        \rho(x)
        \begin{cases}
          = c_2 D(p_{Y|X}(\cdot|x) \| p_Y(\cdot)) + \beta, & 
          \text{if $p(x) > 0$}, \\
          \ge c_2 D(p_{Y|X}(\cdot|x) \| p_Y(\cdot)) + \beta, &
          \text{otherwise,}
        \end{cases}
      \end{equation}
      where $c_2 > 0$ and $\beta$ are constants, and $D(\cdot\|\cdot)$ denotes
      the Kullback-Leibler divergence between two distributions.

    \item If $I(X;Y) = C_0$, condition~3 of \thmref{sloptimality} is satisfied
      for any cost measure~$\rho(x)$.
  \end{enumerate}
\end{lemma}

(For sources and channels with continuous alphabets, the conditions are
sufficient but not necessary.)

As a consequence of these two lemmas, one can find for any source, channel, and
single-letter code a suitable cost measure and a suitable distortion measure
such that the resulting system satisfies $R(D) = C(P)$. There is therefore an
infinity of optimal single-letter communication systems. Two well known examples
of such optimal single-letter systems are the transmission of a binary symmetric
source across a binary symmetric channel under Hamming distortion, and the
transmission of a Gaussian source across an AWGN channel under an average power
constraint and squared error distortion measure (the latter will be described in
detail in \chapref{delaygauss}).


\subsection{Systems with Bandwidth Mismatch}

In a communication system with $k \ne n$, Lemmas~\ref{lem:gastparrd}
and~\ref{lem:gastparcost} still apply: for each~$i$ the conditional marginal
distribution of $\Sh_i$ given~$S_i$ must relate to the distortion measure
according to \lemref{gastparrd}, and the marginal distribution of~$X_i$ must
relate to the cost function according to \lemref{gastparcost}. But this is no
longer enough, as \thmref{optimalityconditions} shows. Thus, for a given source,
channel, and code, one cannot necessarily find suitable cost and distortion
measures to make the system optimal, as opposed to the case when $k = n$.  In
particular, even in the canonical case of transmitting a Gaussian source across
a Gaussian channel under squared error distortion and an input power constraint,
for which uncoded transmission is optimal when $k = n$, no simple transmission
scheme exists when $k \ne n$.


\section{Two Facets of the Problem}\label{sec:twofacets}

The contributions of this thesis address two facets of the source-channel
communication problem. In \chapref{fidelity} the question is: when does a
communication system maximize the \emph{ratio} of fidelity per cost, rather than
maximize the fidelity (\ie, minimize the distortion) for a fixed cost or
minimize the cost for a fixed fidelity.  It turns out that a simple refinement
of Lemmas~\ref{lem:gastparrd} and~\ref{lem:gastparcost} yields new matching
conditions for this case. In particular, these new matching conditions allow to
check when uncoded transmission (with minimal delay) achieves the maximal
fidelity per cost ratio. Moreover, the problem of maximizing the fidelity per
cost can be broken down into separate problems that concern only the source and
the channel, respectively, thus providing a separation theorem. 

Chapters~\ref{ch:delaygauss} and~\ref{ch:mindelbwex} focus solely on the
Gaussian channel. They consider the problem of how to transmit an analog-valued
source across \emph{multiple uses} of a Gaussian channel at minimal delay (and
complexity).  \chapref{delaygauss} draws insight from a well known feedback
communication scheme to obtain intuition about how to take advantage of uncoded
transmission in the absence of feedback. These reflections lead to a tradeoff
between coded and uncoded communication, mirroring a well studied tradeoff
between the length of the signal curve and the distance between the curve's
folds that results from a geometric analysis of the problem.

The results of \chapref{delaygauss} give a new \emph{raison d'\^etre} to a well
known hybrid communication strategy combining quantization and uncoded
transmission.  \chapref{mindelbwex} analyzes this strategy in detail and
provides an exact characterization of its performance in the large SNR regime,
as well as several extensions to the transmission strategy. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% CHAPTER APPENDICES                                                           %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{subappendices}

\section{Proof of the Converse Part of \thmref{separationconverse}}
\label{app:separationproof}

The proof results from the following chain of inequalities.
{\allowdisplaybreaks
\begin{align*}
  kR(D) &= k R\left(\frac1k \sk \E[d(S_i, \Sh_i)] \right) \\
  &\stackrel{(a)}{\le} \sk R\left(\E[d(S_i, \Sh_i)] \right) \\
  &\stackrel{(b)}{\le} \sk I(S_i; \Sh_i) \\
  &= \sk \left( H(S_i) - H(S_i|\Sh_i)  \right)\\
  &= H(S^k) - \sk H(S_i|\Sh_i) \\
  &\stackrel{(c)}{\le} H(S^k) - \sk H(S_i|S^{i-1} \Sh^k) \\
  &=I(S^k;\Sh^k) \\
  &\stackrel{(d)}{\le} I(S^k; Y^n) \\
  &\stackrel{(e)}{\le} I(X^n; Y^n) \\
  &= \sn \left( H(Y_i|Y^{i-1}) - H(Y_i|Y^{i-1} X^n) \right)  \\
  &\stackrel{(f)}{\le} \sn \left( H(Y_i) - H(Y_i|X_i) \right) \\
  &= \sn I(X_i; Y_i) \\
  &\stackrel{(g)}{\le} \sn C(\E \rho(X_i)) \\
  &\stackrel{(h)}{\le} n C(\frac1n \sn \E \rho(X_i)) \\
  &= n C(P).
\end{align*}}%
The inequalities are justified as follows. (a) follows from the convexity$_\cup$
of~$R(D)$ and (b) from its definition. (c) is because
conditioning can only decrease the entropy. (d) and (e) follow from the data
processing inequality. (f) is because conditioning can only decrease entropy and
because the channel is memoryless. Finally, (g) applies by definition and (h) is
due to the concavity$_\cap$ of~$C(P)$.  \hfil\qed


\section{Separation Theorem, Forward Part}\label{app:separationforward}

\begin{theorem}
  \label{thm:separationforward}
  Consider a memoryless source that has rate-distortion function~$R(D)$ and 
  produces a source symbol every $\ts$~seconds, and a memoryless channel with
  capacity-cost function~$C(P)$ that accepts an input symbol for transmission
  every $\tc$~seconds. Then for any $(D,P)$ pair satisfying
  \begin{equation}
    \label{eq:separationforward}
    R(D)/\ts \le C(P)/\tc - \e,
  \end{equation}
  where $\e > 0$, there exists a source code and a channel code that, when
  combined to transmit the source across the channel, result in an average
  distortion of at most~$D$ and an average cost of at most~$P$.
\end{theorem}

\begin{proof}
  Assume $R(D)/\ts \le C(P)/\tc - \e$. According to the source coding theorem
  there exists, for an arbitrary $\e' > 0$, a source code that achieves
  distortion at most~$D$ and produces $R(D) + \e'$ bits per source symbol, or
  $(R(D) + \e')/\ts$ bits per second. According to the channel coding theorem,
  for any $\e' > 0$ there exists a channel code that uses average input
  cost at most~$P$ and can reliably transmit $C(P) - \e'$ bits per channel use
  or $(C(P) - \e')/\tc$ bits per second.

  The output of the source code can be mapped to the input of the channel code
  provided that $(R(D) + \e')/\ts \le (C(P) - \e')/\tc$, 
  or equivalently
  \[ R(D)/\ts \le C(P)/\tc - \e'\left(\frac{1}{\ts} + \frac{1}{\tc} \right). \]
  By choosing $\e'$ small enough, it follows from the assumption on~$D$ and~$P$
  that this inequality holds.
\end{proof}


\end{subappendices}
