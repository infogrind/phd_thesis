\chapter{Fundamentals of Source-Channel Communication}\label{ch:fundamentals}

This thesis is about point-to-point communication of a memoryless source across
a memoryless channel. In its most general form, this problem is made up of the
following six elements, displayed schematically in \figref{scgen}.
\begin{itemize}
  \item A memoryless source with distribution~$\pS$, producing a source symbol
    every $\ts$~seconds.
  \item A memoryless channel with transition distribution~$\pyx$, accepting an
    input symbol for transmission every $\tc$~seconds.
  \item An encoder function~$f$ that maps a block of~$k$ source symbols $S^k
    = (S_1, \dots, S_k)$ into $n$~channel input symbols $X^n = (X_1,
    \dots, X_n)$.
  \item A decoder function~$g$ that maps a block of $n$~channel output symbols
    $Y^n = (Y_1, \dots, Y_n)$ into $k$~source estimates $\Sh^k = (\Sh_1,
    \dots, \Sh_k)$.
  \item A \emph{cost measure} $\rho(x)$ that assigns a transmission cost to each
    channel input symbol, and a \emph{distortion measure} $d(s,\sh)$ that
    assigns a reconstruction ``badness'' to every pair of source symbol and
    estimate.
\end{itemize}
The parameters $\ts$, $\tc$, $k$, and $n$ are given as part of the problem
setting. To match the number of channel inputs produced by the encoder to the
rate at which the channel accepts them, they must satisfy $n/k = \ts / \tc$. The
communication consists thus of identical rounds of transmission of length $k\ts$
(or $n\tc$), and the analysis can be limited to one such round.


\begin{figure}
  \begin{center}
    \input{figures/sc_gen.tex_t}
  \end{center}
  \caption{A general memoryless point-to-point communication system.}
  \label{fig:scgen}
\end{figure}

If the discrete-time source and channel represent an underlying continuous
bandlimited source and channel, $\ts$ and $\tc$ relate directly to
the respective bandwidths. If $\ts = \tc$, one therefore says that the source
and channel are \emph{bandwidth matched}. Correspondingly, a code is said to be
\emph{bandwidth matched} if $k=n$, to be a \emph{bandwidth expansion} code if $k
< n$, and to be a bandwidth compression code if $k > n$. 
The source and the channel, together with the encoder and decoder, imply a joint
distribution of the tuple $(S^k, X^n, Y^n, \Sh^k)$.  Depending on the encoder,
the sequence of channel inputs $X^n$ may not be identically distributed;
similarly, the marginal (joint) distribution of the source/estimate pairs
$(S_i,\Sh_i)$ may not be the same for all~$i$. The average cost and distortion
of a communication system are therefore defined as the empirical average over
blocks of channel inputs and source symbols, respectively.

\begin{definition}
  \label{def:avgcost}
  The \emph{average channel input cost} incurred by a memoryless point-to-point
  communication system is
  \begin{equation*}
    P = \sum_{i=1}^n \E[\rho(X_i)].
  \end{equation*}
\end{definition}

\begin{definition}
  \label{def:avgdist}
  The \emph{average distortion} incurred by a memoryless point-to-point
  communication system is
  \begin{equation*}
    D = \sum_{i=1}^k \E[d(S_i, \Sh_i)],
  \end{equation*}
  where the expectation is taken over the joint distribution of $S_i$
  and~$\Sh_i$.
\end{definition}

\begin{definition}
  \label{def:capacity}
  The \emph{capacity-cost function} of a channel $\pyx$ with cost
  function~$\rho(x)$ is
  \begin{equation*}
    C(P) = \max_{\px: \E[\rho(X)] \le P} I(X;Y).
  \end{equation*}
\end{definition}

\begin{definition}
  \label{def:ratedistortion}
  The \emph{rate-distortion function} of a source~$\ps$ with distortion
  measure~$d(s,\sh)$ is 
  \begin{equation*}
    R(D) = \min_{\pshs: \E[d(S,\Sh)] \le D} I(S;\Sh).
  \end{equation*}
\end{definition}


\section{Fundamental Limits of Performance}

The goal of source-channel communication is to transmit a source at a low cost
of transmission and with a reconstruction at the receiver that has little
distortion from the original. The region of achievable cost and distortion pairs
is thus fundamental in establishing the best possible performance of a given
communication problem.

\begin{definition}
  \label{def:achievableregion}
  For a given source~$\ps$, channel~$\pyx$, cost measure~$\rho(x)$ and
  distortion measure~$d(s,\sh)$, the \emph{achievable cost\slash distortion
  region} is the set of all pairs $(D,P)$ for which there exists either a code
  $(f,g)$ that incurs average distortion~$D$ and average cost~$P$, or a sequence
  of codes that approach the distortion~$D$ and the cost~$P$ in the limit.
\end{definition}

The most fundamental bound on the achievable cost/distortion region is given by
the following result~\cite[Theorem~21]{Shannon1948}.

\begin{theorem}
  \label{thm:separationconverse}
  In any memoryless point-to-point source-channel communication system, the
  average cost~$P$ and the average distortion~$D$ are related by
  \begin{equation}
    \label{eq:separation}
    kR(D) \le nC(P).
  \end{equation}
\end{theorem}

\begin{proof}
  See \appref{separationproof}.
\end{proof}

\thmref{separationconverse} provides an \emph{outer bound} to the achievable
cost/distortion region. It says that regardless how powerful an encoder and
decoder are, the incurred cost and distortion always lie in the region
specified by~\eqref{eq:separation}. Note also that~\eqref{eq:separation} only
depends on the ratio $k/n$, so that the same bound applies to a code that
encodes $1000k$ source symbols into $1000n$ channel inputs. 

This theorem is often referred to as the converse part of the separation
theorem, because it establishes that a $(D,P)$ pair which cannot be achieved (or
approached) using separately performed source and channel coding cannot be
achieved at all.  The theorem should not only be seen in relation to
separation-based coding, though.  In fact, while the forward part of the
separation theorem (given for reference in \appref{separationforward}) is only
valid if one allows for codes of unrestricted delay (and complexity),
\thmref{separationconverse} applies to \emph{all} codes and is thus in a way
more general than the forward part. In particular, it also applies to codes with
limited delay, which are the subject of this thesis.


\section{Optimality Conditions}\label{sec:optimality}

For the purpose of this chapter, an optimal communication system is
defined as follows.

\begin{definition}
  \label{def:optimality}
  An \emph{optimal communication system} is either a communication system whose
  average cost and distortion satisfy~\eqref{eq:separation} with equality, or
  the limit of a sequence of systems whose performance approach equality
  in~\eqref{eq:separation}.\footnote{This is not the strictest form of
  optimality and it precludes the existence of optimal communication systems in
  some situations, for example when $\max_D R(D) < \min_P C(P)$. See
  Gastpar~\cite{GastparThesis} for a more in-depth treatment of alternative
  definitions of optimal communication systems.}
\end{definition}

By going step by step through the inequalities in the proof of
\thmref{separationconverse}, the following result can be established. 

\begin{theorem}
  \label{thm:optimalityconditions}
  A point-to-point memoryless communication system is optimal according to
  \defref{optimality} if and only if the following conditions are all satisfied.
  \begin{enumerate}
    \item For each $i = 1$, \dots, $k$, the joint distribution $p(s_i,\sh_i)$
      achieves the rate-distortion function of the source at the same average
      distortion~$D$.
    \item The \emph{reverse test channel} $p(s^k|\sh^k)$ factors as
      $\prod_{i=1}^k p(s_i | \sh_i)$. 
    \item The encoder is information lossless in the sense that $I(S^k;Y^n) =
      I(X^k; Y^n)$. 
    \item The estimate sequence $\Sh^k$ is a sufficient statistic for $S^k$
      given $Y^n$. (Equivalently, we can say that the decoder must be memoryless
      in the sense that $I(X^n;Y^n) = I(X^n;\Sh^k)$.)
    \item The channel output sequence $Y^n$ is \iid.
    \item The marginal distributions $p(x_i)$ all achieve the capacity at the
      same average cost~$P$.
  \end{enumerate}
\end{theorem}

The separation theorem~\cite[Theorem~21]{Shannon1948} states that under the
conditions of the source coding theorem and the channel coding theorem, there
exist for any source, channel, distortion measure and cost measure, a sequence
of codes that approach $k R(D) = nC(P)$ and thus lead to an optimal
communication system. In the limit, this sequence of codes thus fulfills all the
conditions of \thmref{optimalityconditions}.

Can only codes based on the separation principle achieve any point of the
achievable cost\slash distortion region? Not at all. Using the results of
Gastpar et al.~\cite{GastparRV2003}, one can construct infinitely many examples
of very simple joint source-channel codes that incur minimal delay yet whose
average cost and distortion lie on the boundary of the achievable region
characterized by~\eqref{eq:separation} and can thus be considered optimal. Later
in this chapter we will encounter a prominent example of such a code. First,
though, let us give some general considerations about delay limited codes.


\section{Codes With a Delay Constraint}

The forward part of the separation theorem relies on the source coding and
channel coding theorems, which assume unrestricted block lengths and thus
unrestricted delay. This section looks at the consequences that arise when a
constraint is put on the delay incurred by a point-to-point communication
system. To be precise, we first define what we exactly mean by delay.

\begin{definition}
  \label{def:delay}
  The \emph{delay} incurred by a point-to-point source-channel communication
  system is the time between the instant a source symbol is produced and the
  instant the receiver has gathered enough channel output symbols to decode that
  symbol. 
\end{definition}

What is the delay of a code that encodes $k$~source symbols into $n$~channel
input symbols? Since the source produces a symbol every $\ts$~seconds it takes
$k\ts$ seconds to gather $k$~source symbols. Assuming that the encoding process
takes place instantaneously, it takes an additional $n\tc$ seconds to transmit
the encoded source sequence across the channel. Neglecting the transmission time
of the channel, the receiver can thus decode the first source symbol after $k\ts
+ n \tc$ seconds, or $2k\ts$ seconds. Accordingly, the smallest delay is caused
by a code that encodes a \emph{single} source symbol at a time. We call such a
code a \emph{minimal-delay code}.

Codes based on the separation theorem have the advantage that separate codes can
be designed for the source and the channel. This way, the designer of the source
code does not have to know a priori over what channel the source will be
transmitted, and neither does the designer of the channel code have to know what
kind of source will be transmitted across the channel. The cost paid for this
flexibility is the large delay and complexity required by a separation based
code. 

In situations where external circumstances (such as real-time communication) put
a constraint on the tolerable delay, the flexibility of separate source and
channel coding must be sacrificed in favor of codes that are designed jointly
for a particular source and a channel. As we will see shortly, there exist such
joint source-channel codes that perform as good as any separation-based code,
yet only cause minimal delay.

\begin{remark}
  \label{rem:blocklength}
  In conventional source coding and channel coding, the term ``large block
  length'' is often used to imply a large delay. What is the block length of a
  joint source-channel code? On one hand, the block length of a source code is
  understood to be the number~$k$ of source symbols encoded at a time. On the
  other hand, the block length of a channel code is the length $n$ of each
  channel input codeword. Rather than attempting to define the block length of a
  joint source-channel code one way or the other, we prefer to avoid the term
  block length altogether in the context of such codes. 
\end{remark}

From a theoretical point of view, the drawback of codes with a delay limit is
that there exists no exact characterization of the achievable cost and
distortion region for such codes. While the bound of \thmref{separationconverse}
still applies, it only depends on the \emph{ratio} $k/n$ and therefore does not
take into account delay constraints. There have certainly been attempts to
refine this bound to apply to delay limited codes, most notably Ziv and Zakai's
observation that tighter bounds can be obtained by replacing the logarithm in
the mutual information by a different function, as long as this function
satisfies certain constraints~\cite{ZivZ1973}. This can result in outer bounds
that become tighter for stricter delay constraints. However, none of the bounds
obtained (so far) using this method are provably tight for a given delay
constraint, not even for the canonical case of a Gaussian source and channel,
which is the subject of the next section.

\figref{achievableregions} summarizes the current state of knowledge. For codes
without delay limits, the achievable cost/distortion region is completely
characterized by the separation theorem. For delay limited
codes, on the other hand, no general achievability result exists, but there are
some refined outer bounds. The lower right corner of the figure is thus empty
except for a few dots that represent special cases where minimal delay codes
achieve the outer bound of \thmref{separationconverse}.

\begin{figure}
  \begin{center}
    \input{figures/achievableregions.tex_t}
  \end{center}
  \caption{The current knowledge of the theoretical limits of source-channel
  coding. For codes without delay constraint, the outer bound and the
  region achievable using separation-based codes coincide and thus characterize
  exactly the achievable cost/distortion region. For delay limited codes, some
  bounds exist that improve on the converse separation theorem, but no general
  characterization of the achievable cost/distortion region exist. The few
  special cases where minimal-delay codes achieve the separation theorem bound
  are represented by the dots in the lower right corner.}
  \label{fig:achievableregions}
\end{figure}


\section{The Gaussian Case}

The Gaussian source and channel play a special role in information theory not
only because of the significance of the Gaussian distribution due to the central
limit theorem, but also because the Gaussian case allows for analytical
solutions of many information theoretic quantities, most notably the
rate-distortion function (under squared error distortion) and the capacity
(under an average power constraint). In addition, Gaussian noise is the worst
kind of additive noise under a constraint on the noise variance, so solving a
communication problem for Gaussian noise gives the solution a certain
universality. On the search for the achievable cost and distortion region under
a delay constraint, it is therefore plausible to start by looking at the
Gaussian source and channel. 

\begin{definition}[Gaussian source and channel]
  \label{def:gaussiansc}
  A discrete-time memoryless \emph{Gaussian source} of variance~$\ssq$ is a
  source whose distribution is zero-mean Gaussian with variance~$\ssq$.

  A discrete-time memoryless \emph{Gaussian channel} (also called discrete-time
  additive white Gaussian noise channel, AWGN) with noise variance~$\szq$ is a
  channel whose transition distribution satisfies $p(y|x) \sim \cN(x, \szq)$,
  \ie, given the input~$x$, the output is Gaussian with mean~$x$ and
  variance~$\szq$.
\end{definition}

The distortion measure considered in the sequel is the squared error distortion
$d(s,\sh) = (s - \sh)^2$, and the cost measure is the channel input power
$\rho(x) = x^2$, such that $D = \frac1k \sum_{i=1}^k \E[(S_i - \Sh_i)^2]$ and $P
= \frac1n \sum_{i=1}^n \E[X_i^2]$.

Plugging in the formulas for the rate-distortion function and the capacity-cost
function for the Gaussian source and channel, the bound~\eqref{eq:separation}
becomes
\begin{equation}
  \label{eq:gausssep}
  \frac{\ssq}{D} \le \left( 1 + \frac{P}{\szq} \right)^{n/k}
\end{equation}
or equivalently
\begin{equation}
  \label{eq:gausssepsdr}
  \sdr \le (1 + \snr)^{n/k},
\end{equation}
where we have defined the \emph{source-to-distortion ratio} $\sdr = \ssq/D$ and
the \emph{signal-to-noise ratio} $\snr = P/\szq$. 


\subsection{When Uncoded Transmission is Optimal}

It is a well known fact that when $\ts = \tc$ (\ie, the number of source symbols
produced per second is equal to that of channel inputs accepted per second),
plugging a Gaussian source directly into a Gaussian channel results in an
optimal communication system, as the following example makes clear.

\begin{example}
  \label{ex:gausssingle}
  Let the source be zero-mean Gaussian with variance~$\ssq$ and let the channel
  be AWGN with noise variance~$\szq$. The encoder is given by $X =
  f(S) = \sqrt{P/\ssq} S$ and the decoder is given by $\Sh = g(Y) = \sqrt{P}
  \ssq Y / (P + \szq)$. Using $Y = X + Z$ it is quickly verified that
  \begin{equation*}
    D = \E[(S - \Sh)^2] = \ssq / (1 + P/\szq),
  \end{equation*}
  which is indeed the optimal distortion according to~\eqref{eq:gausssep}.
\end{example}

The example shows that when $\ts = \tc$, a joint source-channel code with the
smallest possible delay (a single source symbol is encoded at a time) leads to
the same region of achievable $(D,P)$ pairs as when the delay is unrestricted.

It is instructive to look at how this example satisfies the conditions of
\thmref{optimalityconditions}. First, since $k = n = 1$, conditions~2 and~5 of
the theorem are trivially satisfied. Because the encoder and the decoder are
invertible, conditions~3 and~4 are satisfied as well. More interestingly,
condition~6 is satisfied because the distribution of the source is in fact the
capacity achieving distribution of the channel (up to scaling), so there is no
need for coding to achieve capacity. Similarly, the resulting joint distribution
of $(S, \Sh)$ is the one that achieves the rate-distortion function of the
source.

Is this another particularity of the Gaussian distribution? The answer is no: in
fact, \emph{any} input distribution achieves the capacity of a given channel for
\emph{some} way of measuring the channel input cost. For example, the Gaussian
distribution achieves the capacity of a Gaussian channel when the cost
measure has the form $\rho(x) = ax + b$ (for arbitrary constants $a > 0$
and~$b$), which applies to the input power measure used here. The uniform
distribution also achieves the capacity of a Gaussian channel, just for a
different cost measure. In the same way, \emph{any} joint distribution of $(S,
\Sh)$ achieves the rate-distortion function of the source for \emph{some} way of
measuring distortion. This paradigm, called \emph{measure matching}, is made
precise and extensively discussed in~\cite{GastparRV2003}.


\subsection{Bandwidth Expansion}

Does a similarly simple transmission scheme exist for the Gaussian case when the
channel accepts more than one input for each source symbol, \ie, $\tc > \ts$?
Unfortunately, no. In fact, the performance of any code that encodes a single
source symbol into $n>1$~channel inputs is strictly bounded away from the
optimum given by~\eqref{eq:gausssep}~\cite{IngberLZF2008}.  The bound given
in~\cite{IngberLZF2008} is not necessarily tight, however, so the region of
achievable cost and distortion pairs is not known when $k = 1$ and $n > 1$. 

While conditions~1 and~6 of \thmref{optimalityconditions} can still be achieved
by a simple linear encoder and a MMSE decoder (just as in \exref{gausssingle}),
condition~5 is no longer trivially satisfied when $n > 1$. In fact, the
difficulty in this situation is to deterministically encode one Gaussian source
symbol into $n$~independent Gaussian channel inputs. The result
of~\cite{IngberLZF2008} implies that not all conditions of
\thmref{optimalityconditions} can be simultaneously satisfied when $k = 1$ and
$n > 1$.  This changes, however, if we modify the scenario and allow the encoder
access to perfect feedback from the receiver, as the next section shows.

\begin{remark}
  \label{rem:ratematched}
  As mentioned above, when $k = n = 1$ then transmitting any source uncoded
  across any channel is optimal, provided that the cost and distortion measures
  are properly matched to the statistics of the system. One could extend the
  concept of \emph{matching} sources and channels in the following way. 

  Consider a source that produces symbols from a $4$-ary alphabet, uniformly
  distributed, and a binary symmetric channel. If $\tc = 2\ts$, this source is
  matched to the channel in the sense that the source can be split into two
  independent random variables, and so it produces essentially two independent
  symbols for every two channel inputs.
\end{remark}


\subsection{Optimality Through Feedback}

If there is a causal, noiseless feedback link from the receiver to the encoder,
then the $i\th$ channel input symbol can depend on the past channel outputs
$Y_1$, \dots, $Y_{i-1}$ as well as on the source. Because feedback does not
increase the capacity of the channel, the bound of \thmref{separationconverse}
still applies and so do the conditions of \thmref{optimalityconditions}. The big
advantage brought by the feedback, though, is that it permits a simple
transmission scheme that has minimal delay, yet achieves the
bound~\eqref{eq:gausssep} with equality, as the following example demonstrates.

\begin{figure}
  \begin{center}
    \input{figures/sc_gen_feedback.tex_t}
  \end{center}
  \caption{A source-channel communication system where the encoder has access to
  causal noiseless feedback from the receiver.}
  \label{fig:scgenfeedback}
\end{figure}

\begin{example}
  \label{ex:gaussfb}
  In this example, a memoryless Gaussian source of variance~$\ssq$ is
  transmitted across a memoryless Gaussian channel with power constraint~$P$ and
  noise variance~$\szq$.  Define $E_0 = S$. In the $i\th$ channel use ($i = 1$,
  \dots, $n$), the encoder produces
  \begin{equation}
    \label{eq:gaussfbxi}
    X_i = \sqrt{\frac{P}{\Var E_{i-1}}} E_{i-1}.
  \end{equation}
  Both the receiver and the sender now compute the minimum mean-squared
  error (MMSE) estimator $\Eh_{i-1}$ of $E_{i-1}$ given $Y_i$. The sender then
  computes $E_i = \Eh_{i-1} - E_{i-1}$ and proceeds to the next round.

  After $n$~rounds of transmission, the receiver has $n$~estimates $\Eh_0$
  to~$\Eh_{n-1}$. Using these, it computes the final estimate~$\Sh$ as
  \begin{equation}
    \label{eq:shdecom1}
    \Sh = \Eh_0 - \Eh_1 + \Eh_2 - \cdots \pm \Eh_{n-1}.
  \end{equation}
  (The sign of the last term is $+$ if $n$~is even and $-$ if $n$~is odd.)

  To compute the overall distortion $\mse$, note that $\Eh_{i-1} = E_{i-1} +
  E_i$ by definition, so \eqref{eq:shdecom1}~can be written as
  \begin{align*}
    \Sh &= (E_0 + E_1) - (E_1 + E_2) + (E_2 + E_3) - \cdots \pm (E_{n-1} + E_n)
    \\
    &= E_0 \pm E_n,
  \end{align*}
  and since we have defined $E_0 = S$, we have $\mse = \E[E_n^2]$, where $E_n$
  is the remaining error after the last round of transmission.

  To compute $\E[E_n^2]$, note that since $\Eh_i$ is the MMSE estimator of
  $E_i$, the estimation error variance is given by (see
  \eg~\cite[Section~8.3]{Scharf1990})
  \begin{equation}
    \label{eq:gaussvardec}
    \E[E_{i+1}^2] = \E[(\Eh_i - E_i)^2] = \frac{\E[E_i^2]}{1 + P/\szq}.
  \end{equation}
  Using $\E[E_0^2] = \E[S^2] = \ssq$ and recursively applying the above, we find
  that
  \begin{align*}
    \frac{\ssq}{\E[E_n^2]} &= \frac{\ssq (1 + P/\szq)}{\E[E_{n-1}^2]} \\
    &= \frac{\ssq(1 + P/\szq)^2}{\E[E_{n-2}^2]} = \dots \\
    &= \frac{\ssq(1 + P/\szq)^n}{\E[E_0^2]} = (1 + P/\szq)^n,
  \end{align*}
  which is indeed the largest possible SDR according to~\eqref{eq:gausssepsdr}.
\end{example}

As we have already seen before, achieving the capacity and the rate-distortion
function (conditions~1 and~6 of \thmref{optimalityconditions}) are just a matter
of using the ``right'' cost measure and distortion measure, respectively. The
transmission scheme of \exref{gaussfb} in addition satisfies all other
conditions of \thmref{optimalityconditions}. In particular, it produces a
sequence of independent channel outputs. Again, we may ask if this is just due
to some special property of the Gaussian distribution. And again, the answer is
no: the next section explains how, using feedback, one can design a
minimal-delay encoder that produces a sequence of independent, capacity
achieving output symbols for any channel (and any cost measure).


\subsubsection{Exploiting Feedback Via Posterior Matching}

Example~\ref{ex:gaussfb} showed how a simple transmission scheme can achieve the
optimal distortion for a Gaussian source and channel if noiseless feedback is
available.  We now show how it is possible to encode one source symbol into
$n$~channel inputs for \emph{any} channel and \emph{any} cost measure (under the
condition that the source is continuous-valued).

Before continuing we prove some properties of cumulative distribution functions
(\cdf s).

\begin{lemma}
  \label{lem:cdfunif}
  Let $X$ be a continuous random variable with density $f(x)$ and \cdf\ $F_X$,
  \ie,
  \begin{equation*}
    F_X(x) = \Pr[X \le x].
  \end{equation*}
  Then the random variable $Y = F_X(X)$ is uniformly distributed on~$[0,1]$.
\end{lemma}

\begin{proof}
  Let $Y = F_X(X)$. Then $\Pr[Y \le y] = \Pr[F_X(X) \le y]$.
  Hence if $y < 0$ then $\Pr[Y \le y] = 0$, and if $y > 1$ then $\Pr[Y \le y] =
  1$.  If $y \in [0,1]$ then
  \begin{align*}
    \Pr[F_X(X) \le y] &= \Pr[X \le F_X^{-1}(y)] \\
    &= \int_{-\infty}^{F_X^{-1}(y)} f(x) dx \\
    &= F_X(F_X^{-1}(y)) = y.
  \end{align*}
  The \cdf\ of~$Y$ is therefore that of a uniform random variable on $[0,1]$.
\end{proof}


\begin{lemma}
  \label{lem:invcdf}
  Let $Y$ be a uniform random variable on~$[0,1]$ and let $F_X$ be the \cdf\ of
  an arbitrary random variable~$X$. If $F_X$~is not invertible, define
  $F_X^{-1}$ with a slight abuse of notation as
  \begin{equation}
    \label{eq:invcdf}
    F_X^{-1}(y) = \sup \{x : F_X(x) \le y\}.
  \end{equation}
  Then the random variable $X' = F_X^{-1}(Y)$ has the same distribution as~$X$.
\end{lemma}

\begin{proof}
  The definition of $F_X^{-1}$ according to~\eqref{eq:invcdf} is such
  that $\Pr[F_X^{-1}(Y) \le x] = \Pr[Y \le F_X(x)]$. Thus,
  \begin{align*}
    \Pr[F_X^{-1}(Y) \le x] &= \Pr[Y \le F_X(x)] \\
    &= \int_0^{F_X(x)} d\xi = F_X(x).
  \end{align*}
\end{proof}

Consider now a channel~$\pyx$ and let $\pi(x)$ be the capacity achieving
distribution at average cost~$P$, \ie, 
\begin{equation*}
  \pi(x) = \arg\max_{p(x): \E[\rho(X)] \le P} I(X;Y)
\end{equation*}
for an arbitrary cost measure~$\rho(x)$.  The problem is to encode one source
symbol of a continuous-valued source into~$n$ channel inputs, making use of the
feedback.

Let $\Fpi$ be the \cdf\ of the distribution $\pi(x)$, and let $F_S$ be the \cdf\
of the source. In the first channel use, the encoder produces
\begin{equation}
  \label{eq:posteriorx1}
  X_1 = \Fpi^{-1}(F_S(S)),
\end{equation}
where $\Fpi^{-1}$ is the inverse of $\Fpi$ according to~\eqref{eq:invcdf}. By
Lemma~\ref{lem:cdfunif}, if $S$ is continuous then $F_S(S)$ has uniform
distribution on $[0,1]$, and so by Lemma~\ref{lem:invcdf}, $\Fpi^{-1}(F_S(S))$
is a random variable with \cdf\ $\Fpi$.

After $i-1$ rounds of transmission, the encoder knows $y_1$, \ldots, $y_{i-1}$
and can compute the conditional \cdf\ $F_{S|y_1, \ldots, y_{i-1}}$. It then
sends
\begin{equation}
  \label{eq:posteriorxi}
  X_i = \Fpi^{-1}(F_{S|y_1,\dots,y_{i-1}}(S)).
\end{equation}
Again, since $S$ is continuous, $F_{S|y_1, \dots, y_{i-1}}(S)$ is uniform. For
any $y_1$, \ldots, $y_{i-1}$, therefore, \begin{equation*}
  p(x_i|s, y_1, \dots, y_{i-1}) = \pi(x)
\end{equation*}
and so $X_i$ is independent of $Y_1$, \ldots, $Y_{i-1}$. 

Using this strategy the encoder produces an \iid\ sequence of inputs $X_i$
with the capacity achieving distribution~$\pi(x)$, satisfying conditions~5
and~6 of \thmref{optimalityconditions}; condition~3 of the theorem is
trivially satisfied because the encoder is deterministic.

Let us now derive the posterior matching encoder for the communication system of
\exref{gaussfb}.

\begin{example}
  \label{ex:gaussfbpost}
  First, a few properties of Gaussian \cdf s are given. Let $F_{\N(\mu, \sq)}$
  be the \cdf\ of a Gaussian random variable of mean~$\mu$ and variance~$\sq$
  and let $F_\N \deq F_{\N(0,1)}$. Then $F_{\N(\mu,\sq)}(x) =
  F_\N((x-\mu)/\sigma)$. Furthermore, the inverse \cdf\ is 
  \begin{equation*}
    F_{\N(\mu,\sq)}^{-1}(y) = \sigma F_\N^{-1}(y) + \mu.
  \end{equation*}

  Let $\pi(x) = \N(0,P)$. According to~\eqref{eq:posteriorx1}, the first channel
  input is
  \begin{equation*}
    X_1 = \sqrt{P} F_{\N}^{-1}(F_{\N}(S/\sigma_S)) = \sqrt{\frac{P}{\ssq}} S,
  \end{equation*}
  which coincides with~\eqref{eq:gaussfbxi} in \exref{gaussfb} when~$i=1$.

  Given $Y_1$, $S$ is Gaussian with mean $\E[S|Y_1]$ and variance $\Var(S -
  \E[S|Y_1])$. Following~\eqref{eq:posteriorxi}, the second channel input is
  thus
  \begin{align*}
    X_2 &= \sqrt P F_{\N}^{-1} \left( F_{\N} \left( \frac{S - \E[S|Y_1]}
    {\sqrt{\Var(S-\E[S|Y_1])}} \right) \right) \\
    &= \sqrt{P} \frac{S - \E[S|Y_1]}{\sqrt{\Var(S-\E[S|Y_1])}}.
  \end{align*}
  Continuing this way, the $i\th$ channel input is found to be
  \begin{equation}
    \label{eq:gausspmenc}
    X_i = \sqrt{P} \frac{S - \E[S|Y_1^{i-1}]}{\sqrt{\Var(S-\E[S|Y_1^{i-1}])}}.
  \end{equation}
  That this is equal to~\eqref{eq:gaussfbxi} can be seen as follows. In
  \exref{gaussfb}, write
  \begin{align*}
    S &= E_0 + (E_1 - E_1) - (E_2 - E_2) + \dots \pm (E_{i-2} -
    E_{i-2}) \\
    &= (E_0 + E_1) - (E_1 + E_2) + (E_2 + E_3) - \dots - E_{i-2} \\
    &= \Eh_0 - \Eh_1 + \Eh_2 - \dots - E_{i-2}.
  \end{align*}
  Since $\E[\Eh_j | Y_1^{i-1}] = \Eh_j$ for $j = 1$, \dots,~$i-2$, and
  $\E[E_{i-2}|Y_1^{i-1}] = \Eh_{i-2}$, 
  \begin{equation*}
    \E[S|Y_1^{i-1}] = \Eh_0 - \Eh_1 + \dots  - \Eh_{i-2}
  \end{equation*}
  and so $S - \E[S|Y_1^{i-1}] = \Eh_{i-2} - E_{i-2} = E_{i-1}$. Plugging this
  into~\eqref{eq:gausspmenc} yields exactly the encoder~\eqref{eq:gaussfbxi} of
  Example~\ref{ex:gaussfb}.
\end{example}

This example shows that the Gaussian example is nothing but a special case of
posterior matching, with the particular property that the posterior matching
encoder is linear.


\subsubsection{Achieving $R(D)$ using Posterior Matching?}

Using posterior matching,  one can turn an arbitrary source distribution into
the capacity achieving distribution. Can the same trick be used to make the
conditional distribution of~$\Sh$ given~$S$ achieve the rate distortion
function? 

For simplicity assume $n = 1$ (whether there is feedback or not is irrelevant).
For a fixed~$D$, let
\begin{equation*}
  \Phi_s(\sh) = \arg\min_{p(\sh|s): \E[d(S,\Sh)] \le D} I(S;\Sh).
\end{equation*}
\Ie, $\Phi_s(\sh)$ is the conditional distribution of~$\Sh$ given~$S$ that
achieves the rate distortion function at expected distortion~$D$. Let the
decoder be
\begin{equation}
  \label{eq:distmatchdec}
  g(y) = F_{\Phi_s}^{-1}(F_{Y|S=s}(y)).
\end{equation}
The resulting joint distribution of $\Sh = g(Y)$ and~$S$ thus satisfies
$I(S;\Sh) = R(D)$. 

It is immediately clear that this approach cannot work -- both \cdf s needed to
implement this decoder depend on the actual value of~$s$, which is obviously not
known at the decoder (there would not really be a communication problem
otherwise). Interestingly, though, in the Gaussian case the dependence on~$s$ of
$F_{\Phi_s}$ and of $F_{Y|S=s}$ cancel each other out, and the
decoder~\eqref{eq:distmatchdec} yields the MMSE decoder, as the following
example shows.

\begin{example}
  Let the source $S$ be distributed as $\N(0,1)$ and let the channel be AWGN
  with noise variance~$1$ and input constraint $\E[X^2] \le P$.
  The distortion is the squared error. The smallest achievable distortion is
  \begin{equation}
    \label{eq:exmindist}
    D_{\min} = \frac{1}{1 + P}.
  \end{equation}
  The capacity-achieving input distribution is $\N(0,P)$, and the conditional
  distribution of $\Sh$ given $S=s$ that achieves the rate-distortion function
  at distortion~$D$ is $\N((1-D)s, D(1-D))$ (see \eg~\cite{CoverT1991}).
  Let $X = \sqrt{P}S$.  The decoder from~\eqref{eq:distmatchdec} is
  \begin{align*}
    g(y) &= F_{\Phi_s}^{-1} (F_{Y|S=s}(y)) \\
    &= \sqrt{D(1-D)} F_{\N}^{-1} \left( F_{\N}\left( y-\sqrt{P}s
    \right) \right) + (1-D)s \\
    &= \sqrt{D(1-D)} \left( y-\sqrt{P}s \right) + (1-D)s.
  \end{align*}
  This expression still depends on~$s$. If we plug in the optimal distortion
  $D_{\min}$ from~\eqref{eq:exmindist}, however, the decoder becomes
  \begin{align*}
    g(y) &= \frac{\sqrt{P}}{P+1} (y - \sqrt{P}s) + \frac{P}{P +
    1}s \\ 
    &= \frac{\sqrt{P}y}{P + 1},
  \end{align*}
  which no longer depends on~$s$. Furthermore, this decoder is the MMSE decoder.
\end{example}


\section{Lessons from the Case With Feedback}

In the previous sections we have seen that if more than one channel use is
available per source symbol then a Gaussian source can be transmitted optimally
over a Gaussian channel at minimal delay only when the encoder has feedback from
the receiver.  What lessons can we draw from the feedback case that help us in
the case without feedback?

The first observation is that if the encoder knows the state of the receiver
then it can transmit the receiver's current estimation error uncoded, as done in
\exref{gaussfb}. Feedback, however, is not the only way for the encoder to know
the receiver's state. Indeed, assume that the encoder uses a perfect source code
followed by a perfect channel code to communicate the source across the first
$n-1$ channel uses (ignoring delay constraints). Because a perfect channel code
turns the channel into an essentially error-free link, the encoder knows (with
high probability) the decoder's estimate after $n-1$ channel uses, just like
when there is feedback.

Let $\Sh'$ be the receiver's estimate after $n-1$ channel uses. According to the
separation theorem and because we have assumed perfect codes,
the estimation error satisfies $\E[(\Sh' - S)^2] = \ssq / (1 + \snr)^{n-1}$ (up
to an arbitrary $\e > 0$). If the encoder transmits the error $E = \Sh' - S$
uncoded in the $n\th$ channel use and the receiver estimates it as~$\Eh$, the
resulting estimation error satisfies
\begin{align*}
  \E[(\Eh - E)^2] &= \frac{\E[E^2]}{ 1 + \snr } \\
  &= \frac{\ssq}{(1 + \snr)^n}
\end{align*}
(cf.~\exref{gausssingle}).
Letting $\Sh = \Sh' - \Eh$, the overall estimation error is then 
\begin{align*}
  \E[(\Sh - S)^2] &= \E[(\Sh' - (E + (\Eh - E)) - S)^2] \\
  &= \E[(\Eh - E)^2] \\
  &= \frac{\ssq}{(1 + \snr)^n},
\end{align*}
which is the optimal distortion. 

Naturally, if the encoder is only allowed to encode one source symbol at a time
then it is impossible to use a perfect source and channel code in the first
$n-1$ channel uses, since this would require infinite delay. As we will see in
the next chapter, however, if some ``weak'' coding is used for the first $n-1$
transmissions such that the estimation error after the first $n-1$ channel uses
is negligible with respect to that from the $n\th$~channel use (in a sense that
will made precise in the next chapter) then we can always increase the SDR that
would be obtained after $n-1$ channel uses by a factor $\snr$ using uncoded
transmission in the $n\th$ channel use. One way to achieve this is by
transmitting a quantized version of the source in the first $n-1$ channel uses
and then sending the resulting quantization error uncoded in the last channel
use. This idea is at the heart of the communication schemes studied in the next
chapter.


\section{Historical Notes}

The separation theorem goes back to Shannon~\cite{Shannon1948}. Gastpar et
al.~\cite{GastparRV2003} analyzed in details the optimality conditions of
\thmref{optimalityconditions}. That a Gaussian source directly plugged into a
Gaussian channel achieves and optimal cost--distortion tradeoff was first
observed by Goblick~\cite{Goblick1965}.

Ziv and Zakai published their method for alternative bounds, where an
alternative mutual information not based on the logarithm is used, in
1973~\cite{ZivZ1973}. A version of their result was later formulated in the
context of convex optimization by Ben-Tal and Teboulle~\cite{BenTalT1988}. In
2008, Ingber et al.\ used the R\'enyi entropy power measure in place of the
logarithm in the mutual information and showed that at high SNR, the achievable
distortion of a Gaussian source across a Gaussian channel is strictly bounded
away from Shannon's limit when a single source symbol is transmitted in several
channel uses. 

The idea underlying posterior matching is well known and was used by Gastpar and
Rimoldi~\cite{GastparR2003} to develop various examples of optimal uncoded
transmission with feedback.  Later, it was studied in depth by Shayevitz and
Feder in~2007~\cite{ShayevitzF2007,ShayevitzF2008} (who baptized it
\emph{posterior matching}) to generalize the capacity achieving channel coding
schemes of Schalkwijk and Kailath~\cite{SchalkwijkK1966} and
Horstein~\cite{Horstein1963} to arbitrary channels with feedback.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% CHAPTER APPENDICES                                                           %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{subappendices}

\section{Proof of the Converse Part of the Separation Theorem}
\label{app:separationproof}

The proof follows from the following chain of inequalities.
{\allowdisplaybreaks
\begin{align*}
  kR(D) &= k R(\frac1k \sk \E[d(S_i, \Sh_i))] \\
  &\stackrel{(a)}{\le} \sk R(\E[d(S_i, \Sh_i))] \\
  &\stackrel{(b)}{\le} \sk I(S_i; \Sh_i) \\
  &= \sk H(S_i) - H(S_i|\Sh_i) \\
  &= H(S^k) - \sk H(S_i|\Sh_i) \\
  &\stackrel{(c)}{\le} H(S^k) - \sk H(S_i|S^{i-1} \Sh^k) \\
  &=I(S^k;\Sh^k) \\
  &\stackrel{(d)}{\le} I(S^k; Y^n) \\
  &\stackrel{(e)}{\le} I(X^n; Y^n) \\
  &= \sn H(Y_i|Y^{i-1}) - H(Y_i|Y^{i-1} X^n)  \\
  &\stackrel{(f)}{\le} \sn H(Y_i) - H(Y_i|X_i) \\
  &= \sn I(X_i; Y_i) \\
  &\stackrel{(g)}{\le} \sn C(\E \rho(X_i)) \\
  &\stackrel{(h)}{\le} n C(\frac1n \E \rho(X_i)) \\
  &= n C(P).
\end{align*}}%
The inequalities are justified as follows. (a) follows from the convexity$_\cup$
of~$R(D)$ and (b) from its definition. (c) is because
conditioning can only decrease the entropy. (d) and (e) follow from the data
processing inequality. (f) is because conditioning can only decrease entropy and
because the channel is memoryless. Finally, (g) applies by definition and (h) is
due to the convexity$_\cap$ of~$C(P)$.  \hfil\qed


\section{Separation Theorem, Forward Part}\label{app:separationforward}

\begin{theorem}
  \label{thm:separationforward}
  Consider a memoryless source with rate-distortion function~$R(D)$ that
  produces a source symbol every $\ts$~seconds, and memoryless channel with
  capacity-cost function~$C(P)$ that accepts an input symbol for transmission
  every $\tc$~seconds. Then for any $(D,P)$ pair satisfying
  \begin{equation}
    \label{eq:separationforward}
    R(D)/\ts \le C(P)/\tc - \e,
  \end{equation}
  where $\e > 0$, there exists a source code and a channel code that, when
  combined to transmit the source across the channel, result in an average
  distortion of at most~$D$ and an average cost of at most~$P$.
\end{theorem}

\begin{proof}
  Assume $R(D)/\ts \le C(P)/\tc - \e$. According to the source coding theorem
  there exists, for an arbitrary $\e' > 0$, a source code that achieves
  distortion at most~$D$ and produces $R(D) + \e'$ bits per source symbol, or
  $(R(D) + \e')/\ts$ bits per second. According to the channel coding theorem,
  for any $\e' > 0$ there exists a channel code that uses average input
  cost at most~$P$ and can reliably transmit $C(P) - \e'$ bits per channel use
  or $(C(P) - \e')/\tc$ bits per second.

  The output of the source code can be mapped to the input of the channel code
  provided that $(R(D) + \e')/\ts \le (C(P) - \e')/\tc$, 
  or equivalently
  \[ R(D)/\ts \le C(P)/\tc - \e'\left(\frac{1}{\ts} + \frac{1}{\tc} \right). \]
  By choosing $\e'$ small enough, it follows from the assumption on~$D$ and~$P$
  that this inequality holds.
\end{proof}


\end{subappendices}
