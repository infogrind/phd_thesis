\chapter{Fundamentals of Source-Channel Communication}\label{ch:fundamentals}

This thesis is about point-to-point communication of a memoryless source across
a memoryless channel. In its most general form, this problem is made up of six
elements:
\begin{itemize}
  \item A memoryless source with distribution~$\pS$, producing a source symbol
    every $\ts$~seconds.
  \item A memoryless channel with transition distribution~$\pyx$, accepting an
    input symbol for transmission every $\tc$~seconds.
  \item An encoder function~$f$ that maps a block of~$k$ source symbols $S^k
    = (S_1, \dots, S_k)$ into $n$~channel input symbols $X^n = (X_1,
    \dots, X_n)$.
  \item A decoder function~$g$ that maps a block of $n$~channel output symbols
    $Y^n = (Y_1, \dots, Y_n)$ to $k$~source estimates $\Sh^k = (\Sh_1,
    \dots, \Sh_k)$.
  \item A \emph{distortion measure} $d(s,\sh)$ that assigns a distortion to
    every pair of source symbol and estimate, and a \emph{cost measure}
    $\rho(x)$ that assigns a transmission cost to each channel input symbol. 
\end{itemize}
These are the elements that make up a general point-to-point communication
system;  they are displayed schematically in \figref{scgen}. The parameters
$\ts$, $\tc$, $n$, and $k$ are given as part of the problem setting. To match
the number of channel inputs produced by the encoder to the rate at which the
channel accepts them, they must satisfy $n/k = \ts / \tc$. 

If the discrete-time source and channel model an underlying continuous
bandlimited source and channel, respectively, $\ts$ and $\tc$ relate directly to
the respective bandwidths. If $\ts = \tc$, one therefore says that the source
and channel are \emph{bandwidth matched}. Correspondingly, a code is said to be
\emph{bandwidth matched} if $k=n$, to be a \emph{bandwidth expansion} code if $k
< n$, and to be a bandwidth compression code if $k > n$.

\begin{figure}
  \begin{center}
    \input{figures/sc_gen.tex_t}
  \end{center}
  \caption{A general memoryless point-to-point communication system.}
  \label{fig:scgen}
\end{figure}

The source and the channel, together with the encoder and decoder, imply a joint
distribution of the tuple $(S^k, X^n, Y^n, \Sh^k)$.  Depending on the encoder,
the sequence $X^n$ of channel inputs may not be identically distributed; the
average cost can therefore only be defined as the empirical average of the
expected cost of each channel input.

\begin{definition}
  \label{def:avgcost}
  The \emph{average channel input cost} incurred by a memoryless point-to-point
  communication system is
  \begin{equation*}
    P = \sum_{i=1}^n \E[\rho(X_i)].
  \end{equation*}
\end{definition}

Similarly, depending on the encoder and the decoder the sequence $\Sh^k$ of
estimates may not be identically distributed, so that the average distortion
must be defined as the average of the expected distortion of each pair $(S_i,
\Sh_i)$. 

\begin{definition}
  \label{def:avgdist}
  The \emph{average distortion} incurred by a memoryless point-to-point
  communication system is
  \begin{equation*}
    D = \sum_{i=1}^k \E[d(S_i, \Sh_i)],
  \end{equation*}
  where the expectation is taken over the joint distribution of $S_i$
  and~$\Sh_i$.
\end{definition}


\section{Fundamental Limits of Performance}

The most fundamental boundary on the performance of a memoryless point-to-point
communication system as described above is given by the converse part of
Shannon's separation theorem~\cite{Gallager1968}. It is formulated in terms of
the rate-distortion function of the source and the capacity-cost function of the
channel, whose definitions are given first. 

\begin{definition}
  \label{def:capacity}
  The \emph{capacity-cost function} of a channel $\pyx$ with cost
  function~$\rho(x)$ is
  \begin{equation*}
    C(P) = \max_{\px: \E[\rho(X)] \le P} I(X;Y).
  \end{equation*}
\end{definition}

\begin{definition}
  \label{def:ratedistortion}
  The \emph{rate-distortion function} of a source~$\ps$ with distortion
  measure~$d(s,\sh)$ is 
  \begin{equation*}
    R(D) = \min_{\pshs: \E[d(S,\Sh)] \le D} I(S;\Sh).
  \end{equation*}
\end{definition}

The converse part of the separation theorem is then stated as follows.

\begin{theorem}
  \label{thm:separationconverse}
  In any memoryless point-to-point source-channel communication system, the
  average cost~$P$ and the average distortion~$D$ are related by
  \begin{equation}
    \label{eq:separation}
    kR(D) \le nC(P).
  \end{equation}
\end{theorem}

A complete proof of this result is given in \appref{separationproof}. 

This theorem has its name because its forward part (discussed in the next
section) establishes that one can come arbitrarily close to equality in the
bound~\eqref{eq:separation} by performing source coding and channel coding
separately, thus giving an operational meaning to the bound.

The converse part of the separation completely characterizes the region of
achievable $(D,P)$ pairs of a point-to-point source-channel communication
system, regardless of whether separate or joint coding for the source and
channel is used. As its proof in the appendix of this chapter shows, it does not
use the operational meanings of $R(D)$ and $C(P)$.


\section{Optimality Conditions}\label{sec:optimality}

For the purpose of this thesis, the optimality of a communication system is
defined as follows.

\begin{definition}
  \label{def:optimality}
  A \emph{optimal communication system} is either a communication system whose
  average cost and distortion satisfy~\eqref{eq:separation} with equality, or
  the limit of a sequence of systems (with increasing block length) whose
  performance approach equality in~\eqref{eq:separation}.
\end{definition}

This is not the strictest form of optimality and it precludes the existence of
optimal communication systems in some situations, for example when $\max_D R(D)
< \min_P C(P)$. See Gastpar~\cite{GastparThesis} for a more in-depth treatment
of alternative definitions of optimal communication systems.

By going step by step through the inequalities in the proof of
\thmref{separationconverse}, the following result can be established. 

\begin{theorem}
  \label{thm:optimalityconditions}
  A point-to-point memoryless communication system is optimal according to
  \defref{optimality} if and only if the following conditions are all satisfied.
  \begin{enumerate}
    \item For each $i = 1$, \dots, $k$, the joint distribution $p(s_i,\sh_i)$
      achieves the rate-distortion function of the source at the same average
      distortion~$D$.
    \item The \emph{reverse test channel} $p(s^k|\sh^k)$ factors as
      $\prod_{i=1}^k p(s_i | \sh_i)$. 
    \item The encoder is information lossless in the sense that $I(S^k;Y^n) =
      I(X^k; Y^n)$. 
    \item The estimate sequence $\Sh^k$ is a sufficient statistic for $S^k$
      given $Y^n$. (Equivalently, we can say that the decoder must be memoryless
      in the sense that $I(X^n;Y^n) = I(X^n;\Sh^k)$.)
    \item The channel output sequence $Y^n$ is \iid.
    \item The marginal distributions $p(x_i)$ all achieve the capacity at the
      same average cost~$P$.
  \end{enumerate}
\end{theorem}


\subsection{Achieving Optimality Through Separation}

\thmref{separationconverse} applies to all communication systems, regardless of
whether they perform source and channel coding separately or jointly. The
forward part of the theorem, on the other hand, uses separate source and channel
coding to proof the achievability of $(D,P)$ pairs (thus giving the theorem its
name).

\begin{theorem}[Separation theorem, forward part]
  \label{thm:separationforward}
  Consider a memoryless source with rate-distortion function~$R(D)$ and
  capacity-cost function~$C(P)$. Then for any $(D,P)$ pair satisfying
  \begin{equation*}
    kR(D) \le n C(P) - \e,
  \end{equation*}
  where $\e > 0$, there exists a source code and a channel code that, when
  combined to transmit the source across the channel, result in an average
  distortion~$D$ and an average cost~$P$.
\end{theorem}

\begin{proof}

\end{proof}



Show why separation codes achieve optimality. According to Verd\'u and Shamai,
the marginal distribution of the output converges to the capacity achieving
distribution. Also, input sequence becomes ``approximately'' independent. Same
analysis could be made for an optimal source code. (Do they mention this in
their paper?)


\subsection{Achieving Optimality Through Measure Matching}

Separation is not the only way to achieve optimality. Conditions
(rate-distortion function achieved) and (capacity) can be achieved by arbitrary
codes (with possibly short block length), as long as the cost and distortion
measures are suitably matched. This is the subject of Gastpar's thesis.

Other conditions are more difficult to achieve, such as the condition of
independent inputs. It seems difficult to deterministically encode an arbitrary
length~$k$ block of source symbol into $n$~independent channel uses. 
It is possible in some cases, e.g. mapping a uniformly distributed $4$~ary
source into 2 independent binary inputs. But this can perhaps be seen as another
kind of matching. Think about this.


\section{Codes With a Delay Constraint}

The forward part of the separation theorem relies on the source coding theorem
and the channel coding theorem, which assume unrestricted block lengths and
ignore the delay thus caused. This section looks at the consequences that arise
when a constraint is put on the delay incurred by a point-to-point communication
system.

\begin{definition}
  \label{def:delay}
  The \emph{delay} incurred by a point-to-point source-channel communication
  system is the time between the instant a source symbol is produced and the
  instant the receiver has gathered enough channel output symbols to decode that
  symbol. 
\end{definition}

What is the delay of a code that encodes $k$~source symbols into $n$~channel
input symbols? Since the source produces a symbol every $\ts$~seconds it takes
$k\ts$ seconds to gather $k$~source symbols. Assuming that the encoding process
takes place instantaneously, it takes an additional $n\tc$ seconds to transmit
the encoded source sequence across the channel. Neglecting the transmission time
of the channel, the receiver can thus decode the first source symbol after $k\ts
+ n \tc$ seconds, or $2k\ts$ seconds.

\begin{remark}
  \label{rem:blocklength}
  In conventional source coding and channel coding, the term ``large block
  length'' is often used to imply a large delay. What is the block length of a
  joint source-channel code? On one hand, the block length of a source code is
  understood to be the number~$k$ of source symbols encoded at a time. On the
  other hand, the block length of a channel code is the length $n$ of each
  channel input codeword. Rather than attempting to define the block length of a
  joint source-channel code one way or the other, we prefer to avoid the term
  block length altogether in the context of such codes. 
\end{remark}

If we put a limit on the delay a communication system may incur, the forward
part of the separation theorem no longer applies because the assumptions of the
source and channel coding theorems are no longer satisfied. As a consequence it
is not clear whether a communication system can still achieve (or approach) any
$(D,P)$ pair that satisfies $k R(D) \le n C(P)$. 
In fact, the region of achievable cost/distortion pairs is not known under a
delay constraint, not even for the canonical case of a Gaussian source
transmitted across a Gaussian channel -- with the notable exception of the case
$k=n=1$, as we shall see presently.


\section{The Gaussian Case}

The Gaussian source and channel play a special role in information theory not
only because of the significance of the Gaussian distribution due to the central
limit theorem, but also because the Gaussian case allows for analytical
solutions of many information theoretic quantities, most notably the
rate-distortion function (under squared error distortion) and the capacity
(under an average power constraint). In addition, Gaussian noise is the worst
kind of noise under a constraint on the noise variance. On the search for the
achievable cost and distortion pairs under a delay constraint, it is therefore
only natural to start by looking at the Gaussian source and channel. 

\begin{definition}[Gaussian source and channel]
  \label{def:gaussiansc}
  A discrete-time memoryless \emph{Gaussian source} of variance~$\ssq$ is a
  source whose distribution is zero-mean Gaussian with variance~$\ssq$.

  A discrete-time memoryless \emph{Gaussian channel} (also called discrete-time
  additive white Gaussian noise channel, AWGN) with noise variance~$\szq$ is a
  channel whose transition distribution satisfies $p(y|x) \sim \cN(x, \szq)$,
  \ie, given the input~$x$, the output is Gaussian with mean~$x$ and
  variance~$\szq$.
\end{definition}

The distortion measure considered in the sequel is the squared error distortion
$d(s,\sh) = (s - \sh)^2$, and the cost measure is the channel input power
$\rho(x) = x^2$, such that $D = \frac1k \sum_{i=1}^k \E[(S_i - \Sh_i)^2]$ and $P
= \frac1n \sum_{i=1}^n \E[X_i^2]$.

Plugging in the formulas for the rate-distortion function and the capacity-cost
function for the Gaussian source and channel, the bound~\eqref{eq:separation}
can be written as
\begin{equation}
  \label{eq:gausssep}
  \frac{\ssq}{D} \le \left( 1 + \frac{P}{\szq} \right)^{n/k}
\end{equation}
or equivalently
\begin{equation}
  \label{eq:gausssepsdr}
  \sdr \le (1 + \snr)^{n/k},
\end{equation}
where we have defined the \emph{source-to-distortion ratio} $\sdr = \ssq/D$ and
the \emph{signal-to-noise ratio} $\snr = P/\szq$. 


\subsection{When Uncoded Transmission is Optimal}

It is a well known fact that when $\ts = \tc$, plugging a Gaussian source
directly into a Gaussian channel results in an optimal communication system, as
the following example shows. 

\begin{example}
  \label{ex:gausssingle}
  Let the source be zero-mean Gaussian with variance~$\ssq$ and let the channel
  be white Gaussian with noise variance~$\szq$. The encoder is given by $X =
  f(S) = \sqrt{P/\ssq} S$ and the decoder is given by $\Sh = g(Y) = \sqrt{P}
  \ssq Y / (P + \szq)$. Using $Y = X + Z$ it is quickly verified that
  \begin{equation*}
    D = \E[(S - \Sh)^2] = \ssq / (1 + P/\szq),
  \end{equation*}
  which is indeed the optimal distortion according to~\eqref{eq:gausssep}.
\end{example}

The example shows that when $\ts = \tc$, a joint source-channel code with the
smallest possible delay (a single source symbol is encoded at a time) leads to
the same region of achievable $(D,P)$ pairs as for unrestricted delay.

It is instructive to look at how this example satisfies the conditions of
\thmref{optimalityconditions}. First, since $k = n = 1$, conditions~2 and~5 of
the theorem are trivially satisfied. Because the encoder and the decoder are
invertible, conditions~3 and~4 are satisfied as well. More interestingly,
condition~6 is satisfied because the distribution of the source is in fact the
capacity achieving distribution of the channel (up to scaling), so there is no
need for coding to achieve capacity. Similarly, the resulting joint distribution
of $(S, \Sh)$ is the one that achieves the rate-distortion function of the
source.

Is this another particularity of the Gaussian distribution? The answer is no: in
fact, \emph{any} distribution achieves the capacity of a given channel for
\emph{some} way of measuring the channel input cost. For example, the Gaussian
distribution achieves the capacity of a Gaussian channel when the cost
measure has the form $\rho(x) = ax + b$ (for arbitrary constants $a > 0$
and~$b$), which is true for the input power measure used here. The uniform
distribution also achieves the capacity of a Gaussian channel, just for a
different cost measure. In the same way, \emph{any} joint distribution of $(S,
\Sh)$ achieves the rate-distortion function of the source for \emph{some} way of
measuring distortion. This paradigm, called \emph{measure matching}, is made
precise and extensively discussed in~\cite{GastparRV2003}.


\subsection{Bandwidth Expansion}

Does a similarly simple transmission scheme exist for the Gaussian case when
$\ts = n \tc$, where $n > 1$? Unfortunately, no. In fact, the performance of any
code that encodes a single source symbol into $n>1$~channel inputs is strictly
bounded away from the optimum given by~\eqref{eq:gausssep}~\cite{IngberLZF2008}.
The bound given in~\cite{IngberLZF2008} is not necessarily tight, however, so
the region of achievable cost and distortion pairs is not known when $k = 1$ and
$n > 1$. 

While conditions~1 and~6 of \thmref{optimalityconditions} can still be achieved
by a simple linear encoder and a MMSE decoder (as for the case $k = n = 1$),
condition~5 is no longer trivially satisfied when $n > 1$. In fact, the
difficulty in this situation is to deterministically encode one Gaussian source
symbol into $n$~\iid\ Gaussian channel inputs. The result
of~\cite{IngberLZF2008} shows that not all conditions of
\thmref{optimalityconditions} can be simulaneously satisfied when $k = 1$ and $n
> 1$.  This changes, however, if we modify the scenario and allow the encoder
access to perfect feedback from the receiver, as the next section shows.


\subsection{Optimality Through Feedback}

If there is a causal, noiseless feedback link from the receiver to the encoder,
then the $i\th$ channel input symbol can depend on the past channel outputs
$Y_1$, \dots, $Y_{i-1}$, in addition to the source. Because feedback does not
increase the capacity of the channel, the bound of \thmref{separationconverse}
still applies and so do the conditions of \thmref{optimalityconditions}. The big
advantage brought by the feedback, though, is that it permits a simple
transmission scheme that has minimal delay, yet achieves the
bound~\eqref{eq:gausssep} with equality, as the following example demonstrates.

\begin{figure}
  \begin{center}
    \input{figures/sc_gen_feedback.tex_t}
  \end{center}
  \caption{A source-channel communication system where the encoder has access to
  causal noiseless feedback from the receiver.}
  \label{fig:scgenfeedback}
\end{figure}

\begin{example}
  \label{ex:gaussfb}
  In this example, a memoryless Gaussian source of variance~$\ssq$ is
  transmitted across a memoryless Gaussian channel with power constraint~$P$ and
  noise variance~$\ssq$.  Define $E_0 = S$. In the $i\th$ channel use, the
  encoder produces
  \begin{equation}
    \label{eq:gaussfbxi}
    X_i = \sqrt{\frac{P}{\Var E_{i-1}}} E_{i-1}.
  \end{equation}
  Both the receiver and the sender now compute the minimum mean-squared
  error (MMSE) estimator $\Eh_{i-1}$ of $E_{i-1}$ given $Y_i$. The sender then
  computes $E_i = \Eh_{i-1} - E_{i-1}$ and proceeds to the next round.

  After $n$~rounds of transmission, the receiver has $n$~estimates $\Eh_0$
  to~$\Eh_{n-1}$. Using these, it computes the final estimate~$\Sh$ as
  \begin{equation}
    \label{eq:shdecom1}
    \Sh = \Eh_0 - \Eh_1 + \Eh_2 - \cdots \pm \Eh_{n-1}.
  \end{equation}
  (The sign of the last term is $+$ if $n$~is even and $-$ if $n$~is odd.)

  To compute the overall distortion $\mse$, note that $\Eh_{i-1} = E_{i-1} +
  E_i$ by definition, so \eqref{eq:shdecom1}~can be written as
  \begin{align*}
    \Sh &= (E_0 + E_1) - (E_1 + E_2) + (E_2 + E_3) - \cdots \pm (E_{n-1} + E_n)
    \\
    &= E_0 \pm E_n,
  \end{align*}
  and since we have defined $E_0 = S$, we have $\mse = \E[E_n^2]$, where $E_n$
  is the remaining error after the last round of transmission.

  To compute $\E[E_n^2]$, note that since $\Eh_i$ is the MMSE estimator of
  $E_i$, the estimation error variance is given by (see
  \eg~\cite[Section~8.3]{Scharf1990})
  \begin{equation}
    \label{eq:gaussvardec}
    \E[E_{i+1}^2] = \E[(\Eh_i - E_i)^2] = \frac{\E[E_i^2]}{1 + P/\szq}.
  \end{equation}
  Using $\E[E_0^2] = \E[S^2] = \ssq$ and recursively applying the above, we find
  that
  \begin{align*}
    \frac{\ssq}{\E[E_n^2]} &= \frac{\ssq (1 + P/\szq)}{\E[E_{n-1}^2]} \\
    &= \frac{\ssq(1 + P/\szq)^2}{\E[E_{n-2}^2]} = \dots \\
    &= \frac{\ssq(1 + P/\szq)^n}{\E[E_0^2]} = (1 + P/\szq)^n,
  \end{align*}
  which is indeed the largest possible SDR according to~\eqref{eq:gausssepsdr}.
\end{example}

As we have already seen before, achieving the capacity and the rate-distortion
function (conditions~1 and~6 of \thmref{optimalityconditions}) are just a matter
of using the ``right'' cost measure and distortion measure, respectively. The
transmission scheme of \exref{gaussfb} in addition satisfies all other
conditions of \thmref{optimalityconditions}, in particular, it produces a
sequence of independent channel outputs. Again, we may ask if this is just due
to some special property of the Gaussian distribution. And again, the answer is
no: the next section explains how to design a minimal-delay encoder that
produces a sequence of independent, capacity achieving output symbols, for any
channel (and any cost measure).


\subsubsection{Exploiting Feedback Via Posterior Matching}

Example~\ref{ex:gaussfb} showed how a simple transmission scheme can achieve the
optimal distortion for a Gaussian source and channel if noiseless feedback is
available. In the sequel
we show how it is possible to encode one source symbol into $n$~channel inputs
for \emph{any} channel and \emph{any} cost measure (under the condition that the
source is continuous-valued).

Before continuing we prove some properties of cumulative distribution functions
(\cdf s).

\begin{lemma}
  \label{lem:cdfunif}
  Let $X$ be a continuous random variable with density $f(x)$ and \cdf\ $F_X$,
  \ie,
  \begin{equation*}
    F_X(x) = \Pr[X \le x].
  \end{equation*}
  Then the random variable $Y = F_X(X)$ is uniformly distributed on~$[0,1]$.
\end{lemma}

\begin{proof}
  Let $Y = F_X(X)$. Then $\Pr[Y \le y] = \Pr[F_X(X) \le y]$.
  Hence if $y < 0$ then $\Pr[Y \le y] = 0$, and if $y > 1$ then $\Pr[Y \le y] =
  1$.  If $y \in [0,1]$ then
  \begin{align*}
    \Pr[F_X(X) \le y] &= \Pr[X \le F_X^{-1}(y)] \\
    &= \int_{-\infty}^{F_X^{-1}(y)} f(x) dx \\
    &= F_X(F_X^{-1}(y)) = y.
  \end{align*}
  The \cdf\ of~$Y$ is therefore that of a uniform random variable on $[0,1]$.
\end{proof}


\begin{lemma}
  \label{lem:invcdf}
  Let $Y$ be a uniform random variable on~$[0,1]$ and let $F_X$ be the \cdf\ of
  an arbitrary random variable~$X$. If $F_X$~is not invertible, define
  $F_X^{-1}$ with a slight abuse of notation as
  \begin{equation}
    \label{eq:invcdf}
    F_X^{-1}(y) = \sup \{x : F_X(x) \le y\}.
  \end{equation}
  Then the random variable $X' = F_X^{-1}(Y)$ has the same distribution as~$X$.
\end{lemma}

\begin{proof}
  The definition of $F_X^{-1}$ according to~\eqref{eq:invcdf} is such
  that $\Pr[F_X^{-1}(Y) \le x] = \Pr[Y \le F_X(x)]$. Thus,
  \begin{align*}
    \Pr[F_X^{-1}(Y) \le x] &= \Pr[Y \le F_X(x)] \\
    &= \int_0^{F_X(x)} d\xi = F_X(x).
  \end{align*}
\end{proof}

Consider now a channel~$\pyx$ and let $\pi(x)$ be the capacity achieving
distribution at average cost~$P$, \ie, 
\begin{equation*}
  \pi(x) = \arg\max_{p(x): \E[\rho(X)] \le P} I(X;Y).
\end{equation*}
The problem is to encode one source symbol of a continuous-valued source
into~$n$ channel inputs, making use of the feedback.

Let $\Fpi$ be the \cdf\ of the distribution $\pi(x)$, and let $F_S$ be the \cdf\
of the source. In the first channel use, the encoder produces
\begin{equation}
  \label{eq:posteriorx1}
  X_1 = \Fpi^{-1}(F_S(S)),
\end{equation}
where $\Fpi^{-1}$ is the inverse of $\Fpi$ according to~\eqref{eq:invcdf}. By
Lemma~\ref{lem:cdfunif}, if $S$ is continuous then $F_S(S)$ has uniform
distribution on $[0,1]$, and so by Lemma~\ref{lem:invcdf}, $\Fpi^{-1}(F_S(S))$
is a random variable with \cdf\ $\Fpi$.

After $i-1$ rounds of transmission, the encoder knows $y_1$, \ldots, $y_{i-1}$
and can compute the conditional \cdf\ $F_{S|y_1, \ldots, y_{i-1}}$. It then
sends
\begin{equation}
  \label{eq:posteriorxi}
  X_i = \Fpi^{-1}(F_{S|y_1,\dots,y_{i-1}}(S)).
\end{equation}
Again, since $S$ is continuous, $F_{S|y_1, \dots, y_{i-1}}(S)$ is uniform. For
any $y_1$, \ldots, $y_{i-1}$, therefore, \begin{equation*}
  p(x_i|s, y_1, \dots, y_{i-1}) = \pi(x)
\end{equation*}
and so $X_i$ is independent of $Y_1$, \ldots, $Y_{i-1}$. 

Using this strategy the encoder produces an \iid\ sequence of inputs $X_i$
with the capacity achieving distribution~$\pi(x)$, satisfying conditions~5
and~6 of \thmref{optimalityconditions}; condition~3 of the theorem is
trivially satisfied because the encoder is deterministic.

Let us now derive the posterior matching encoder for the communication system of
\exref{gaussfb}.

\begin{example}
  \label{ex:gaussfbpost}
  First, a few properties of Gaussian \cdf s are given. Let $F_{\N(\mu, \sq)}$
  be the \cdf\ of a Gaussian random variable of mean~$\mu$ and variance~$\sq$
  and let $F_\N \deq F_{\N(0,1)}$. Then $F_{\N(\mu,\sq)}(x) =
  F_\N((x-\mu)/\sigma)$. Furthermore, the inverse \cdf\ is 
  \begin{equation*}
    F_{\N(\mu,\sq)}^{-1}(y) = \sigma F_\N^{-1}(y) + \mu.
  \end{equation*}

  Let $\pi(x) = \N(0,P)$. According to~\eqref{eq:posteriorx1}, the first channel
  input is
  \begin{equation*}
    X_1 = \sqrt{P} F_{\N}^{-1}(F_{\N}(S/\sigma_S)) = \sqrt{\frac{P}{\ssq}} S,
  \end{equation*}
  which coincides with~\eqref{eq:gaussfbxi} in \exref{gaussfb} when~$i=1$.

  Given $Y_1$, $S$ is Gaussian with mean $\E[S|Y_1]$ and variance $\Var(S -
  \E[S|Y_1])$. Following~\eqref{eq:posteriorxi}, the second channel input is
  thus
  \begin{align*}
    X_2 &= \sqrt P F_{\N}^{-1} \left( F_{\N} \left( \frac{S - \E[S|Y_1]}
    {\sqrt{\Var(S-\E[S|Y_1])}} \right) \right) \\
    &= \sqrt{P} \frac{S - \E[S|Y_1]}{\sqrt{\Var(S-\E[S|Y_1])}}.
  \end{align*}
  Continuing this way, the $i\th$ channel input is found to be
  \begin{equation}
    \label{eq:gausspmenc}
    X_i = \sqrt{P} \frac{S - \E[S|Y_1^{i-1}]}{\sqrt{\Var(S-\E[S|Y_1^{i-1}])}}.
  \end{equation}
  That this is equal to~\eqref{eq:gaussfbxi} can be seen as follows. In
  \exref{gaussfb}, write
  \begin{align*}
    S &= E_0 + (E_1 - E_1) - (E_2 - E_2) + \dots \pm (E_{i-2} -
    E_{i-2}) \\
    &= (E_0 + E_1) - (E_1 + E_2) + (E_2 + E_3) - \dots - E_{i-2} \\
    &= \Eh_0 - \Eh_1 + \Eh_2 - \dots - E_{i-2}.
  \end{align*}
  Since $\E[\Eh_j | Y_1^{i-1}] = \Eh_j$ for $j = 1$, \dots,~$i-2$, and
  $\E[E_{i-2}|Y_1^{i-1}] = \Eh_{i-2}$, 
  \begin{equation*}
    \E[S|Y_1^{i-1}] = \Eh_0 - \Eh_1 + \dots  - \Eh_{i-2}
  \end{equation*}
  and so $S - \E[S|Y_1^{i-1}] = \Eh_{i-2} - E_{i-2} = E_{i-1}$. Plugging this
  into~\eqref{eq:gausspmenc} yields exactly the encoder~\eqref{eq:gaussfbxi} of
  Example~\ref{ex:gaussfb}.
\end{example}

This example shows that the Gaussian example is nothing but a special case of
posterior matching, with the particular property that the posterior matching
encoder is linear.


\subsubsection{Achieving $R(D)$ using Posterior Matching?}

Using posterior matching,  one can turn an arbitrary source distribution into
the capacity achieving distribution. Can the same trick be used to make the
conditional distribution of~$\Sh$ given~$S$ achieve the rate distortion
function? 

For simplicity assume $n = 1$ (whether there is feedback or not is irrelevant).
For a fixed~$D$, let
\begin{equation*}
  \Phi_s(\sh) = \arg\min_{p(\sh|s): \E[d(S,\Sh)] \le D} I(S;\Sh).
\end{equation*}
\Ie, $\Phi_s(\sh)$ is the conditional distribution of~$\Sh$ given~$S$ that
achieves the rate distortion function at expected distortion~$D$. Let the
decoder be
\begin{equation}
  \label{eq:distmatchdec}
  g(y) = F_{\Phi_s}^{-1}(F_{Y|S=s}(y)).
\end{equation}
The resulting joint distribution of $\Sh = g(Y)$ and~$S$ thus satisfies
$I(S;\Sh) = R(D)$. 

It is immediately clear that this approach cannot work -- both \cdf s needed to
implement this decoder depend on the actual value of~$s$, which is obviously not
known at the decoder (there would not really be a communication problem
otherwise). Interestingly, though, in the Gaussian case the dependence on~$s$ of
$F_{\Phi_s}$ and of $F_{Y|S=s}$ cancel each other out, and the
decoder~\eqref{eq:distmatchdec} yields the MMSE decoder, as the following
example shows.

\begin{example}
  Let the source $S$ be distributed as $\N(0,1)$ and let the channel be AWGN
  with noise variance~$1$ and input constraint $\E[X^2] \le P$.
  The distortion is the squared error. The smallest achievable distortion is
  \begin{equation}
    \label{eq:exmindist}
    D_{\min} = \frac{1}{1 + P}.
  \end{equation}
  The capacity-achieving input distribution is $\N(0,P)$, and the conditional
  distribution of $\Sh$ given $S=s$ that achieves the rate-distortion function
  at distortion~$D$ is $\N((1-D)s, D(1-D))$ (see \eg~\cite{CoverT1991}).
  Let $X = \sqrt{P}S$.  The decoder from~\eqref{eq:distmatchdec} is
  \begin{align*}
    g(y) &= F_{\Phi_s}^{-1} (F_{Y|S=s}(y)) \\
    &= \sqrt{D(1-D)} F_{\N}^{-1} \left( F_{\N}\left( y-\sqrt{P}s
    \right) \right) + (1-D)s \\
    &= \sqrt{D(1-D)} \left( y-\sqrt{P}s \right) + (1-D)s.
  \end{align*}
  This expression still depends on~$s$. If we plug in the optimal distortion
  $D_{\min}$ from~\eqref{eq:exmindist}, however, the decoder becomes
  \begin{align*}
    g(y) &= \frac{\sqrt{P}}{P+1} (y - \sqrt{P}s) + \frac{P}{P +
    1}s \\ 
    &= \frac{\sqrt{P}y}{P + 1},
  \end{align*}
  which no longer depends on~$s$. Furthermore, this decoder is the MMSE decoder.
\end{example}


\section{Lessons from the Case With Feedback}

In the previous sections we have seen that if more than one channel use is
available per source symbol, then a Gaussian source can be transmitted optimally
over a Gaussian channel only when the encoder has feedback from the receiver.
What lessons can we draw from the feedback case that help us in the case without
feedback?

The first observation is that if the encoder knows the state of the receiver
then it can transmit the receiver's current estimation error uncoded, as done in
\exref{gaussfb}. Feedback, however, is not the only way for the encoder to know
the receiver's state. Indeed, assume that the encoder uses a perfect source code
followed by a perfect channel code to communicate the source across the first
$n-1$ channel uses (ignoring delay constraints). Because a perfect channel code
turns the channel into an essentially error-free link, the encoder knows (with
high probability) the decoder's estimate after $n-1$ channel uses, just like
when there is feedback.

Let $\Sh'$ be the receiver's estimate after $n-1$ channel uses. According to the
source and channel coding theorems and because we have assumed perfect codes,
the estimation error satisfies $\E[(\Sh' - S)^2] = \ssq / (1 + \snr)^{n-1}$ (up
to an arbitrary $\e > 0$). If the encoder transmits the error $E = \Sh' - S$
uncoded in the $n\th$ channel use and the receiver estimates it as~$\Eh$, the
resulting estimation error satisfies
\begin{align*}
  \E[(\Eh - E)^2] &= \frac{\E[E^2]}{ 1 + \snr } \\
  &= \frac{\ssq}{(1 + \snr)^n}
\end{align*}
(cf.~\exref{gausssingle}).
Letting $\Sh = \Sh' - \Eh$, the overall estimation error is then 
\begin{align*}
  \E[(\Sh - S)^2] &= \E[(\Sh' - (E + (\Eh - E)) - S)^2] \\
  &= \E[(\Eh - E)^2] \\
  &= \frac{\ssq}{(1 + \snr)^n},
\end{align*}
which is the optimal distortion. 

Naturally, if the encoder is only allowed to encode one source symbol at a time
then it is impossible to use a perfect source and channel code in the first
$n-1$ channel uses, since this would require infinite delay. As we will see in
the next chapter, however, if some ``mild'' coding is used for the first $n-1$
transmissions such that the estimation error after the first $n-1$ channel uses
is negligible with respect to that from the $n\th$~channel use (in a sense to be
made precise) then we can always increase the SDR that would be obtained after
$n-1$ channel uses by a factor $\snr$ using uncoded transmission in the $n\th$
channel use. One way to achieve this is by transmitting a quantized version of
the source in the first $n-1$ channel uses and then sending the resulting
quantization error uncoded in the last channel use. This idea is at the heart of
the communication schemes studied in the next chapter.


\section{Historical Notes}

% Stuff about posterior matching:
The underlying idea is well known and was used by Gastpar and
Rimoldi~\cite{GastparR2003} to develop various examples of optimal uncoded
transmission with feedback.  Later, it was studied in depth by Shayevitz and
Feder in~2007~\cite{ShayevitzF2007,ShayevitzF2008} (who baptized it
\emph{posterior matching}) to generalize the capacity achieving channel coding
schemes of Schalkwijk and Kailath~\cite{SchalkwijkK1966} and
Horstein~\cite{Horstein1963} to arbitrary channels with feedback.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% CHAPTER APPENDICES                                                           %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{subappendices}

\section{Proof of the Converse Part of the Separation Theorem}
\label{app:separationproof}

The proof follows from the following chain of inequalities.
{\allowdisplaybreaks
\begin{align*}
  kR(D) &= k R(\frac1k \sk \E[d(S_i, \Sh_i))] \\
  &\stackrel{(a)}{\le} \sk R(\E[d(S_i, \Sh_i))] \\
  &\stackrel{(b)}{\le} \sk I(S_i; \Sh_i) \\
  &= \sk H(S_i) - H(S_i|\Sh_i) \\
  &= H(S^k) - \sk H(S_i|\Sh_i) \\
  &\stackrel{(c)}{\le} H(S^k) - \sk H(S_i|S^{i-1} \Sh^k) \\
  &=H(S^k) - H(S^k|\Sh^k) \\
  &=I(S^k;\Sh^k) \\
  &\stackrel{(d)}{\le} I(S^k; Y^n) \\
  &= H(Y^n) - H(Y^n|S^k) \\
  &= \sn H(Y_i|Y^{i-1}) - H(Y_i|Y^{i-1} S^k)  \\
  &\stackrel{(e)}{\le} \sn H(Y_i) - H(Y_i|Y^{i-1} S^k) \\
  &\stackrel{(f)}{\le} \sn H(Y_i) - H(Y_i|Y^{i-1} S^k X_i) \\
  &= \sn H(Y_i) - H(Y_i|X_i) \\
  &= \sn I(X_i; Y_i) \\
  &\stackrel{(g)}{\le} \sn C(\E \rho(X_i)) \\
  &\stackrel{(h)}{\le} n C(\frac1n \E \rho(X_i)) \\
  &= n C(P).
\end{align*}}%
The inequalities are justified as follows. (a) follows from the convexity$_\cup$
of~$R(D)$ and (b) from its definition. (c), (e), and (f) are because
conditioning can only decrease the entropy. (d) is the data processing
inequality. Finally, (g) applies by definition and (h) is due to the
convexity$_\cap$ of~$C(P)$.  \hfil\qed

\end{subappendices}
