\chapter{Minimal-Delay Codes for Bandwidth Expansion}

\section{Introduction}

How do you transmit a single source symbol in $n$~channel uses? This problem can
be approached from two different perspectives.  The first perspective is to take
inspiration from the feedback case of \exref{gaussfb}. The second, more
traditional perspective is a geometric analysis of the signal constellation.
This chapter starts with a presentation of the two points of view and shows how
they both lead tho the same tradeoff. Based on this, subsequent sections present
and analyze a concrete transmission scheme. 


\subsection{The Point of View from Feedback}

The lesson from \exref{gaussfb} is that uncoded transmission is asymptotically
optimal if the transmitter knows the receiver's estimate given the previous
channel outputs.  Because of the inherent randomness of uncoded transmission,
however, the transmitter has little chance of guessing the receiver's estimate
correctly after a round of uncoded transmission if there is no feedback.
Nevertheless, if the transmission in the first $n-1$ channel uses was such that
the transmitter has a reasonably good idea of the receiver's estimate (by using
some form of coding, for example) then the transmitter can send the estimation
error uncoded in the last channel use, even if there is no feedback. 

Indeed, suppose that after $n-1$~channel uses the receiver has a preliminary
estimate~$\Sh'$ and suppose the transmitter knows~$\Sh'$. Then it 
can use uncoded transmission to send the error $E \deq \Sh' - S$ in the last
channel use. Upon receiving the corresponding channel output the receiver
computes an estimate $\Eh$ of~$E$ and sets the final estimate of~$S$ to be $\Sh
= \Sh' - \Eh$. This results in the overall error $\Sh - S = \Sh' - E - S + E -
\Eh = E - \Eh$ and the SDR becomes
\begin{equation*}
  \sdr = \frac{\ssq}{\mse} = \frac{\ssq}{\E[(\Sh' - S)^2]} \cdot
  \frac{\E[(\Sh' - S)^2]}{\msee}.
\end{equation*}
The first term on the right hand side is~$\sdr'$, the SDR after $n-1$ channel
uses. The second term, equal to $\E[E^2]/\msee$, is the SDR resulting from the
uncoded transmission of~$E$, which, as seen in \exref{gausssingle}, scales
linearly with the SNR. Hence, the overall SDR scales as $\sdr' \cdot \snr$ and
so uncoded transmission in the last channel use boosts the SDR by a
factor~$\snr$. 

The above argument assumed that the transmitter knew the receiver's estimate
exactly, even if there was no feedback. This is possible only in two cases.
Either the estimate after $n-1$ channel uses is constant regardless of the value
of~$S$, which is not very useful, or some form of coding is used to combat the
unreliability of the channel. 

Suppose a perfect source code combined with a perfect channel code is
used to transmit~$S$ across the first $n-1$~channel uses. This implies a
decomposition of~$S$ as $S = Q + E$, where $Q$~is transmitted error free and
the associated distortion $\E[E^2]$ scales as $\snr^{-(n-1)}$
(cf.~\thmref{sdrub}). If $E$ is transmitted uncoded in the $n\th$~channel use
and the receiver sets $\Sh = Q + \Eh$, the overall SDR is
\begin{equation*}
  \sdr = \frac{\ssq}{\mse} = \frac{\ssq}{\E[E^2]} \cdot \frac{\E[E^2]}{\E[(\Eh -
  E)^2]},
\end{equation*}
which scales as $\snr^{n-1} \cdot \snr = \snr^n$. 

Unfortunately the constraint that a single source symbol must be encoded at a
time prevents the use of a perfect source and channel code. Suppose thus that
the receiver decodes not~$Q$ after $n-1$~channel uses but has only an
estimate~$\Qh$. In that case, the overall estimate is $\Sh = \Qh - \Eh$ and the
overall SDR, assuming $Q$ and~$E$ are estimated separately, is
\begin{equation}
  \label{eq:sdrqesep}
  \sdr = \frac{\ssq}{\mseq + \msee} =
  \frac{\ssq}{\E[E^2]} \cdot \frac{\E[E^2]}{\mseq + \E[(\Eh - E)^2]}.
\end{equation}
If now $\mseq \in O(\msee)$ as $\snr \goesto \infty$, \ie,
if the error in estimating~$\Qh$ is dominated by that of estimating~$E$, then
the SDR scales as
\begin{equation}
  \label{eq:sdrscalingapprox}
  \frac{\ssq}{\E[E^2]} \cdot \frac{\E[E^2]}{\msee}.
\end{equation}
The right hand side is again the SDR from the uncoded transmission of~$E$ and
scales as~$\snr$; overall, the SDR scales thus as $\snr / \E[E^2]$.

There is a tradeoff in how~$Q$ (and~$E$) are chosen as a function of~$S$. If
$Q$~is such that
$\mseq$ decreases too slowly, the approximation~\eqref{eq:sdrscalingapprox}
is not valid and~\eqref{eq:sdrqesep} scales instead as $\ssq / \mseq$, and the
SDR gain from the uncoded transmission is lost.  If, on the other hand, $\mseq$
decreases to fast then $\E[E^2]$ decreases only slowly and the overall SDR,
$\snr / \E[E^2]$, grows only slowly with~$\snr$. It is clear, then, that the
best SDR scaling is obtained when $\mseq$ scales the same as~$\msee$.

The conclusion from all this is that to take advantage of uncoded communication
in the last channel use, error free communication in the first $n-1$~channel
uses is not necessary. All that is needed is that the error from the first
$n-1$~channel uses be dominated by that of the last channel use as $\snr \goesto
\infty$. 


\subsection{The Geometric Point of View}

If $f(s)$ is an encoding function that maps a single source symbol into
$n$~channel inputs, one can gain insights about its performance by studying the
set $\{f(s): s \in \Ss\}$, where $\Ss$~is the support set of the source. This
set is called the \emph{signal curve} or \emph{signal locus} corresponding
to~$f(s)$. A sample signal locus is shown in \figref{nonlinlocus}. Because of
the power constraint, the signal locus must be roughly contained within a sphere
of radius~$\sqrt{P}$.

\begin{figure}
  \begin{center}
    \input{figures/nonlinlocus.tex_t}
  \end{center}
  \caption{The signal locus corresponding to a nonlinear encoder from~$\R$
  to~$\R^2$.}
  \label{fig:nonlinlocus}
\end{figure}

The estimate that minimizes the mean squared error is the conditional mean $\E[S
| Y^n]$. For nonlinear encoders, mathematical evaluation of the corresponding
estimation error is in general hopelessly complicated. A decoder whose
performance is easier to evaluate is the \emph{maximum likelihood} (ML) decoder.
It computes $\sh = \arg\max_{s} f(\y|s)$, where $f(\y|s)$ is the conditional
\pdf\ of~$\Y$ given~$S$. For the AWGN channel, $\Y = f(S) + \Zv$, where $\Zv$ is
circularly symmetric Gaussian noise of variance~$\szq$. In this case $f(\y|s)$
is a decreasing function of $\|\y - f(s)\|$, so the ML decoder can
equivalently be described by $\sh = \arg\min_{s} \|\y - f(s)\|$. It decides thus
for $\sh$ such that $f(\sh)$ is the point on the signal curve closest to~$\y$. 

Suppose now that the transmitted point is $f(s_0)$ and suppose further that
$f(s)$ is differentiable in~$s_0$. Then the signal curve can be linearly
approximated in a small neighborhood around $s_0$ as
\begin{equation}
  \label{eq:linapprox}
  f(s_0 + \Delta) = f(s_0) + \Delta \left. \frac{df(s)}{ds}\right|_{s = s_0}.
\end{equation}
If the noise is small, a valid approximation of the ML estimate is therefore
the projection of the received point onto the tangent through~$f(s_0)$ (see
\figref{tangentproj}). From~\eqref{eq:linapprox}, the estimation error $\Delta =
\sh - s_0$ is then
\begin{equation*}
  \| \Delta \|^2 = \frac{\| f(s_0 + \Delta) - f(s_0) \|^2}
  {\left\| \left. \frac{d f(s)}{ds} \right|_{s = s_0} \right|^2}.
\end{equation*}
The term $\| f(s_0 + \Delta) - f(s_0) \|$ is the length of the noise
vector~$\Zv$ projected onto the tangent through $f(s_0)$
(cf.~\figref{tangentproj}). It is Gaussian with variance~$\szq$, so the average
squared error given $S = s_0$ is approximately
\begin{equation}
  \label{eq:errorlinapprox}
  \E[(\Sh - S)^2 \mid S = s_0] \approx \frac{\szq}{\left\| \left. \frac{df(s)}{ds}
  \right|_{s=s_0} \right\|^2}.
\end{equation}
Expressing $f(s) = \sqrt{P} \ft(s)$, with $\E[\|\ft(S)\|^2] \le 1$, its
derivative is
$df(s)/ds = \sqrt{P} d\ft(s)/s$. The quantity $l(s) \deq d\ft(s)/ds$ is called
the \emph{stretch} of the signal locus. If the stretch does not depend on~$s$,
the overall MSE simplifies to
\begin{equation}
  \label{eq:constantstretch}
  \mse \approx (\snr l^2)^{-1}.
\end{equation}

\begin{figure}
  \begin{center}
    \input{figures/tangentproj.tex_t}
  \end{center}
  \caption{If the noise level is small, the ML estimate can be approximated
  by projecting the noise vector~$\Zv$ onto a tangent through~$f(s_0)$.}
  \label{fig:tangentproj}
\end{figure}

As long as the noise level is small with respect to the distance between
different folds of the signal curve, \eqref{eq:errorlinapprox}~is a valid
approximation and the MSE decreases with growing stretch. It is clear, though,
thath the MSE cannot decrease arbitrarily: a larger stretch implies a longer
signal curve, so the folds of the curve must be placed closer together to
satisfy the power constraint. If the stretch becomes too large, the small noise
assumption will no longer be valid.  On the other hand, if the SDR is to scale
more than linearly with the SNR, then by~\eqref{eq:errorlinapprox} the stretch
must increase with the SNR.

The choice of the stretch represents a tradeoff. A bigger stretch results in a
smaller error, provided that the correct fold of the signal curve is decoded.
But a bigger stretch also increases the probability that the wrong fold of the
curve is decoded. Optimally, thus, the stretch is such that the two kinds of
errors contribute equally to the overall error. 

There are clear parallels to the previous point of view. In the
feedback-inspired discussion, the error from decoding~$Q$ had to be dominated by
that from decoding~$E$ in order for the
approximation~\eqref{eq:sdrscalingapprox} to work. Here, the error due to
decoding the wrong fold of the curve must be dominated by that from decoding the
wrong point on the same fold, in order for the
approximation~\eqref{eq:errorlinapprox} to be valid. Note in particular the
striking similarity between Equations~\ref{eq:sdrscalingapprox}
and~\ref{eq:constantstretch}: both equations express the SDR as the product of
the SNR and a factor that should grow with the SNR but whose growth rate is
limited by the conditions of the underlying approximation. 

Translated to the geometric perspective, the principle that uncoded transmission
be used only in the last channel use means that the signal curve should consist
of parallel straight line segments, aligned along the axis of~$X_n$. This is
illustrated on \figref{hybridlocus}. 

\begin{figure}
  \centerline{%
  \subfloat[$n = 2$]{\label{fig:hybridlocus2}%
  \input{figures/hybridlocus2.tex_t}}
  \hfil
  \subfloat[$n = 3$]{\label{fig:hybridlocus3}%
  \input{figures/hybridlocus3.tex_t}}
  }
  \caption{If uncoded transmission is used only in the last channel use, the
  signal locus consists of parallel straight lines, aligned with the axis
  of~$X_n$. This is shown here for $n = 2$ and $n = 3$, respectively.}
  \label{fig:hybridlocus}
\end{figure}


\subsection{Chapter Organization}

The remainder of this chapter is structured as follows. \secref{scalarquant}
introduces a communication scheme for transmitting a single source symbol across
$n$~channel uses, based on the ideas just discussed. It is shown that the SDR
achieved by this scheme scales as $\snr^n / (\log\snr)^{n-1}$, and that no
scheme of the same type achieves a better SDR scaling. In \secref{latticequant},
the scheme of \secref{scalarquant} is extended to encode \emph{blocks} of
$m$~source symbols into $mn$~channel inputs, using lattice quantizers.  It turns
out that even when powerful high dimensional lattices are used, the SDR for any
finite blocklength~$m$ scales just like when $m = 1$ (although the
multiplicative constants are larger). \secref{genbwexp} extends the scheme of
\secref{scalarquant} in a different way, namely to encode $k$~source symbols
into $n$~channel inputs when $\gcd(k,n) = 1$, \ie, when communcation cannot be
broken down into smaller blocks. For this case the achievable SDR is shown to
scale as $\snr^{n/k}/(\log\snr)^{(n-k)/k}$. 

None of the considered transmission schemes achieves the optimal SDR
scaling~$\snr^n$. Instead, the optimal scaling is divided by a ``penalty
factor'' of $\log\snr$ raised to a power equal to the fraction of channel input
symbols per source symbol used for ``coded'' transmission. Is this phenomenon
particular to the transmission strategy used here or is it due to a more
fundamental limit?



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: SCALAR QUANTIZER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{A Simple Hybrid Transmission Scheme}\label{sec:scalarquant}

Based on the ideas outlined in the introduction of this chapter, this section
introduces a simple communication strategy. It splits a single source symbol
into $n-1$ discrete parts and a continous part (hence the term ``hybrid''), to
be transmitted, respectively, in the first $n-1$ and in the last channel use. 


\subsection{Transmission Strategy}\label{sec:commscheme}

The communication strategy described hereafter is displayed schematically on
\figref{1nencoding}.
\begin{figure}
  \begin{center}
    \input{figures/1nencoding.tex_t}
  \end{center}
  \caption{Schematic display of the encoder described in \secref{commscheme} for
  $n = 4$.}
  \label{fig:1nencoding}
\end{figure}
To encode a single source letter $S$ into $n$~channel input symbols $X_1$,
\dots, $X_n$, we proceed as follows. Define $E_0 = S$ and recursively compute
the pairs $(Q_i, E_i)$ as
\begin{subequations}\label{eq:QEdef}
\begin{align}
  Q_i &= \frac{1}{\beta}\Int(\beta E_{i-1}) \label{eq:QEdefA} \\
  E_i &= \beta (E_{i-1} - Q_i) \label{eq:QEdefB}
\end{align}
\end{subequations}
for $i = 1$, \dots, $n-1$ where $\Int(x)$ is the unique integer~$i$ satisfying
\begin{equation*}
  x \in \left[i - \frac12, i +\frac12\right).
\end{equation*}

The equations~\ref{eq:QEdef} essentially define a \emph{hierarchical
quantization} of the source.  The parameter $\beta \in \NN$ determines the
quantization resolution; the larger~$\beta$ the finer the quantization.
Note that \eqref{eq:QEdef}~implies a partition of the source space into
intervals of length $1/\beta^{n-1}$, as is illustrated in
\figref{sourcepartition}. The $Q_i$ determine the interval that contains~$S$,
and $E_{n-1}$ determines the position of~$S$ within the interval. 

The following result will be useful in the sequel.

\begin{figure}
  \begin{center}
    \input{figures/sourcepartition.tex_t}
  \end{center}
  \caption{Example partition of the source space $[-1/2,1/2]$ for $\beta = 3$
  and $n = 4$. }
  \label{fig:sourcepartition}
\end{figure}



\begin{lemma}
  \label{lem:Qvarbound}
  The variance of $Q_i$ satisfies
  \begin{equation*}
    \Var Q_i \le \Var E_{i-1} + \frac{\sqrt{\Var E_{i-1}}}{\beta} +
    \frac{1}{\beta^2}
  \end{equation*}
  for all $i = 1$, \dots, $n-1$. 
\end{lemma}

\begin{proof}
  See \appref{qvarboundproof}.
\end{proof}

\begin{proposition}
  \label{prop:qeproperties}
  The $Q_i$ and $E_i$ satisfy the following properties:
\begin{enumerate}
  \item The map $S \mapsto (Q_1, \dots, Q_{n-1}, E_{n-1})$ is one-to-one, with
    the inverse given by
    \begin{equation}
      \label{eq:unwraprec}
      S = \sum_{i=1}^{n-1} \frac{1}{\beta^{i-1}} Q_i + \frac{1}{\beta^{n-1}}
      E_{n-1}.
    \end{equation}

  \item There exists a constant~$\gamma > 0$ such that $\Var Q_i \le \gamma^2$
    and $\Var E_i \le \gamma^2$ for all~$i$, regardless of the value of~$\beta$.
\end{enumerate}
\end{proposition}

\begin{proof}
  \begin{enumerate}
    \item From the definition~\eqref{eq:QEdef} we have
    \begin{equation}
      \label{eq:reverserec}
      E_{i-1} = \frac{1}{\beta} E_i + Q_i.
    \end{equation}
    Repeated use of this relationship leads to the given expression for~$S$. 

  \item First, $\Var E_0 = \Var S = \ssq$, which clearly doesn't depend
    on~$\beta$. For $i = 1$, \ldots, $n-1$, $E_i \in [-1/2, 1/2)$ and so its
    variance is upper bounded as well. Finally, since $\beta \ge 1$ and by
    \lemref{Qvarbound}, 
    \begin{align*}
      \Var Q_i &\le \Var E_{i-1} + \sqrt{\Var E_{i-1}} + 1,
    \end{align*}
    which completes the proof.
  \end{enumerate}
\end{proof}

The channel input symbols $X_i$ are determined from the $Q_i$ and from $E_{n-1}$
according to 
\begin{align}
  X_i &= (\sqrt{P}/\gamma) Q_i \quad
  \text{for $i = 1$, \dots, $n-1$ and} \nonumber\\
  X_n &= (\sqrt{P}/\gamma) E_{n-1}.
  \label{eq:Xdef}
\end{align}
Following Proposition~\ref{prop:qeproperties}, this ensures that $\E[X_i^2] \le
P$ for all~$i$. 

\begin{remark}
  \label{rem:betagrow}
  If the quantization resolution $\beta$ is upper bounded as $\snr \goesto
  \infty$, the SDR scales at best as $\snr$, as a simple argument shows. If
  $\beta \le \beta_{\max}$, the entropy of the $Q_i$ is upper bounded, and so is
  the entropy of the $X_i$ for $i = 1$, \ldots, $n$. As $\snr \goesto \infty$,
  the entropy of the $Y_i$ converges to that of the $X_i$, and so for all $i =
  1$, \ldots, $n$, the mutual information $I(X_i; Y_i)$ is upper bounded by a
  constant independent of the SNR. Since the SDR scales at best
  as $2^{I(X^n;Y^n)}$ (see Chapter~\ref{ch:prelim}), this limits the SDR scaling
  to at best linear in the SNR.
\end{remark}


\subsection{Lower Bound on the Squared Error}\label{sec:scalarlowerbound}

To obtain a lower bound on the MSE that holds for all possible decoders, the
obvious thing to do is to assume a minimum mean squared error (MMSE) decoder. As
mentioned in this chapter's introduction, though, the MMSE decoder is in general
hard to evaluate mathematically. This is no different for the scheme at hand.
Fortunately there is a way around the MMSE decoder, devised by Ziv and
Zakai~\cite{ZivZ1969} and later used by Ziv to develop fundamental lower bounds
for the communication of analog sources~\cite{Ziv1970}. By exploiting certain
regularity properties of the encoder, it is in fact possible to lower bound the
MSE by the error probability of binary signaling. In this section, this method
is applied to upper bound the MSE of the transmission scheme introduced in the
previous section regardless of the decoder used. 

\begin{remark}
  \label{rem:betaepswlog}
  Throughout this section we assume $\beta = \lceil \snr^{(1-\e)/2}\rceil$,
  where $\e = \e(\snr)$ is a positive function of~$\snr$. This results in no
  loss of generality, since for an arbitrary positive function~$f$ we can set
  $\e(\snr) = 1-2 \log(f(\snr))/\log\snr$ to get $\beta(\snr) = \lceil f(\snr)
  \rceil$.  Writing $\beta$ in this form will slightly simplify the mathematical
  derivations to follow. Note that we can bound $\beta$ by $\snr^{(1-\e)/2} \le
  \beta \le \snr^{(1-\e)/2} + 1$, which implies $\beta \in
  \Theta(\snr^{(1-\e)/2})$ (the $\Theta$-notation is defined in
  \appref{asymptotic}).
\end{remark}

\begin{remark}
  \label{rem:functionnotation}
  Note that by~\eqref{eq:QEdef} the $Q_i$ are completely determined by~$S$.
  With a slight abuse of notation, $Q_i(s)$ is therefore used in the sequel to
  denote the value of~$Q_i$ when $S = s$. $E_i(s)$ and $X_i(s)$ are defined in
  an analogous manner. Furthermore, $\X(s) \deq (X_1(s), \dots, X_n(s))$.
\end{remark}

The following result, adapted from Ziv~\cite{Ziv1970}, is a key ingredient in
the proofs of the lemmas that follow.

\begin{lemma}
  \label{lem:zivbound}
  Consider a communication system where a con\-tin\-u\-ous-valued source~$S$ is
  encoded into an $n$-dimensional vector $\X(S)$, sent across $n$~independent
  parallel AWGN channels with noise variance~$\szq$, and decoded at the receiver
  to produce an estimate~$\Sh$.  If the density $p_S$ of the source is such that
  there exists an interval $[A,B]$ and a number $p_{\min} > 0$ such that $p_S(s)
  \ge p_{\min}$ whenever $s \in [A,B]$, then for any $\Delta \in [0,B-A)$ the
  mean squared error incurred by the communication system satisfies
  \begin{equation}
    \label{eq:zivbound}
    \E[(\Sh - S)^2] \ge p_{\min} \left(\frac{\Delta}{2} \right)^2 
    \int_A^{B-\Delta} Q(d(s, \Delta) / 2 \sz) ds,
  \end{equation}
  where $d(s, \Delta) \deq \|\vect{X}(s) - \vect{X}(s+\Delta)\|$ and 
  \[Q(x) = \int_x^{\infty} (1/\sqrt{2\pi}) \exp\{-\xi^2/2\} d\xi.\]
\end{lemma}

\begin{proof}
  See Appendix~\ref{app:zivboundproof}.
\end{proof}

The next two lemmata provide two different asymptotic lower bounds on the
mean squared error of the transmission strategy considered, each of which is
tighter for a different class of~$\e$. They hold regardless of the decoder used.
(The $\Omega$-notation is defined in Appendix~\ref{app:asymptotic}.)

\begin{lemma}
  \label{lem:lowerbound1}
  For an arbitrary function $\e(\snr) \ge 0$, the mean squared error satisfies
  \begin{equation*}
    \mse \in \Omega(\snr^{-n + (n-1)\e}).
  \end{equation*}
\end{lemma}

\begin{lemma}
  \label{lem:lowerbound2}
  For an arbitrary function $\e(\snr) \ge 0$, the mean squared error
  satisfies
  \begin{equation*}
    \mse \in \Omega(\snr^{-1+\e/2} \exp\{-c\snr^\e\}),
  \end{equation*}
  where $c>0$ does not depend on~$\snr$.
\end{lemma}

\emph{Discussion:} An immediate consequence of the lemmata is that the
theoretically optimal scaling $\snr^{-n}$ is not achievable with the given
encoding strategy: by Lemma~\ref{lem:lowerbound1} this would require $\e = 0$,
but following Lemma~\ref{lem:lowerbound2} the scaling is at best $\snr^{-1}$ if
$\e = 0 $.  More generally, which one of the two lower bounds decays more slowly
and is therefore tighter depends on the scaling of~$\e(\snr)$. How to
choose~$\e(\snr)$ optimally will be the subject of Theorem~\ref{thm:scalinglb}.

\begin{proof}[Proof of Lemma~\ref{lem:lowerbound1}]
  Assume $\Delta \in [0, \beta^{-(n-1)})$ and define for $j \in \Z$
  \[ \I_j^\Delta = \left[ (j - \frac12 )\beta^{-(n-1)}, 
    (j + \frac12 ) \beta^{-(n-1)} - \Delta \right).\]
  It can be verified from~\eqref{eq:QEdef} that if $s \in \I_j^\Delta$ for
  some~$j$, the following properties hold: 1) $Q_i(s) = Q_i(s+\Delta)$ for
  $i=1$, \dots, $n-1$, and 2) $E_{n-1}(s+\Delta) - E_{n-1}(s) =
  \beta^{n-1}\Delta$.  From~\eqref{eq:Xdef} it follows that
  $s \in \I_j^\Delta$ implies $d(s, \Delta) = \sqrt{P/\gamma^2} \beta^{n-1}\Delta$.

  We now apply Lemma~\ref{lem:zivbound} and restrict the integral to the
  set~$\psi(\Delta) \deq [A,B-\Delta) \cap \bigcup_{j\in\Z} \I_j^\Delta$. The
  lower bound is then relaxed to give
  \begin{equation*}
    \mse \ge \frac{\pmin}{4} \Delta^2 Q(\sqrt{\snr/\gamma^2} \beta^{n-1}
    \Delta/2) \int_{\psi(\Delta)} ds.
  \end{equation*}
  Letting $\Delta = 1/(\sqrt{\snr}\beta^{n-1})$ and using $\beta^2 \in
  \Theta(\snr^{1-\e})$ yields (for sufficiently large~$\snr$)
  \begin{equation*}
    \mse \ge c \snr^{-n+(n-1)\e} Q\left(1/2\gamma\right)
    \int_{\psi(\Delta)} ds.
  \end{equation*}

  The proof is almost complete, but we still have to show that
  $\int_{\psi(\Delta)}ds$ can be bounded below by a constant for large SNR. The
  length of a single interval~$\I_j^\Delta$ is $\beta^{-(n-1)} - \Delta$. Within
  $[A,B-\Delta)$ there are $(B-A-\Delta)\beta^{n-1}$ such
  intervals. The total length of all intervals~$\I_j^\Delta$ in $[A, B-\Delta)$
  is therefore
  \[ \int_{\psi(\Delta)} ds = (B-A-\Delta)
  (1 - \beta^{n-1}\Delta), \]
  which, for the given values of~$\beta$ and~$\Delta$, 
  converges to $B-A$ for $\snr \ra \infty$ and thus can be lower bounded by a
  constant for $\snr$ greater than some $\snr_0$. With this, the proof is
  complete.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:lowerbound2}]
  Observe first that~\eqref{eq:QEdef} implies $Q_1(s + \beta^{-1}) = Q_1(s) +
  \beta^{-1}$ and $E_1(s + \beta^{-1}) = E_1(s)$. Since all $Q_i$ and $E_i$ for
  $i \ge 2$ are by recursion a function of $E_1$ only, $Q_i(s) = Q_i(s +
  \beta^{-1})$ for $i = 2$, \dots, $n-1$, and $E_{n-1}(s) = E_{n-1}(s +
  \beta^{-1})$. Consequently,  $X_i(s) = X_i(s + \beta^{-1})$ for all $i =
  2$, \dots, $n$. By~\eqref{eq:Xdef} and the above, the Euclidean distance
  between $\X(s)$ and~$\X(s+\beta^{-1})$ is therefore
  \begin{equation}
    \label{eq:xbetadist}
    \frac{\sqrt P}{\gamma} |Q_1(s) - Q_1(s+\beta^{-1})| 
    = \frac{\sqrt P}{\gamma\beta}.
  \end{equation}

  We now apply Lemma~\ref{lem:zivbound} with $\Delta = \beta^{-1}$. The
  parameter $\beta$ will be chosen to increase with the SNR, therefore $\Delta
  \in [0, B-A)$ holds for sufficiently large values of~$\snr$.

  Using~\eqref{eq:xbetadist}, the resulting bound on the mean squared error is
  \begin{equation*}
    \mse \ge \frac{\pmin}{4} \beta^{-2}
    Q\left (\frac{\sqrt{\snr}}{2\gamma\beta}  \right) (B-A-\beta^{-1}
    ).
  \end{equation*}
  Because $\beta^2 \in \Theta(\snr^{1-\e})$ (see \remref{betaepswlog}),
  $\sqrt{\snr}/\beta \in \Theta(\snr^{\e/2})$. If $\e(\snr)$ is such that
  $\limtoinf{\snr} \snr^{\e(\snr)} = \infty$, use the fact that $Q(x)$~converges
  to $e^{-x^2/2}/\sqrt{2\pi}x$ (cf.~\cite[\Spg 26.2.12]{AbramowitzS1964}).
  Otherwise $\snr^{\e(\snr)}$ is upper bounded by a constant, in which case
  $Q(x)$ is lower bounded and so is $e^{-x^2/2}/x$ for $x = \snr^{\e(\snr)}$.
  In any case, for sufficiently large values of~$\snr$,
  \begin{equation*}
    \mse \ge c_1 \snr^{-1 + \e/2} \exp\{-c_2\snr^\e\},
  \end{equation*}
  with $c_1$ and $c_2$ positive constants that do not depend on~$\snr$, thus
  proving the lemma.
\end{proof}

The following lemma will be used to prove Theorem~\ref{thm:scalinglb}, the main
result of this section.

\begin{lemma}
  \label{lem:epssolution}
  Define $W(x)$ to be the function that satisfies $W(x)e^{W(x)} = x$ for $x >
  0$.  This function is well defined and is sometimes called the \emph{Lambert
  $W$-function}~\textnormal{\cite{CorlessGHJK1996}}. Then for $\snr > 1$ and
  arbitrary real constants $a$, $b>0$, and $c > 0$, 
  \begin{equation}
    \label{eq:epsequation}
    \snr^{a+b\e} = \exp\{-c\snr^\e\},
  \end{equation}
  if and only if
  \begin{equation}
    \label{eq:epssolution}
    \snr^\e = (b/c) W(c\snr^{-a/b} / b).
  \end{equation}
\end{lemma}

\begin{proof}
  Let $\snr>1$. Since $\snr^{a+b\e}$ is strictly increasing and
  $\exp\{-c \snr^\e\}$ is strictly decreasing in~$\e$, there is at most one
  solution to~\eqref{eq:epsequation}.  Assume now $\snr^\e$ is as
  in~\eqref{eq:epssolution}. Then
  \begin{equation*}
    \exp\{-c \snr^\e\} = \exp\{-b W(c \snr^{-a/b}/b)\}.
  \end{equation*}
  On the other hand,
  \begin{align*}
    \snr^{a+b\e} &= \snr^a \left( (b/c) W(c\snr^{-a/b}/b) \right)^b \\
    &= \left( W(c\snr^{-a/b}/b) / (c\snr^{-a/b}/b) \right)^b.
  \end{align*}
  By definition, $W(x)/x = e^{-W(x)}$, so the above is equal to
  \begin{equation*}
    \snr^{a+b\e} = \exp\{-bW(c \snr^{-a/b}/b)\},
  \end{equation*}
  which proves the claim.
\end{proof}

The following is the main result of this section.
\begin{theorem}
  \label{thm:scalinglb}
  For any parameter~$\beta$ and for any decoder, the mean squared error of the
  transmission strategy described in Section~\ref{sec:commscheme} satisfies
  \begin{equation*}
    \mse \in \Omega(\snr^{-n}(\log\snr)^{n-1}).
  \end{equation*}
\end{theorem}

% [Marius] Removed the following paragraph, since in this chapter the
% achievability result comes _after_ the bound.
%\emph{Discussion:} The asymptotic lower bound on the mean squared error given
%by the theorem coincides with the asymptotic performance achieved by the
%suboptimal decoder in Section~\ref{sec:achievable}; the bound is therefore
%asymptotically tight. 

\begin{proof}
  For notational simplicity define
  \begin{align*}
    l_1(\snr, \e) &= \snr^{-n+(n-1)\e} \quad\text{and} \\
    l_2(\snr,\e) &= \snr^{-1+\e/2} \exp\{-c\snr^\e\}.
  \end{align*}
  By Lemmata~\ref{lem:lowerbound1} and~\ref{lem:lowerbound2},
  \begin{equation*}
    \mse \in \Omega\big(\max \left( l_1(\snr,\e), l_2(\snr,\e) \right) \big).
  \end{equation*}
  The optimal parameter $\e(\snr)$ is therefore such that for
  any~$\snr$
  \begin{equation}
    \label{eq:epsmax}
    \max\left( l_1(\snr,\e), l_2(\snr,\e) \right)
  \end{equation}
  is minimized. Now for any fixed~$\snr$, $l_1(\snr,\e)$ is increasing in~$\e$,
  and $l_2(\snr,\e)$ is increasing in~$\e$ for $0 \le \e < \xi =
  \log(1/2c)/\log\snr$ and decreasing in~$\e$ for $\e \ge \xi$.  The maximum
  in~\eqref{eq:epsmax} is therefore minimized either for $\e = 0$ or for $\e \ge
  \xi$ such that $l_1(\e) = l_2(\e)$. As remarked before, $\e = 0$ leads to
  $\mse \in \Omega(\snr^{-1})$. In the following, let thus $\e(\snr)$ be such
  that $l_1(\snr,\e) = l_2(\snr, \e)$, to see whether this gives a better lower
  bound.  Inserting the definitions of $l_1$ and $l_2$ and rearranging the terms
  yields
  \begin{equation*}
    \snr^{-(n-1) + (n-3/2)\e} = \exp\{-c\snr^\e\},
  \end{equation*}
  which is of the form~\eqref{eq:epsequation} with $a = -(n-1)$ and $b = n-3/2$.
  By Lemma~\ref{lem:epssolution}, for $\snr > 1$,
  \begin{equation*}
    \snr^\e = \frac{n-3/2}{c}
    W\left( \frac{c\snr^{\frac{2(n-1)}{2n-3}}}{n-3/2} \right).
  \end{equation*}
  Using L'H\^opital's rule and because the derivative of
  $W(x)$ is $W(x)/[x(1 + W(x))]$ (cf.~\cite{CorlessGHJK1996}), it is
  straightforward to check that $W(x)/\log x$ converges to~$1$ for $x \ra
  \infty$.  For sufficiently large $\snr$, therefore, there exists a constant
  $c_1 > 0$ such that
  \begin{equation*}
    \snr^\e \ge c_1 \frac{n-3/2}{c} \left[ \frac{2(n-1)}{2n-3}\log\snr -
    \log\left(\frac{n-3/2}{c}\right)
    \right],
  \end{equation*}
  and so $\snr^\e \in \Omega(\log\snr)$. Plugging this into the bound of
  Lemma~\ref{lem:lowerbound1} finally results in\footnote{If $a(x) \in
  \Omega(f(x))$ and $b(x) \in \Omega(g(x))$, then $a(x)b(x)^m \in
  \Omega(f(x)g(x)^m)$.}
  \begin{equation*}
    \mse \in \Omega(\snr^{-n}(\log\snr)^{n-1}),
  \end{equation*}
  and no choice of $\e(\snr)$ can improve this bound.
\end{proof}


\subsection{Asymptotical Achievability of Lower Bound}
\label{sec:achievable}

The previous section showed that the hybrid transmission strategy studied here
achieves an SDR scaling of at best $\snr^n/(\log\snr)^{n-1}$. Using a simple
decoder, this scaling is in fact achievable, as will now be shown.

\subsubsection{A Suboptimal Decoder}

The $X_i$ are transmitted across the channel, producing at the channel output
the symbols
\begin{equation*}
  Y_i = X_i + Z_i, \quad i = 1, \dots, n,
\end{equation*}
where the $Z_i$ are iid Gaussian random variables of variance~$\szq$. 
To estimate $S$ from  $Y_1$, \dots, $Y_n$, the decoder first
computes separate estimates $\Qh_1$, \dots, $\Qh_{n-1}$ and $\Eh_{n-1}$, and
then combines them to obtain the final estimate~$\Sh$.  While this strategy is
suboptimal in terms of achieving a small MSE, it will turn out to be good enough
to achieve the desired SDR scaling.

The $Q_i$ are estimated using a maximum likelihood (ML) decoder, which yields
the minimum distance estimate
\begin{equation}
  \label{eq:mldecoder}
  \Qh_i = \frac{1}{\beta} \arg \min_{j\in \Z} \left| \frac{j\sqrt{P}}
  {\gamma\beta} - Y_i \right|.
\end{equation}
To estimate $E_{n-1}$, a linear minimum mean-square error (LMMSE)
estimator is used (see \eg~\cite[Section~8.3]{Scharf1990}), which computes
\begin{equation}
  \label{eq:lmmse}
  \Eh_{n-1} = \frac{\E[E_{n-1} Y_n]}{\E[Y_n^2]} Y_n.
\end{equation}
Finally, using~\eqref{eq:unwraprec}, the estimate of~$S$ is computed as
\begin{equation}
  \label{eq:unwrapestim}
  \Sh = \sum_{i=1}^{n-1} \frac{1}{\beta^{i-1}} \Qh_i + \frac{1}{\beta^{n-1}}
  \Eh_{n-1}.
\end{equation}


\subsubsection{Error Analysis}

The overall MSE $\E[(S-\Sh)^2]$ can be broken up into contributions due to the
errors in decoding $Q_i$ and $E_{n-1}$ as follows. From~\eqref{eq:unwraprec}
and~\eqref{eq:unwrapestim}, the difference between $S$ and $\Sh$ is
\begin{equation*}
  S - \Sh = \sum_{i=1}^{n-1} \frac1{\beta^{i-1}} (Q_i - \Qh_i) + \frac1{\beta^{n-1}}
  (E_{n-1} - \Eh_{n-1}).
\end{equation*}
The error terms $Q_i - \Qh_i$ depend only on the noise of the respective channel
uses and are therefore independent of each other and of $E_{n-1} - \Eh_{n-1}$,
so the error variance can be written componentwise as
\begin{equation}
  \label{eq:totalerror}
  \E[(S-\Sh)^2] = \sum_{i=1}^{n-1} \frac{1}{\beta^{2(i-1)}} \Eqi +
  \frac{1}{\beta^{2(n-1)}} \Ee, 
\end{equation}
where $\Eqi \deq \E[(Q_i - \Qh_i)^2]$ and $\Ee \deq \E[(E_{n-1} -
\Eh_{n-1})^2]$.

\begin{lemma}
  \label{lem:eqbound}
  For each $i = 1$, \dots, $n-1$, 
  \begin{equation}
    \label{eq:eqidecay}
    \Eqi \in O\left(\exp\{-c \snr/\beta^2\}\right),
  \end{equation}
  where $\snr = P/\szq$ and $c > 0$~is a constant.
\end{lemma}
(The $O$-notation is defined in Appendix~\ref{app:asymptotic}.)

\begin{proof}
  Define the interval
  \begin{equation*}
    \I_j = \left[ \frac{(j - \frac12) \sqrt{P}}{\gamma\beta},
    \frac{(j + \frac12) \sqrt{P}}{\gamma\beta } \right).
  \end{equation*}
  According to the minimum distance decoder~\eqref{eq:mldecoder}, $\Qh_i - Q_i
  \le j/\beta$ whenever $Z_i \in \I_j$.  The error $\Eqi$ satisfies thus
  \begin{align}
    \E[(Q_i - \Qh_i)^2] &\le \frac{1}{\beta^2} \sum_{j \in \Z} j^2 \Pr[Z_i \in
    \I_j]  \nonumber \\
    &= \frac{2}{\beta^2} \sum_{j = 1}^\infty j^2 \Pr[Z_i \in \I_j],
    \label{eq:eqexact}
  \end{align}
  where the second equality follows from the symmetry of the distribution
  of~$Z_i$. Now,
  \begin{equation*}
    \Pr[Z_i \in \I_j] = Q\left( \frac{(j - \frac12) \sqrt{\snr}}{\gamma\beta}
    \right) - Q\left( \frac{(j + \frac12) \sqrt{\snr}}{\gamma\beta } \right),
  \end{equation*}
  where
  \begin{equation*}
    Q(x) = \frac{1}{\sqrt{2\pi}} \int_x^\infty e^{-\xi^2/2} d\xi,
  \end{equation*}
  which can be bounded from above for $x \ge 0$ as
  \begin{equation*}
    Q(x) \le \frac12 e^{-x^2/2}.
  \end{equation*}
  Since $\beta \ge 1$, \eqref{eq:eqexact} is then upper bounded by
  \begin{equation*}
    \Eqi \le \sum_{j=1}^\infty j^2 \exp\left\{ - \frac{(j - 1/2)^2
    \snr}{2\gamma^2\beta^2} \right\}.
  \end{equation*}
  Note that for $j \ge 2$, $(j - 1/2)^2 > j$.  Thus
  \begin{eqnarray}
    \Eqi &\le & \exp \left \{ - \frac{\snr}{8 \gamma^2 \beta^2} \right\}
    \nonumber \\
    & & \mbox{} + 
    \sum_{j = 2}^\infty j^2 \exp \left \{ - \frac{j \snr}{2\gamma^2 \beta^2}
    \right\}. \label{eq:eqibound}
  \end{eqnarray}
  To bound the infinite sum, use 
  \begin{equation}
    \label{eq:geomsum}
    \sum_{j=2}^\infty j^2 p^j \le \sum_{j=1}^\infty j^2 p^j = 
    \frac{p^2+p}{(1-p)^3}
  \end{equation}
  with $p = \exp\{-\snr/2 \gamma^2 \beta^2\}$. The first term
  of~\eqref{eq:eqibound} thus dominates for large values of
  $\snr/\beta^2$ and
  \begin{equation*}
    \Eqi \le c_1\exp\left\{ - \frac{\snr}{c_2 \beta^2} \right\}
  \end{equation*}
  for some~$c_1 > 0$ and $c_2 = 8 \gamma^2$, which completes the proof. 
\end{proof}

\begin{lemma}
  \label{lem:eedecay}
  $\Ee \in O(\snr^{-1})$. 
\end{lemma}
\begin{proof}
  The mean-squared error that results from the LMMSE estimation~\eqref{eq:lmmse}
  is
  \begin{equation}
    \label{eq:lmmse-error}
    \Ee = \seq - \frac{(\E[E_{n-1}
    Y_n])^2}{\E[Y_n^2]}. 
  \end{equation}
  Since
  \begin{equation*}
    Y_n = X_n + Z_n = \frac{\sqrt{P}}{\gamma} E_{n-1} + Z_n,
  \end{equation*}
  we have $\E[E_{n-1}Y_n] = \sqrt{P}\seq/\gamma$. Moreover, $\E[Y_n^2] = \E[X^2]
  + \E[Z^2] = P\seq / \gamma^2 +\szq$.  Inserting this
  into~\eqref{eq:lmmse-error} we obtain
  \begin{align*}
    \Ee &= \seq - \frac{P \se^4/\gamma^2}{P\seq/\gamma^2 + \szq} \\
    &= \seq \left( 1 - \frac{P\seq/\gamma^2}{P\seq/\gamma^2 + \szq} \right) \\
    &= \frac{\seq}{1 + \snr\seq/\gamma^2} \\
    & < \frac{\gamma^2}{\snr}.
  \end{align*}
  Since $\gamma$ is independent of~SNR (cf.\
  Proposition~\ref{prop:qeproperties}), $\Ee \in O(\snr^{-1})$ as claimed.
\end{proof}


\subsubsection{Optimizing the Quantization Resolution}

Recall the formula for the overall error
\begin{equation*}
  \E[(S-\Sh)^2] = \sum_{i=1}^{n-1} \frac{1}{\beta^{2(i-1)}} \Eqi +
  \frac{1}{\beta^{2(n-1)}} \Ee.
\end{equation*}
According to Lemma~\ref{lem:eqbound}, $\Eqi$ decreases exponentially
when $\snr/\beta^2$ goes to infinity. This happens for increasing SNR if
$\beta$~is set \eg\ to
\begin{equation*}
  \beta = \lceil \snr^{(1-\e)/2} \rceil
\end{equation*}
for some $\e > 0$, in which case $\Eqi \in O\left(\exp(-c \snr^\e) \right)$.
From this and Lemma~\ref{lem:eedecay}, the overall error satisfies
\begin{equation}
  \label{eq:overallO}
  \E[(S-\Sh)^2] \in O(\snr^{-(n - \e')}),
\end{equation}
where $\e' = (n-1)\e$ can be made as small as desired.
%The scaling exponent for
%a fixed $\e$ satisfies therefore
%\begin{equation}
%  \label{eq:sdrepsilon}
%  \limtoinf{\snr} \frac{\log\sdr}{\log\snr} \ge
%  \limtoinf{\snr} \frac{\log \ssq + (n - \e') \log\snr}{\log \snr} = n - \e'. 
%\end{equation}

Note that the choice of $\e$ represents a tradeoff: for small $\e$ the error due
to the ``discrete'' part vanishes only slowly, but the scaling exponent in the
limit is larger. For larger $\e$, $\Eq$ vanishes quickly but the resulting
exponent is smaller. The remainder of this section shows how to choose $\e$ as a
function of~$\snr$ to optimize SDR scaling. 

Let
\begin{equation}
  \label{eq:esnrdecay}
  \e = \e(\snr) = \frac{\log(n \log\snr / c)}{\log\snr},
\end{equation}
where $c$~is the constant indicating the decay of $\Eqi$ in~\eqref{eq:eqidecay}.
With this choice of $\e$,
\begin{align*}
  \Eqi &\in O\left( \exp\left( - c \snr^\e \right) \right) \\
 % &= O\left( \exp\left( - \exp\left( \e \log \snr \right) \right) \right) \\
 &= O(\snr^{-n}),
\end{align*}
hence the overall error is still dominated as in~\eqref{eq:overallO}.
Inserting~\eqref{eq:esnrdecay} in~\eqref{eq:overallO},
\begin{equation*}
  \mse \in \snr^n (\log\snr)^{-(n-1)},
\end{equation*}
which coincides with the lower bound on the MSE scaling of \thmref{scalinglb}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: LATTICE QUANTIZERS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Encoding Blocks of Source Symbols}\label{sec:latticequant}

The scalar quantizer scheme described in the previous section can be extended
quite easily to treat blocks of $m$ source symbols and to use lattice
quantizers. For those unfamiliar with lattices or in need of a refresher, a
concise presentation of the necessary concepts and results can be found in
\appref{latticebasics}.

To anticipate the conclusion of this section, it turns out that for no
finite~$m$ a scaling better than that for $m=1$ can be obtained. Choosing a high
dimensional lattice does result, however, in larger (and thus better)
multiplying constants.


\subsection{Transmission Strategy}

The procedure to encode a vector~$\S$ of $m$~source symbols into $mn$~channel
input vectors $\X_1$, \ldots, $\X_n$ using the lattice~$\Lambda$ for
quantization is analog to that in \secref{scalarquant}, except that now all
involved quantities are $m$-dimensional vectors.

Let $\Lambda$ be some fixed lattice of dimension~$m$ and define $\Evc_0 = \S$.
For $i = 1$, \ldots, $n-1$ define
\begin{align}
  \Q_i &= \QLb(\Evc_{i-1})  \nonumber \\
  \Evc_i &= \beta ( \Evc_{i-1} - \Q_i), \label{eq:latticeQE}
\end{align}
where $\beta \in \NN$. According to \lemref{latquantvar}, $\E[\|\Q_i\|^2]$~is
upper bounded by a constant independent of~$\beta$. Moreover, $\Evc_i$
is contained within the Voronoi region of~$\Lambda$ around the origin, so by
\lemref{voronoivarbound} $\E[\|\Evc_i\|^2]$ is also upper bounded independent
of~$\beta$. Let $\gamma$~denote the upper bound on~$\E[\|\Q_i\|^2]$
and~$\E[\|\Evc_i\|^2]$.  The channel inputs are then given by
\begin{align*}
  \X_i &= \frac{\sqrt{mP}}{\gamma} \Q_i \quad \text{for $i = 1$, \dots, $n-1$
  and} \\
  \X_n &= \frac{\sqrt{mP}}{\gamma} \Evc_{n-1}.
\end{align*}
By the above argument, this ensures that $\E[\X_i^2]/m \le P$ for all~$i$.



\subsection{Error Lower Bound}

Ziv's lower bound on the mean squared error, used in \secref{scalarquant},
allows a straightforward extension to vector sources. With this, essentially the
same argument sequence as in \secref{scalarquant} can be used to lower bound the
squared error, independent of the particular decoder used.
(We again write the quantization resolution in terms of~$\snr$
as $\beta = \lceil \snr^{(1-\e)/2} \rceil$ without loss of generality
(cf.~\remref{betaepswlog}).)

\begin{lemma}
  \label{lem:zivboundvec}
  Consider a communication system where a continuous-valued source vector~$\S$
  is encoded into a vector~$\X(\S)$, sent across independent parallel scalar
  AWGN channels with noise variance~$\szq$, and decoded at the receiver to
  produce an estimate~$\Shv$. If the density $f_{\S}(\s)$ of the source is such
  that there exists a set~$\Xi$ and a number $\pmin > 0$ such that $f_{\S}(\s)
  \ge \pmin$ whenever $s \in \Xi$, then for any vector~$\Dv$ the mean squared
  error incurred by the communication system satisfies
  \begin{equation*}
    \msev \ge \pmin  \left( \frac{\|\Dv\|}{2}\right)^2 \int_{\Xi \cap (\Xi -
    \Dv)} Q(d(\s, \Dv)/2 \sz) d\s,
  \end{equation*}
  where $d(\s,\Dv) = \|\X(\s) - \X(\s + \Dv)\|$. (The addition of a set $A$ and
  a vector~$\x$ is defined as $A + \x = \{\vect{a} + \x : \vect{a} \in A\}$.)
\end{lemma}

\begin{remark}
  \label{rem:zivboundvec}
  The set $\Xi \cap (\Xi - \Dv)$ is the equivalent of the interval $[A,
  B-\Delta]$ in the scalar case. It has the property that for every $\s \in \Xi
  \cap (\Xi - \Dv)$, $\s + \Dv \in \Xi$. To get a meaningful lower bound,
  $\Dv$~should of course be chosen such that $\Xi \cap (\Xi - \Dv)$ is nonempty.
  Note also that when $\|\Dv\| \goesto 0$, the volume (or area) of $\Xi \cap
  (\Xi - \Dv)$ converges to the volume of~$\Xi$.
\end{remark}

\begin{proof}[Proof of \lemref{zivboundvec}]
  The proof is essentially the same as that for the scalar case
  (\lemref{zivbound}). The only difference is that the
  integrals~$\int_A^{B-\Delta}$ and~$\int_{A+\Delta}^B$ are replaced,
  respectively, with~$\int_{\Xi \cap (\Xi - \Dv)}$ and~$\int_{\Xi \cap (\Xi +
  \Dv)}$.
\end{proof}

Using \lemref{zivboundvec}, Lemmata~\ref{lem:lowerbound1}
and~\ref{lem:lowerbound2} can be rederived for the case of lattices; the
statements are in fact identical to the scalar case.

\begin{lemma}
  \label{lem:lowerbound1vec}
  For an arbitrary function~$\e(\snr)$, the mean squared error of the lattice
  quantizer transmission scheme of \secref{latticequant} satisfies
  \begin{equation*}
    \msev \in \Omega(\snr^{-n + (n-1)\e}).
  \end{equation*}
\end{lemma}

\begin{lemma}
  \label{lem:lowerbound2vec}
  For an arbitrary function $\e(\snr) \ge 0$, the mean squared error of the
  strategy of \secref{latticequant} satisfies
  \begin{equation*}
    \msev \in \Omega(\snr^{-1 + \e/2} \exp\{- c \snr^\e \},
  \end{equation*}
  where $c > 0$ does not depend on~$\snr$.
\end{lemma}

See \appref{lbvecproofs} for the proofs.

Since Lemmata~\ref{lem:lowerbound1vec} and~\ref{lem:lowerbound2vec} are
identical to Lemmata~\ref{lem:lowerbound1} and~\ref{lem:lowerbound2}, it is a
direct consequence that \thmref{scalinglb} also applies to lattice quantizers.
It is restated here for completeness.

\begin{theorem}
  \label{thm:scalinglbvec}
  For any choice of the parameter~$\beta$ (as a function of the SNR) and for any
  decoder, the mean squared error of the lattice quantizer transmission strategy
  of \secref{latticequant} satisfies
  \begin{equation*}
    \msev \in \Omega(\snr^{-n} (\log\snr)^{n-1}).
  \end{equation*}
\end{theorem}

\begin{proof}
  The proof is identical to that of \thmref{scalinglb}.
\end{proof}


\subsection{Achievability}

The scalar quantizer scheme of \secref{scalarquant} is nothing but a lattice
scheme using a onedimensional rectangular lattice. The achievability proof of
that section therefore applies to general lattices as well, and the bound of
\thmref{scalinglbvec} is trivially achievable using a lattice quantizer.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: GENERAL BANDWIDTH EXPANSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{General Bandwidth Expansion}\label{sec:genbwexp}

If the source~$S$ has bounded support, the transmission strategy of
\secref{scalarquant} can easily be adapted to encode not one but $k$~source
symbols into $n$~channel inputs (where $k < n$). For simplicity it is first
assumed that the source support is contained in the interval $[-1/2, 1/2]$; a
generalization follows in \secref{extgensources}.


\subsection{Transmission Strategy}

The strategy used to encode the source symbols $S_1$, \dots, $S_k$ into the
channel inputs $X_1$, \dots, $X_n$ is displayed schematically on
\figref{knencoding} for~$k=3$ and~$n=5$. 
\begin{figure}
  \begin{center}
    \input{figures/knencoding.tex_t}
  \end{center}
  \caption{Schematic display of the transmission strategy of \secref{genbwexp}
  for~$k = 3$ and~$n=5$. The boxes labeled~\textsf{Q} are uniform scalar
  quantizers with resolution~$\beta$ as described in~\eqref{eq:QEdefkn}. The
  part marked in thick lines corresponds exactly to the encoding scheme
  described in \secref{scalarquant} to encode one source symbol into three
  channel uses; compare this to \figref{1nencoding}.}
  \label{fig:knencoding}
\end{figure}
The encoder consists of $k$~parallel scalar encoders that encode each~$S_i$ into
$n-k$~quantizer outputs $Q_{i,1}$, \dots, $Q_{i,n-k}$ and a quantization
error~$E_{j,n-k}$, just like for the case~$k=1$. The quantizer outputs at each
level are then combined into a single value $Q_{\tot,j}$
(cf.~\figref{knencoding}). More precisely, letting $i \in \{1, \dots, k\}$ be
the index of the source symbols, the encoder defines $E_{i,0} = S_i$ for
each~$i$ and computes
\begin{align}
  Q_{i,j} &= \frac{1}{\beta} \Int(\beta E_{i,j-1}) \quad\text{and} \nonumber \\
  E_{i,j} &= \beta(E_{i,j-1} - Q_{i,j}). \label{eq:QEdefkn}
\end{align}
(This is exactly the same as~\eqref{eq:QEdef}, except for the addition of the
subscript~$i$; see also \figref{knencoding}.) For each quantization level~$j =
1$, \dots, $n-k$, the $k$~parallel quantizer outputs are combined into
\begin{equation}
  \label{eq:Qtot}
  Q_{\tot,j} = \sum_{i=1}^k \frac{1}{\beta^{i-1}} Q_{i,j}.
\end{equation}
Since each $Q_{i,j}$ is a multiple of $1/\beta$ and is assumed to lie in $[-1/2,
1/2]$, the mapping from the $Q_{i,j}$ into~$Q_{\tot,j}$ is invertible, and the
inverse can be recursively computed as
\begin{align}
  Q_{k,j} &= \beta^{k-1} Q_{\tot,j} \bmod 1 \nonumber \\
  Q_{k-i,j} &= \beta^{k-1-i} Q_{\tot,j} - \sum_{l = 1}^i \beta^{-l} Q_{k - i +
  l} \bmod 1, \quad\text{$i = 1$, \dots, $k-1$,} 
  \label{eq:Qtotinv}
\end{align}
where $x \bmod 1 \deq x - \Int(x)$.

Point~2 of \propref{qeproperties} from \secref{scalarquant} applies here as
well, so there exists a constant~$\gamma^2$ that upper bounds the variances of
all $Q_{i,j}$ and~$E_{i,j}$ for all~$\beta$. The channel inputs are thus
\begin{align*}
  X_j &= (\sqrt{P}/\gamma) Q_{\tot,j} &&\quad\text{for $j = 1$, \dots, $n-k$ and}
  \\
  X_j &= (\sqrt{P}/\gamma) E_{j-n+k,n-k} &&\quad\text{for $j = n-k+1$, \dots,
  $n$.}
\end{align*}


\subsection{Decoder}

Just like when $k=1$, the decoder first computes separate estimates~$\Qh_{i,j}$
and~$E_{i,n-k}$ and then combines them into the estimates~$\Sh_i$. The
$\Qh_{i,j}$ are obtained using a maximum likelihood (minimum distance) estimate
of~$Q_{\tot,j}$ and then applying~\eqref{eq:Qtotinv}. The estimate of
$Q_{\tot,j}$ is
\begin{equation*}
  \Qh_{\tot,j} = \frac{1}{\beta^k} \arg\min_{l\in\Z}
  \left| \frac{l\sqrt P}{\gamma \beta^k} - Y_j \right|,
\end{equation*}
and the quantization errors~$E_{i,n-k}$ are estimated as
\begin{equation*}
  E_{i,n-k} = \frac{\E[E_{i,n-k} Y_{n-k+i}]}{\E[Y_{n-k+i}^2]}.
\end{equation*}
Using~\eqref{eq:unwraprec} and~\eqref{eq:Qtotinv}, the final estimate for $i =
1$, \dots, $k$ is
\begin{equation*}
  \Sh_i = \sum_{j=1}^{n-k} \frac{1}{\beta^{j-1}} \Qh_{i,j} +
  \frac{1}{\beta^{n-k}} \Eh_{i,n-k}.
\end{equation*}


\subsection{Error Analysis}

As in the case~$k=1$, the overall mean squared error can be written as
\begin{equation*}
  \mse = \sum_{j=1}^{n-k} \frac{1}{\beta^{2(j-1)}} \Err_{Q,i,j} + 
  \frac{1}{\beta^{2(n-k)}} \Err_{E,i}
\end{equation*}
where $\Err_{Q,i,j} \deq \E[(\Qh_{i,j} - Q_{i,j})^2]$ and $\Err_{E,i} \deq
\E[(\Eh_{i,n-k} - E_{i,n-k})^2]$. The behavior of the~$\Err_{E,i}$ is exactly
the same as when~$k=1$; the following lemma is therefore given without proof.

\begin{lemma}
  \label{lem:Eedecayk}
  The estimation error of the $E_{i,n-k}$ satisfies
  \begin{equation*}
    \Err_{E,i} \in O(\snr^{-1})
  \end{equation*}
  for all $i = 1$, \dots, $k$. 
\end{lemma}

The main difference to the case~$k=1$ concerns the behavior of~$\Err_{Q,i,j}$.
Because $k$~quantizer outputs are packed into a single channel input as
described by~\eqref{eq:Qtot}, $\beta^2$ is raised to the exponent~$k$ in the
following lemma (compare with \lemref{eqbound}).

\begin{lemma}
  \label{lem:eqboundk}
  For each $i = 1$, \dots, $k$ and for each $j = 1$, \dots, $n-k$,
  \begin{equation*}
    \Err_{Q,i,j} \in O(\exp\{-c \snr/\beta^{2k} \})
  \end{equation*}
  where~$c > 0$ is a constant.
\end{lemma}

\begin{proof}
  For any $i$ and~$j$, $| \Qh_{i,j} - Q_{i,j}| \ne 0$ only if $|Z_j| \ge
  \sqrt{P} / 2 \gamma \beta^k$. Furthermore, since $\Qh_{i,j}, Q_{i,j} \in
  [-1/2, 1/2)$, $|\Qh_{i,j} - Q_{i,j}| \le 1$. The error~$\Err_{Q,i,j}$ can thus
  be upper bounded as
  \begin{align*}
    \E[(\Qh_{i,j} - Q_{i,j})^2] &\le \Pr\left[|Z_j| \ge \frac{\sqrt{P}}{2 \gamma
    \beta^k} \right] \\
    &= 2 Q\left( \frac{\sqrt{\snr}}{2\gamma\beta^k} \right) \\
    &\le \exp\{-c \snr / \beta^{2k} \}
  \end{align*}
  with $c = 1/8\gamma$.
\end{proof}


Let now $\beta = \lceil \snr^{(1-\e)/2k} \rceil$. Then $\beta^{2k} \in
O(\snr^{1-\e})$, and the bound from \lemref{eqboundk} becomes
\begin{equation}
  \label{eq:qboundkepsilon}
  \Err_{Q,i,j} \in O(\exp\{-c \snr^\e\})
\end{equation}
(where $c > 0$ is not necessarily the same constant as in \lemref{eqboundk}).
Moreover, using \lemref{Eedecayk},
\begin{equation}
  \label{eq:Eeboundkepsilon}
  \frac{\Err_{E,i}}{\beta^{2(n-k)}} \in O(\snr^{\frac{n}{k} - \e
  \frac{n-k}{k}}).
\end{equation}

The final step is to choose $\e$ as a function of~$\snr$ (again just as
for~$k=1$). Let
\begin{equation*}
  \e(\snr) = \frac{\log(n \log\snr / c)}{\log\snr}.
\end{equation*}
Inserting this in~\eqref{eq:qboundkepsilon} and~\eqref{eq:Eeboundkepsilon}, 
\begin{align*}
  \Err_{Q,i,j} &\in O(\snr^{-n}) \quad\text{and} \\
  \Err_{E,i} &\in O(\snr^{-n/k} (\log\snr)^{(n-k)/k}).
\end{align*}
The overall MSE scales thus as
\begin{equation*}
  \mse \in O(\snr^{-n/k} (\log\snr)^{(n-k)/k}).
\end{equation*}


\subsection{Extension to General Sources}\label{sec:extgensources}

The assumption in \secref{genbwexp} has so far been that the support of the
source is limited to~$[-1/2,1/2]$. If a source~$S$ has support outside this
interval but its support still lies within a bounded set, just define $S' =
S/\alpha$, with $\alpha > 1$ such that $S' \in [-1/2, 1/2]$. Then use
the described scheme to transmit~$S'$ and let $\Sh = \alpha \Sh'$. The
incurred distortion is $\E[(S - \Sh)^2] = \alpha^2 \E[(S' - \Sh')^2]$; the SDR
therefore still scales in the same way as when~$S \in [-1/2, 1/2]$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: TOWARDS A GENERAL SDR UPPER BOUND
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Towards a General SDR Upper Bound}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: HISTORICAL REMARKS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Historical Remarks}

Schemes similar to the one proposed here have been considered before. Indeed,
one of the first schemes to transmit an analog source across two uses of a
Gaussian channel was suggested by Shannon~\cite{Shannon1949}. Notice its
resemblance to the constellation studied here, shown in
Figure~\ref{fig:shannoncomparison}.
\begin{figure}
  \centerline{\subfloat[Shannon's original proposition.]{\input{figures/shannonline.tex_t}}
  \hfil
  \subfloat[Mapping proposed in this paper
  (for~$n=2$).]{\input{figures/ourconstellation.tex_t}} }% end centerline
  \caption{A minimum-delay source-channel code for $n=2$ can be visualized as a
  curve in $\R^2$ parametrized by the source. Here we compare the mapping
  presented in this chapter (right) to Shannon's original suggestion (left).}
  \label{fig:shannoncomparison}
\end{figure}

After Shannon, Wozencraft and Jacobs~\cite{WozencraftJ1965} were among the first
to study source-channel mappings as curves in $n$-dimensional space.
Ziv~\cite{Ziv1970} found important theoretical limitations of such mappings.
Much of the later work is due to Ramstad and his coauthors
(see~\cite{Ramstad2002}, \cite{FloorR2006}, \cite{CowardR2000,CowardR2000a},
\cite{WernerssonSR2007}, \cite{HeklandFR2009}). A proof that the performance of
minimal-delay codes is strictly smaller than that of codes with unrestricted
delay when $n>1$ was given in 2008 by Ingber et al.~\cite{IngberLZF2008}.

For $n=2$, the presented scheme is almost identical to the HSQLC scheme by
Coward~\cite{Coward2001}, which uses a numerically optimized quantizer,
transmitter and receiver to minimize the mean-squared error (MSE) for finite
values of the SNR. Coward correctly conjectured that the right strategy for $n >
2$ would be to repeatedly quantize the quantization error from the previous
step, which is exactly what we do here.

Another closely related communication scheme is the \emph{shift-map} scheme due
to Chen and Wornell~\cite{ChenW1998}.  Vaishampayan and
Costa~\cite{VaishampayanC2003} showed in their analysis that it achieves the
scaling exponent $n-\e$ for any $\e > 0$ if the relevant parameters are chosen
correctly as a function of the SNR. Up to rotation and a different constellation
shaping, the shift-map scheme is in fact virtually identical to the one
presented here, a fact that was pointed out recently by Taherzadeh and
Khandani~\cite{TaherzadehK2008}. In their own paper they develop a scheme that
achieves the optimal scaling exponent exactly and is in addition robust to SNR
estimation errors; their scheme, however, is based on rearranging the digits of
the binary expansion of the source and is thus quite different from the one
presented here.

Shamai, Verd\'u and Zamir~\cite{ShamaiVZ1998} used Wyner-Ziv coding to extend an
existing analog system with a digital code when additional bandwidth is
available. Mittal and Phamdo~\cite{MittalP2002} (see also the paper by Skoglund,
Phamdo and Alajaji~\cite{SkoglundPA2002}) split up the source into a quantized
part and a quantization error, much like we do here, but they use a
separation-based code (or ``tandem'' code) to transmit the quantization symbols.
Reznic et al.~\cite{ReznicFZ2006} use both quantization and Wyner-Ziv coding,
and their scheme includes Shamai et al.\ and Mittal \& Phamdo as extreme cases.
All three schemes, however, use long block codes for the digital phase and incur
correspondingly large delays, so they are not directly comparable with minimum
delay schemes.












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% CHAPTER APPENDICES                                                           %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{subappendices}
  % [Marius] This is no longer needed, since we use the more general lemma that
  % gives the upper bound on the quantization error for all lattices.
%  \section{Proof of Lemma~\ref{lem:qvarconvergence}}
%  \label{app:lemma1proof}
%
%  Since all involved distributions are symmetric, $\E[Q_i] = 0$.  Writing $Q_i$
%  as a function of $E_{i-1}$, we have
%  \begin{equation}
%    \var(Q_i) = \E[Q_i^2] = \int_{-\infty}^\infty Q_i(\xi)^2 f(\xi) d\xi,
%    \label{eq:varqintegral}
%  \end{equation}
%  %\begin{align}
%  %  \var(Q_i) &= \E[Q_i^2] \nonumber\\
%  %  &= \int_{-\infty}^\infty Q_i(\xi)^2 f(\xi) d\xi,
%  %  \label{eq:varqintegral}
%  %\end{align}
%  where $f(\xi)$ is the pdf\footnote{probability density function} of $E_{i-1}$.
%  Now, $Q_i(\xi) = j/\beta$ whenever
%  \begin{equation*}
%    \xi \in \left[ \frac{j - 1/2}{\beta}, \frac{j + 1/2}{\beta} \right).
%  \end{equation*}
%  With this, the integral~\eqref{eq:varqintegral} becomes
%  \begin{align*}
%    \var(Q_i) &= \frac{1}{\beta^2} \sum_{j \in \Z} j^2 
%    \int_{\frac{j - 1/2}{\beta}}^{\frac{j + 1/2}{\beta}} f(\xi) d\xi \\
%    &= \sum_{j\in \Z} \left( \frac{j}{\beta} \right)^2 \left[ F\left( 
%    { \textstyle
%    \frac{j + 1/2}{\beta} }\right) - F \left( { \textstyle \frac{j - 1/2}{\beta}
%    } \right) \right],
%  \end{align*}
%  where $F(\xi)$ is the cdf\footnote{cumulative distribution function} of
%  $E_{i-1}$. As $\beta$ goes to infinity, this sum converges to a
%  Riemann-Stieltjes integral:
%  \begin{equation*}
%    \var(Q_i) \longrightarrow \int \xi^2 dF(\xi) = \var(E_{i-1}) \quad
%    \text{as $\beta \goesto \infty$.}
%  \end{equation*}
%  \qed


\section{Proof of \lemref{Qvarbound}}\label{app:qvarboundproof}

\begin{proof}
  Let $f(\xi)$ be the probability density function of~$E_{i-1}$. Then
  \begin{align*}
    \Var Q_i &= \frac{1}{\beta^2} \int_{\R} \Int(\beta E_{i-1})^2 f(\xi) d\xi \\
    &= \frac{1}{\beta^2} \sum_{i\in\Z} i^2
    \int_{\frac{i-1/2}{\beta}}^{\frac{i+1/2}{\beta}} f(\xi) d\xi.
  \end{align*}
  Whenever $\xi \in [(i-1/2)/\beta, (i+1/2)/\beta)$, $|i| \le \beta|\xi| + 1/2$,
  so
  \begin{align*}
    \Var Q_i &\le \frac{1}{\beta^2} \sum_{i\in \Z}
    \int_{\frac{i-1/2}{\beta}}^{\frac{i+1/2}{\beta}} (\beta|\xi| + 1/2)^2 
    f(\xi) d\xi \\
    &= \frac{1}{\beta^2} \int_{\R} (\beta|\xi| + 1/2)^2 f(\xi) d\xi \\
    &= \Var E_{i-1} + \frac{1}{4\beta^2} + \frac{1}{\beta} \int_{\R} |\xi|
    f(\xi)d\xi.
  \end{align*}
  To bound the last integral, use the fact that $\E[|E_{i-1}|] \le \sqrt{\Var
  E_{i-1}}$ to finally obtain
  \begin{equation*}
    \Var Q_i \le \Var E_{i-1} + \frac{\sqrt{\Var E_{i-1}}}{\beta} +
    \frac{1}{\beta^2}.
  \end{equation*}
\end{proof}
  

  \section{Proof of Ziv's Lower Bound (Lemma~\ref{lem:zivbound})}
  \label{app:zivboundproof}

  If we condition the mean squared error on~$S$ and use the assumption on~$p_S$
  we obtain
  \begin{equation*}
    \mse \ge \pmin \int_A^B \msecond ds.
  \end{equation*}
  For $\Delta \in [0, B-A]$ we can further bound this in two ways:
  \begin{align*}
    \mse &\ge \pmin \intabd \msecond ds \\
    \mse &\ge \pmin \int_{A+\Delta}^B \msecond ds \\
    &= \pmin \intabd \msecondd ds.
  \end{align*}
  Averaging the two lower bounds yields
  \begin{equation}
    \label{eq:avglbd}
    \mse \ge \frac{\pmin}{2} \intabd \bigg( \msecond + \\
    \msecondd \bigg) ds,
  \end{equation}
  and applying Markov's inequality to the expectation terms leads to
  \begin{equation}
    \label{eq:markov1}
    \msecond \ge \dhsq \Pr[|\Sh - S| \ge \Delta/2 \mid s]
  \end{equation}
  and
  \begin{equation}
    \label{eq:markov2}
    \msecondd \\
    \ge \dhsq \Pr[|\Sh - S - \Delta| \ge \Delta/2 \mid s+\Delta].
  \end{equation}

  Now suppose that we use the communication system in question for binary
  signaling. We want to send either $s$ or $s+\Delta$; at the decoder we use the
  estimate~$\Sh$ to decide for~$s$ or $s + \Delta$ depending on which one $\Sh$
  is closer to. When $s$ is sent, the decoder makes an error only if $|\Sh - s|
  \ge \Delta/2$; when $s + \Delta$ is sent, it makes an error only if $|\Sh - s
  - \Delta| \ge \Delta/2$. The conditional error probabilities therefore satisfy
  $\Pr[\text{error} | s] \le \Pr[|\Sh - S| \ge \Delta/2 \mid s]$ and
  $\Pr[\text{error} | s + \Delta] \le \Pr[|\Sh - S - \Delta| \ge \Delta/2 \mid s +
  \Delta]$. Applying this to~\eqref{eq:markov1} and~\eqref{eq:markov2} and
  inserting the result in~\eqref{eq:avglbd} yields
  \begin{equation}
    \label{eq:zivalmostproved}
    \mse \ge \pmin \dhsq \intabd \pe(s, \Delta) ds,
  \end{equation}
  where $\pe(s, \Delta) = \left(\Pr[\text{error}|s] + \Pr[\text{error}|s +
  \Delta] \right)/2$ is the average error probability.

  If $s$ and $s+\Delta$ are picked with equal probability and transmitted across
  $n$~parallel Gaussian channels as $\X(s)$ and $\X(s+\Delta)$, and if $d(s,
  \Delta) = \| \X(s) - \X(s + \Delta)\|$, then the error probability of the MAP
  decoder is $Q(d(s,\Delta) / 2 \sz)$, a standard result of communication theory
  (see \eg~\cite[Section~4.5]{WozencraftJ1965}). Because the MAP decoder minimizes
  the error probability, $Q(d(s,\Delta)/2\sz) \le \pe(s,\Delta)$, which, when
  inserted into~\eqref{eq:zivalmostproved}, completes the proof. \hfill\qed



  \section{Lattice Basics}\label{app:latticebasics}

  This section contains the very basics on lattices and lattice quantization
  needed in the remainer of the chapter. For a comprehensive treatment of
  lattices and/or quantization the reader is referred to the books by Conway and
  Sloane~\cite{ConwayS1988} and by Gersho and Gray~\cite{GershoG1992}.

  \begin{definition}
    An $n$-dimensional \emph{lattice} $\Lambda$ is a discrete set of vectors
    (points) in~$\R^n$ such that for any $\x$, $\y \in \Lambda$, $\x + \y \in
    \Lambda$. A sublattice of~$\Lambda$ is a subset $\Lambda' \subseteq \Lambda$
    that is itself a lattice. 
  \end{definition}

  \begin{example}
    \label{ex:scalarlattice}
    In~$\R$ there exists only a single lattice (up to scaling), the scalar
    lattice~$\Z$. Two examples of lattices in~$\R^2$ are displayed in
    \figref{r2lattices}.
  \end{example}
  \begin{figure}
    \centerline{%
    \subfloat[Rectangular lattice.]{%
    \label{fig:rectlattice}\input{figures/rectlattice.tex_t}}%
    \hfil
    \subfloat[Hexagonal lattice.]{%
    \label{fig:hexlattice}\input{figures/hexlattice.tex_t}}%
    }
    \caption{Two lattices in~$\R^2$ and the corresponding partition into Voronoi
    regions.}
    \label{fig:r2lattices}
  \end{figure}
  \begin{proposition}
    \label{prop:intsublattice}
    If $\Lambda$ is a lattice and $\beta \in \NN$, then $\beta \Lambda$ is a
    sublattice of~$\Lambda$. (The set $\beta \Lambda$ is defined as $\{ \beta \x
    : \x \in \Lambda\}$.)
  \end{proposition}

  \begin{proof}
    By definition of a lattice, $\beta \Lambda \subseteq \Lambda$. Moreover, if
    $\x, \y \in \beta \Lambda$, then $\x = \beta \x'$ and $\y = \beta \y'$ with
    $\x', \y' \in \Lambda$. It follows that $\x + \y = \beta (\x' + \y') \in \beta
    \Lambda$, so $\beta\Lambda$ is itself a lattice.
  \end{proof}

  \begin{definition}
    The \emph{Voronoi region} $V(\p)$ of a lattice point $\p \in \Lambda$ is
    defined as \begin{equation*} V(\p) = \{\x \in \R^n : \|\x - \p\| \le \|\x -
      \q\|, \forall \q \in \Lambda\}, \end{equation*} \ie, $V(\p)$ is the set of
      points in $\R^n$ that at least as close to $\p$ as to any other lattice
      point.
  \end{definition}
  See \figref{r2lattices} for an illustration of the Voronoi region.

  \begin{definition}
    The \emph{packing radius} $\rho$ of a lattice is half the minimal distance
    between lattice points. Thus, $\rho$~is the largest radius of spheres that
    can be packed in~$\R^n$ by placing them at the lattice points.
  \end{definition}

  \begin{definition}
    The \emph{covering radius}~$R$ of a lattice~$\Lambda$ is the least upper
    bound for the distance from any point of~$\R^n$ to the closest point $\x \in
    \Lambda$. Thus, spheres of radius~$\rho$ around each lattice point will
    cover~$\R^n$, and no smaller radius will do.~\cite{ConwayS1988}
  \end{definition}

  The packing radius and the covering radius are illustrated on
  \figref{packingcoveringr} for a hexagonal lattice.
  \begin{figure}
    \begin{center}
      \input{figures/packingcoveringr.tex_t}
    \end{center}
    \caption{The packing radius~$\rho$ and the covering radius~$R$ for the
    rectangular lattice and the hexagonal lattice.}
    \label{fig:packingcoveringr}
  \end{figure}

  \begin{definition}
    \label{def:latticequant}
    A \emph{lattice quantizer} $\QL : \R^n \ra \Lambda$ maps each point
    of~$\R^n$ to the closest lattice point. Thus, for any $\x \in \R^n$, $\y \in
    \Lambda$,
    \begin{equation*}
      \| \x - \QL(\x) \| \le \|\x - \y\|.
    \end{equation*}
  \end{definition}

  \begin{remark}
    \label{rem:latticequant}
    \defref{latticequant} does not unambiguously specify $\QL(\x)$ if $\x$~lies
    on the boundary between the Voronoi regions of two adjacent lattice points.
    Since quantization is only applied to continuous-valued random variables in
    this chapter, however, the probability of this happening is zero, and this
    ambiguity can be left alone without causing any trouble.
  \end{remark}

  \begin{example}
    Let $\Lambda = \Z / \beta$ for some $\beta >0$. The associated
    quantizer~$\QL$ maps each real number to the closest multiple of $1/\beta$.
  \end{example}

  %The following simple property will be useful later.
  %\begin{proposition}
  %  Let $\Lambda = \Z / \beta$ with $\beta \in \NN$ and let $x \in [-1/2, 1/2]$.
  %  Then
  %  \begin{equation*}
  %    \QL(x) \in
  %    \begin{cases}
  %      \left\{ -\frac{\beta/2}{\beta}, -\frac{\beta/2 - 1}{\beta}, \dots, 
  %      -\frac{1}{\beta}, 0, \frac1\beta, \dots, \frac{\beta/2}{\beta} \right\}
  %      & \text{if $\beta$ is even} \\
  %      \left\{ -\frac{(\beta - 1)/2}{\beta}, -\frac{(\beta-1)/2 - 1}{\beta}, 
  %      \dots, -\frac1\beta, 0, \frac1\beta, \dots, \frac{(\beta-1)/2}{\beta}
  %      \right\}
  %      & \text{if $\beta$ is odd}.
  %    \end{cases}
  %  \end{equation*}
  %\end{proposition}
  %
  %\begin{proof}
  %  The proof is left as an exercise for the reviewers.
  %\end{proof}
  %
  The next two lemmata are useful to bound the transmit power when transmitting
  a quantized random vector.
  \begin{lemma}
    \label{lem:latquantvar}
    Let $\X$ be a random vector satisfying $\E[\|\X\|^2]
    = \sq < \infty$. Let $\Y = \QL(\X)$. Then $\E[\|\Y\|^2] \le \sq + 2R\sigma +
    R^2$, where $R$~is the covering radius of~$\Lambda$.
  \end{lemma}

  \begin{proof}
    The power of~$\Y$ is given by
    \begin{equation*}
      \E[\|\QL(\X)\|^2] = \int_{R^n} \|\QL(\x)\|^2 \fXv(\x) d\x
      = \sum_{\p \in \Lambda} \|\p\|^2 \int_{V(\p)} \fXv(\x) d\x.
    \end{equation*}
    By definition of the covering radius, $\|\p\| \le \|\x\| + R$ for all $\x
    \in V(\p)$. Thus,
    \begin{align*}
      \E[\|\QL(\X)\|^2] &\le \sum_{\p \in \Lambda} \int_{V(\p)} (\|\x\| + R)^2
      \fXv(\x) d\x \\
      &= \int_{R^n} (\|\x\| + R)^2 \fXv(\x) d\x.
    \end{align*}
    By assumption, $\int_{\R^n} \|\x\|^2 \fXv(\x) d\x = \sq$. Moreover, by the
    positivity of the variance, $\E[\xi] \le (\E[\xi^2])^{1/2}$, and so
    $\int_{\R^n} \|\x\| \fXv(\x) d\x \le \sigma$. Applying this to the above
    yields
    \begin{equation*}
      \E[\|\QL(\X)\|^2] \le \sq + 2R\sigma + R^2,
    \end{equation*}
    thus completing the proof.
  \end{proof}

  \begin{example}
    Let $X$ be a scalar zero-mean random variable of variance~$\sq$ and let
    $\Lambda = \Z / \beta$, for some $\beta > 0$. The covering radius of this
    lattice is $R = 1/\beta$, so $\E[\QL(X)^2] \le \sq + 2 \beta^{-1} \sigma +
    \beta^{-2}$.
  \end{example}

  \begin{lemma}
    \label{lem:voronoivarbound}
    Let $\X$~be a random vector whose support is limited to the Voronoi
    region~$V(\vz)$ of a lattice~$\Lambda$. Then $\E[\|\X\|^2] \le
    R\Vol(V(\vz))$, where $R$~is the covering
    radius of~$\Lambda$ and $\Vol(V(\vz))$ is the volume of a Voronoi region.
  \end{lemma}

  \begin{proof}
    \begin{align*}
      \E[\|\X\|^2] & = \int_{V(\vz)} \|x\|^2 \fXv(\x) d\x \\
      & \le R \int_{V(\vz)} \fXv(\x) d\x = R \Vol(V(\vz)).
    \end{align*}
  \end{proof}




  \section{Proof of Lemmata~\ref{lem:lowerbound1vec}
  and~\ref{lem:lowerbound2vec}}\label{app:lbvecproofs}

  The following auxiliary result and its corollary will be useful for the proofs
  to come. See \figref{voronoiintersect} for an illustration.

  \begin{lemma}
    \label{lem:subvoronoi}
    Let $\Lambda$ be an arbitrary $m$-dimensional lattice and let $\Lambda' =
    \Lambda/\beta$, where $\beta \in \NN$. (Clearly $\Lambda$ is a sublattice of
    $\Lambda'$.) Then the fraction of Voronoi cells of $\Lambda$ that do not lie
    on the boundary between two Voronoi cells of $\Lambda'$ is bounded away from
    zero as $\beta$~grows large.
  \end{lemma}

  \begin{proof}
    Let $V(\Lambda)$ be a Voronoi region of $\Lambda$ and let $V(\Lambda')$ be a
    Voronoi region of~$\Lambda'$.  Consider a sphere of radius $\rho - 2R'$
    around the center of $V(\Lambda)$, where $\rho$ is the
    packing radius of $\Lambda$ and $R' = R/\beta$ is
    the covering radius of~$\Lambda'$. By definition of the packing radius, this
    sphere is completely contained within $V(\Lambda)$. Furthermore, the
    distance from the border of the sphere to the border of $V(\Lambda)$ is
    at least~$2R'$. Any Voronoi cell of $\Lambda'$ that intersects
    with the boundary of $V(\Lambda)$ lies therefore outside the sphere. 
    
    The fraction of Voronoi cells of $\Lambda'$ that do not lie on the boundary
    of $V(\Lambda)$ is therefore lower bounded by
    \begin{align*}
      \frac{(\rho - 2R')^m V^{(m)}(1)}{\Vol V(\Lambda)} &= \frac{(\rho -
      2R/\beta)^n V^{(m)}(1)}{\Vol V(\Lambda)},
    \end{align*}
    where $V^{(m)}(1)$ is the volume of the $m$-dimensional unit sphere. As beta
    grows large, this converges to $\rho V^{(n)}(1) / \Vol V(\Lambda) > 0$, and
    the proof is complete.
  \end{proof}

  \begin{corollary}
    Let $\Lambda$ be a fixed lattice and consider the sequence of lattices
    $\Lambda/\beta$, $\Lambda/\beta^2$, \dots, $\Lambda/\beta^{n-1}$, where
    $\beta \in \NN$. Then the fraction of Voronoi regions of
    $\Lambda/\beta^{n-1}$ that do not lie on the boundary between two Voronoi
    regions of any of its sublattices $\Lambda/\beta^i$, $i < n-1$, is bounded
    away from zero as $\beta$ grows large.
  \end{corollary}

  \begin{proof}
    Apply \lemref{subvoronoi} successively to the pairs $\Lambda/\beta^i$,
    $\Lambda/\beta^{i+1}$, for $i = 1$, \dots, $n-2$.
  \end{proof}

  \begin{figure}
    \centerline{
    \subfloat[$\beta = 3$]{\input{figures/hierhex3.tex_t}}
    \hfil
    \subfloat[$\beta = 4$]{\input{figures/hierhex4.tex_t}}
    }
    \caption{Illustration for \lemref{subvoronoi} and the corollary thereof.
    Note how in the right picture the fraction of small Voronoi cells that are
    not ``cut'' by a Voronoi boundary of the sublattice is larger.}
    \label{fig:voronoiintersect}
  \end{figure}

  \begin{proof}[Proof of \lemref{lowerbound1vec}]
    Consider the lattice $\Ln \deq \Lambda/\beta^{n-1}$. For $\p \in
    \Ln$, let $V(\p)$ be the Voronoi region of~$\p$.
    For some $\vxi \in V(\vz)$ define $\Dv = \Delta \vxi$ for some $\Delta \le
    1$ and define $\Vd(\p) = V(\p) \cap (V(\p) - \Dv)$. This set has the
    property that $\x + \Dv \in V(\p)$ whenever $\x \in \Vd(\p)$.\footnote{For
    reference, the sets $\Vd(\p)$ are the equivalent of the intervals
    $\I_j^{\Delta}$ in the proof of \lemref{lowerbound1}.}

    If $\p$ is such that $V(\p)$ does not lie on the boundary of the Voronoi
    region of a sublattice, then $\Vd(\p)$ defined this way has the property
    that if $\s \in \Vd(\p)$,
    \begin{align*}
      d(\s,\Dv) &= \frac{\sqrt{mP}}{\gamma} \| \Evc_{n-1}(\s) - \Evc_{n-1}(\s +
      \Dv) \| \\
      &= \frac{\sqrt{mP}}{\gamma} \| \vxi \| \beta^{n-1} \Delta.
    \end{align*}
    We can now apply \lemref{zivboundvec} and restrict the integral to the set
    \[ \Psi(\Dv) \deq \Xi \cap (\Xi - \Dv) \cap \bigcup_{\p \in \Ln'} \Vd(\p),
    \]
    where $\Ln'$ is defined to be the subset of those points in $\Ln$ whose
    Voronoi region does not lie on the boundary between Voronoi regions of a
    sublattice. By the corollary to \lemref{subvoronoi}, this set has positive
    probability.

    The lower bound of \lemref{zivboundvec} is then relaxed to give
    \begin{equation*}
      \msev \ge c_1 \Delta^2 Q(c_2 \sqrt{\snr} \beta^{n-1} \Delta) 
      \int_{\Psi(\Dv)} d\s
    \end{equation*}
    for some positive constants~$c_1$ and~$c_2$. Letting $\Delta =
    1/(\sqrt{\snr} \beta^{n-1})$ and $\beta^2 = \snr^{1-\e}$ yields
    \begin{equation*}
      \msev \ge c_3 \snr^{-n+(n-1)\e} \int_{\Psi(\Dv)} d\s,
    \end{equation*}
    with $c_3 > 0$ a constant independent of~$\snr$.

    It remains to prove the convergence of $\int_{\Psi(\Dv)} d\s$ to a constant.
    Since $\Delta$ goes to zero as $\snr \goesto \infty$, the set $\Psi(\Dv)$
    converges to the set $\Xi \cap \bigcup_{\p \in \Ln'} V(\p)$. Since both
    $\Xi$ and~$\bigcup_{\p \in \Ln'} V(\p)$ have positive volume, the overall
    set also has positive volume.
  \end{proof}


  \begin{proof}[Proof of \lemref{lowerbound2vec}]
    Let $\Lambda$~be the normalized lattice used for quantization. Let $\vxi \in
    \Lambda$ be arbitrary but fixed. Let $\Dv = \beta^{-1} \vxi$. Using the same
    reasoning as in the proof of \lemref{lowerbound2}, it follows that $\Q_1(\s
    + \Dv) = \Q_1(\s) + \Dv$, and also $\Q_i(\s + \Dv) = \Q_i(\s)$ for $i = 2$,
    \ldots, $n-1$, and $\Evc_{n-1}(\s+\Dv) = \Evc_{n-1}(\s)$. Consequently,
    \begin{equation*}
      \| \X(\s) - \X(\s +\Dv) \| = \frac{\sqrt{mP}}{\gamma}\|\vxi\| \beta^{-1}.
    \end{equation*}
    According to \lemref{zivboundvec}, therefore,
    \begin{equation*}
      \msev \ge c_1 \beta^{-2} Q(c_2 \sqrt{\snr} \beta^{-1}) \int_{\Xi \cap (\Xi
      - \Dv)} d\s,
    \end{equation*}
    where $c_1$ and~$c_2$ are positive constants independent of~$\snr$.  The
    rest of the proof is essentially identical to that of
    \lemref{lowerbound2}. 
  \end{proof}

\end{subappendices}
