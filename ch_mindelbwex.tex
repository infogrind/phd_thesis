\chapter{Minimal-Delay Codes for Bandwidth Expansion}

To get a good $(1,n)$~bandwidth expansion code, a promising method is to turn
the channel into a reliable channel, thus emulating the effect of feedback. This
is the lesson from the previous chapter. Upon closer inspection, the encoder in
fact only needs to know the channel state after the first $n-1$ channel uses. If
the encoder knows the best estimate of the decoder after $n-1$ channel uses, it
can send the remaining error uncoded. 

Suppose that after $n-1$~channel uses the decoder makes a preliminary
estimate~$\Sh'$, and that the encoder knows this estimate. Let $E = \Sh' - S$ be
the corresponding estimation error and let $\seq$ be its variance. In the
$n\th$~channel use, the method of \exref{gausssingle} is used to transmit~$E$.

The decoder estimates~$\Eh$ and computes the final source estimate $\Sh = \Sh' -
\Eh$. The resulting squared error is
\begin{align*}
  \E[(\Sh - S)^2] &= \E[(S + E - \Eh - S)^2] \\
  &= \E[(\Eh - E)^2] \\
  &= \frac{\seq}{1 + \snr}.
\end{align*}
The SDR after the first $n-1$~rounds of transmission was $\sdr' = \ssq/\seq$.
The final SDR is thus
\begin{equation*}
  \sdr = \frac{\ssq}{\E[(\Sh - S)^2]} = \sdr' (1 + \snr).
\end{equation*}
Hence no matter what the strategy in the first $n-1$ channel uses was, we can
get full advantage of the last channel use using uncoded transmission.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: BASICS OF NONLINEAR MODULATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Basics of Nonlinear Modulation}

The simplest way to encode $k$~source symbols into $n$~channel inputs is to use
a linear encoder. A linear encoder computes $\X = \A \S$, where $\S = (S_1,
\dots, S_k)$, $\X = (X_1, \dots, X_n)$, and $\A$~is a $n \times k$ matrix. With
such an encoder, however, one cannot hope to get an SDR that scales better than
linearly with the~SNR. In fact, since $\X$~is contained in a $k$-dimensional
subspace of~$\R^n$, its covariance matrix has at least $n-k$ zero eigenvalues.
By \lemref{asymptoptinputs}, therefore, the mutual information $I(X^n;Y^n)$ is
such that $2^{2 I(X^n;Y^n)}$ scales at best as $\snr$; the performance is
therefore no better than if there had been only one channel use per source
symbol.

In order to get a better SDR scaling, therefore, the encoder must be nonlinear.
If $k = 1$, a nonlinear encoder~$f(s)$ maps the source space into a curve
in~$\R^n$.  More generally, the source space is mapped into a $k$-dimensional
manifold in~$\R^n$. For simplicity of the notations and illustrations only the
case $k=1$ is considered in this section; all results and insights carry over
naturally to the more general case. 

The set $\{f(s): s \in \Ss\}$ (where $\Ss$~is the support set of the source) is
called the \emph{signal curve} or \emph{signal locus}. \figref{nonlinlocus}
shows an example of a signal locus in~$\R^2$.  Because of the power constraint,
the signal locus is roughly contained in a sphere of radius~$\sqrt{P}$. 
\begin{figure}[tpbh]
  \begin{center}
    \input{figures/nonlinlocus.tex_t}
  \end{center}
  \caption{The signal locus corresponding to a nonlinear encoder from~$\R$
  to~$\R^2$.}
  \label{fig:nonlinlocus}
\end{figure}

The estimate that minimizes the mean squared error is the conditional mean $\E[S
| Y^n]$. For nonlinear encoders, mathematical evaluation of the corresponding
estimation error is usually hopelessly complicated. A decoder whose performance
is easier to evaluate is the \emph{maximum likelihood} (ML) decoder. It computes
$\sh = \arg\max_{s} f(\y|s)$, where $f(\y|s)$ is the conditional \pdf\ of~$\Y$
given~$S$. For the AWGN channel, $\Y = f(S) + \Zv$, where $\Zv$ is circularly
symmetric Gaussian noise of variance~$\szq$. In that case, $f(\y|s)$ is a
monotone decreasing function of $\|\y - f(s)\|$, so the ML decoder can
equivalently be described by $\sh = \arg\min_{s} \|\y - f(s)\|$; it
decides thus for $\sh$ such that $f(\sh)$ is the point on the signal locus
closest to~$\y$. 

Suppose now that the transmitted point is $f(s_0)$ and suppose further that the
curve~$f(s)$ is differentiable in~$s_0$. Then the signal curve can be linearly
approximated in a small neighborhood around $s_0$ as
\begin{equation}
  \label{eq:linapprox}
  f(s_0 + \Delta) = f(s_0) + \Delta \left. \frac{df(s)}{ds}\right|_{s = s_0}.
\end{equation}
If the noise is small, therefore, a valid approximation for the ML estimate is
the projection of the noise vector onto the tangent through~$f(s_0)$ (see
\figref{tangentproj}. This projection is a Gaussian random variable of
variance~$\szq$, from~\eqref{eq:linapprox} with $\Delta = \Sh - s_0$ the average
squared error given $S = s_0$ can therefore be approximated as
\begin{equation}
  \label{eq:errorlinapprox}
  \E[(\Sh - s_0)^2] \approx \frac{\szq}{\displaystyle\left\| \left. \frac{df(s)}{ds}
  \right|_{s=s_0} \right\|^2}.
\end{equation}
Write now $f(s) = \sqrt{P} \ft(s)$, with $\E[\|\ft(S)\|^2] \le 1$. Then
$df(s)/ds = \sqrt{P} d\ft(s)/s$; the quantity $l(s) \deq d\ft(s)/ds$ is called
the \emph{stretch} of the signal locus. In the case where $l(s) = l$ does not
depend on~$s$, the overall MSE simplifies to
\begin{equation}
  \label{eq:constantstretch}
  \mse \approx (\snr l^2)^{-1}.
\end{equation}


\begin{figure}[tbph]
  \begin{center}
    \input{figures/tangentproj.tex_t}
  \end{center}
  \caption{If the noise level is small, the ML estimate can be approximated
  by projecting the noise vector~$\Zv$ onto a tangent through~$f(s_0)$.}
  \label{fig:tangentproj}
\end{figure}

As long as the noise level is small with respect to the distance between
different folds of the signal curve, \eqref{eq:errorlinapprox}~is a valid
approximation. It is clear, though, thath the MSE cannot be arbitrarily
decreased by increasing the stretch: a larger stretch means also a longer signal
curve, so the folds of the curve have to be placed closer together for the power
constraint to remain satisfied. If the stretch becomes too large, the small
noise assumption will be invalidated. On the other hand, if the SDR is to scale
better than linearly with the SNR, then by~\eqref{eq:errorlinapprox} the stretch
must increase with the SNR.

The choice of the stretch represents a tradeoff. A bigger stretch results in a
smaller error provided that the correct fold of the signal curve is decoded, but
it also increases the probability that the wrong fold of the curve is decoded.
Optimally, thus, the stretch is such that the two kinds of errors contribute
equally to the overall error. The communication strategy presented in the
following sections is based on this idea; it will be shown that for a simple
type of signal locus the optimal stretch grows as $(\snr/\log\snr)^{(n-1)/2}$
and so from~\eqref{eq:constantstretch} the resulting SDR scales as
$\snr^n/(\log\snr)^{n-1}$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: HYBRID TRANSMISSION SCHEME
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hybrid Transmission Scheme}\label{sec:hybridscheme}

In the previous section we saw that to achieve SDR scaling better than linear in
the SNR the encoder must be nonlinear. Let us now informally introduce a
particular nonlinear encoding scheme and then analyze its performance in the
sections to come.

The principle of the transmission scheme is the following. To transmit $k$
source symbols in $n$ channel uses, the $k$-dimensional source space is first
partitioned into regular $k$-dimensional subsets. Given $S^k = s^k$, an index of
the subset containing~$s^k$ is communicated in the first $n-k$ channel uses, and
the position of~$s^k$ within the subset is communicated in the last $k$~channel
uses.

How the source space is partitioned and how the index of the subsets is
transmitted in $n-k$ channel uses is explained easiest for $k=1$. Once we have
completely analyzed this case we will move on to the general case.

The partition of the source space is controlled by the integer~$\beta$. For $k =
1$ the one-dimensional source space is first divided into intervals of
length~$1/\beta$. Each interval is then itself divided into $\beta$~subsets of
length~$1/\beta^2$. This procedure is repeated a total of $n-1$~times, so that
at the lowest level an interval has length~$1/\beta^{n-1}$.  The resulting
partition is illustrated on \figref{sourcepartition}.

\begin{figure}[tbph]
  \begin{center}
    \figbox{sourcepartition}
  \end{center}
  \caption{Example partition of the source space $[-1/2,1/2]$ for $\beta = 3$
  and $n = 4$. The numbers in parentheses represent the indices of the
  corresponding intervals.}
  \label{fig:sourcepartition}
\end{figure}

Each of the small intervals of length $1/\beta^{n-1}$ is uniquely identified by
specifying, for each of the $n-1$ partition levels, the position of the interval
of the given level within the interval at the next higher level.
\figref{sourcepartition} illustrates this for two particular intervals. Any
source point~$s$ is thus unambiguously specified by these $n-1$ numbers along
with the position of the point within the lowest level interval. To
transmit~$s$, these $n$~numbers are now simply scaled to satisfy the power
constraint and sent across the $n$~channel uses.

The signal locus resulting from this scheme is illustrated for $n = 2$ and $n =
3$ in Figures~\ref{fig:hybridlocus-2} and~\ref{fig:hybridlocus-3}, respectively.
It consists of parallel straight line segments arranged in a regular
$n-1$-dimensional grid, each line segment corresponding to one of the intervals
of length~$1/\beta^{n-1}$ of the source space. 

\begin{figure}[tbph]
  \centerline{%
  \subfloat[$\beta = 2$]{\label{fig:hybridlocus-2}%
  \figbox[.45\textwidth]{hybridlocus-2}}
  \hfil
  \subfloat[$\beta = 3$]{\label{fig:hybridlocus-3}%
  \figbox[.45\textwidth]{hybridlocus-3}}
  }
  \caption{The signal loci for the hybrid communication scheme for $\beta = 2$
  and $\beta = 3$, respectively.}
  \label{fig:hybridlocus}
\end{figure}

Given the transmission strategy, the problem is now how to choose~$\beta$ as a
function of the SNR. Note that the larger $\beta$, the larger the number of
intervals into which the source space is divided. Hence the number of parallel
line segments in the signal locus increases, and for a fixed power constraint
the segments have to be placed closer together. Conversely, if $\beta$ is small,
the signal locus consists of only a few parallel lines, which are spaced far
apart. Note also that for a fixed power constraint, each interval of length
$1/\beta^{n-1}$ of the source space is scaled up to a constant length; the
stretch of the signal curve therefore grows proportionally to $\beta^{n-1}$.

Hence, if $\beta$ does not vary as the SNR increases, the stretch remains
constant and the SDR scales only linearly in the SNR, as we have seen in the
previous section. On the other hand, if $\beta$ increases too fast as a function
of the SNR, the distance between segments of the signal locus decreases too
rapidly and the probability that the wrong segment is decoded increases.

The trick, then, is to make $\beta$ grow just fast enough so that the stretch
increases quickly but the error due to decoding the wrong segment is kept in
check. This idea is formally developped in \secref{scalarquant}, and the
resulting optimal SDR scaling is derived. Subsequently, \secref{genbwexp}
extends the communication scheme to transmit $k$~source symbols across
$n$~channel uses, and \secref{latticequant} shows how $m$~source symbols can be
encoded into $mn$~channel inputs using lattices.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: SCALAR QUANTIZER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Scalar Quantizer}\label{sec:scalarquant}

The simplest version of the hybrid transmission scheme introduced in
\secref{hybridscheme} encodes one source symbol into $n$~channel inputs using a
scalar quantizer. This section derives upper and lower bounds on the SDR
scaling for this case. The sections that follow will progressively extend the
analysis to more general hybrid schemes.


\subsection{Transmission Strategy}\label{sec:commscheme}

The communication strategy described hereafter is displayed schematically on
\figref{1nencoding}.
\begin{figure}[tbph]
  \begin{center}
    \input{figures/1nencoding.tex_t}
  \end{center}
  \caption{Schematic display of the encoder described in \secref{commscheme} for
  $n = 4$.}
  \label{fig:1nencoding}
\end{figure}
To encode a single source letter $S$ into $n$~channel input symbols $X_1$,
\dots, $X_n$, we proceed as follows. Define $E_0 = S$ and recursively compute
the pairs $(Q_i, E_i)$ as
\begin{align}
  Q_i &= \frac{1}{\beta}\Int(\beta E_{i-1}) \nonumber \\
  E_i &= \beta (E_{i-1} - Q_i) \label{eq:QEdef}
\end{align}
for $i = 1$, \dots, $n-1$ where $\Int(x)$ is the unique integer~$i$ satisfying
\begin{equation*}
  x \in \left[i - \frac12, i +\frac12\right).
\end{equation*}
The parameter $\beta \in
\NN$ determines the quantization resolution; the larger~$\beta$ the finer the
quantization. Following the argument at the beginning of this chapter,
$\beta$~is assumed to grow with the~SNR. The following result will be useful in
the sequel.

\begin{lemma}
  \label{lem:Qvarbound}
  The variance of $Q_i$ satisfies
  \begin{equation*}
    \Var Q_i \le \Var E_{i-1} + \frac{\sqrt{\Var E_{i-1}}}{\beta} +
    \frac{1}{\beta^2}
  \end{equation*}
  for all $i = 1$, \dots, $n-1$. 
\end{lemma}

\begin{proof}
  See \appref{qvarboundproof}.
\end{proof}



\begin{proposition}
  \label{prop:qeproperties}
  The $Q_i$ and $E_i$ satisfy the following properties:
\begin{enumerate}
  \item The map $S \mapsto (Q_1, \dots, Q_{n-1}, E_{n-1})$ is one-to-one, with
    the inverse given by
    \begin{equation}
      \label{eq:unwraprec}
      S = \sum_{i=1}^{n-1} \frac{1}{\beta^{i-1}} Q_i + \frac{1}{\beta^{n-1}}
      E_{n-1}.
    \end{equation}

  \item There exists a constant~$\gamma > 0$ such that $\Var Q_i \le \gamma^2$
    and $\Var E_i \le \gamma^2$ for all~$i$, regardless of the value of~$\beta$.
\end{enumerate}
\end{proposition}

\goodbreak
\begin{proof}
  \begin{enumerate}
    \item From the definition~\eqref{eq:QEdef} we have
    \begin{equation}
      \label{eq:reverserec}
      E_{i-1} = \frac{1}{\beta} E_i + Q_i.
    \end{equation}
    Repeated use of this relationship leads to the given expression for~$S$. 

  \item First, $\Var E_0 = \Var S = \ssq$, which clearly doesn't depend
    on~$\beta$. For $i = 1$, \ldots, $n-1$, $E_i \in [-1/2, 1/2)$ and so its
    variance is upper bounded as well. Finally, since $\beta \ge 1$ and by
    \lemref{Qvarbound}, 
    \begin{align*}
      \Var Q_i &\le \Var E_{i-1} + \sqrt{\Var E_{i-1}} + 1,
    \end{align*}
    which completes the proof.
  \end{enumerate}
\end{proof}

\begin{remark}
  \label{rem:scgeometry}
  The hierarchical quantization described by~\eqref{eq:QEdef} divides the source
  space into intervals of the form $[(j-1/2)\beta^{-(n-1)},
  (j+1/2)\beta^{-(n-1)})$, as illustrated in \figref{hierarchicalsq}.
\end{remark}
\begin{figure}[tbph]
  \begin{center}
    \figbox{hierarchicalsq}
  \end{center}
  \caption{The hierarchical quantization implied by the transmission scheme in
  \secref{commscheme}.}
  \label{fig:hierarchicalsq}
\end{figure}

The channel input symbols $X_i$ are determined from the $Q_i$ and from $E_{n-1}$
according to 
\begin{align}
  X_i &= (\sqrt{P}/\gamma) Q_i \quad
  \text{for $i = 1$, \dots, $n-1$ and} \nonumber\\
  X_n &= (\sqrt{P}/\gamma) E_{n-1}.
  \label{eq:Xdef}
\end{align}
Following Proposition~\ref{prop:qeproperties}, this ensures that $\E[X_i^2] \le
P$ for all~$i$. 


\subsection{Lower Bound on the Squared Error}\label{sec:scalarlowerbound}

The goal of this section is to lower bound the scaling of the mean squared error
of the transmission strategy described in Section~\ref{sec:commscheme}.

\begin{remark}
  \label{rem:betaepswlog}
  Throughout this section we assume $\beta = \lceil \snr^{(1-\e)/2}\rceil$,
  where $\e = \e(\snr)$ is a positive function of~$\snr$. This results in no
  loss of generality, since for an arbitrary positive function~$f$ we can set
  $\e(\snr) = 1-2 \log(f(\snr))/\log\snr$ to get $\beta(\snr) = \lceil f(\snr)
  \rceil$.  Writing $\beta$ in this form will slightly simplify the mathematical
  derivations to follow. Note that we can bound $\beta$ by $\snr^{(1-\e)/2} \le
  \beta \le \snr^{(1-\e)/2} + 1$, which implies $\beta \in
  \Theta(\snr^{(1-\e)/2})$ (the $\Theta$-notation is defined in
  \appref{asymptotic}).
\end{remark}

\begin{remark}
  \label{rem:functionnotation}
  Note that by~\eqref{eq:QEdef} the $Q_i$ are completely determined by~$S$.
  With a slight abuse of notation, $Q_i(s)$ is therefore used in the sequel to
  denote the value of~$Q_i$ when $S = s$. $E_i(s)$ and $X_i(s)$ are defined in
  an analogous manner. Furthermore, $\X(s) \deq (X_1(s), \dots, X_n(s))$.
\end{remark}

The following result, adapted from Ziv~\cite{Ziv1970}, is a key ingredient in
the proofs of the lemmas that follow.

\begin{lemma}
  \label{lem:zivbound}
  Consider a communication system where a con\-tin\-u\-ous-valued source~$S$ is
  encoded into an $n$-dimensional vector $\X(S)$, sent across $n$~independent
  parallel AWGN channels with noise variance~$\szq$, and decoded at the receiver
  to produce an estimate~$\Sh$.  If the density $p_S$ of the source is such that
  there exists an interval $[A,B]$ and a number $p_{\min} > 0$ such that $p_S(s)
  \ge p_{\min}$ whenever $s \in [A,B]$, then for any $\Delta \in [0,B-A)$ the
  mean squared error incurred by the communication system satisfies
  \begin{equation}
    \label{eq:zivbound}
    \E[(\Sh - S)^2] \ge p_{\min} \left(\frac{\Delta}{2} \right)^2 
    \int_A^{B-\Delta} Q(d(s, \Delta) / 2 \sz) ds,
  \end{equation}
  where $d(s, \Delta) \deq \|\vect{X}(s) - \vect{X}(s+\Delta)\|$ and 
  \[Q(x) = \int_x^{\infty} (1/\sqrt{2\pi}) \exp\{-\xi^2/2\} d\xi.\]
\end{lemma}

\begin{proof}
  See Appendix~\ref{app:zivboundproof}.
\end{proof}

The next two lemmata provide two different asymptotic lower bounds on the
mean squared error of the transmission strategy considered, each of which is
tighter for a different class of~$\e$. They hold regardless of the decoder used.
(The $\Omega$-notation is defined in Appendix~\ref{app:asymptotic}.)

\begin{lemma}
  \label{lem:lowerbound1}
  For an arbitrary function $\e(\snr) \ge 0$, the mean squared error satisfies
  \begin{equation*}
    \mse \in \Omega(\snr^{-n + (n-1)\e}).
  \end{equation*}
\end{lemma}

\begin{lemma}
  \label{lem:lowerbound2}
  For an arbitrary function $\e(\snr) \ge 0$, the mean squared error
  satisfies
  \begin{equation*}
    \mse \in \Omega(\snr^{-1+\e/2} \exp\{-c\snr^\e\}),
  \end{equation*}
  where $c>0$ does not depend on~$\snr$.
\end{lemma}

\emph{Discussion:} An immediate consequence of the lemmata is that the
theoretically optimal scaling $\snr^{-n}$ is not achievable with the given
encoding strategy: by Lemma~\ref{lem:lowerbound1} this would require $\e = 0$,
but following Lemma~\ref{lem:lowerbound2} the scaling is at best $\snr^{-1}$ if
$\e = 0 $.  More generally, which one of the two lower bounds decays more slowly
and is therefore tighter depends on the scaling of~$\e(\snr)$. How to
choose~$\e(\snr)$ optimally will be the subject of Theorem~\ref{thm:scalinglb}.

\begin{proof}[Proof of Lemma~\ref{lem:lowerbound1}]
  Assume $\Delta \in [0, \beta^{-(n-1)})$ and define for $j \in \Z$
  \[ \I_j^\Delta = \left[ (j - \frac12 )\beta^{-(n-1)}, 
    (j + \frac12 ) \beta^{-(n-1)} - \Delta \right).\]
  It can be verified from~\eqref{eq:QEdef} that if $s \in \I_j^\Delta$ for
  some~$j$, the following properties hold: 1) $Q_i(s) = Q_i(s+\Delta)$ for
  $i=1$, \dots, $n-1$, and 2) $E_{n-1}(s+\Delta) - E_{n-1}(s) =
  \beta^{n-1}\Delta$.  From~\eqref{eq:Xdef} it follows that
  $s \in \I_j^\Delta$ implies $d(s, \Delta) = \sqrt{P/\gamma^2} \beta^{n-1}\Delta$.

  We now apply Lemma~\ref{lem:zivbound} and restrict the integral to the
  set~$\psi(\Delta) \deq [A,B-\Delta) \cap \bigcup_{j\in\Z} \I_j^\Delta$. The
  lower bound is then relaxed to give
  \begin{equation*}
    \mse \ge \frac{\pmin}{4} \Delta^2 Q(\sqrt{\snr/\gamma^2} \beta^{n-1}
    \Delta/2) \int_{\psi(\Delta)} ds.
  \end{equation*}
  Letting $\Delta = 1/(\sqrt{\snr}\beta^{n-1})$ and using $\beta^2 \in
  \Theta(\snr^{1-\e})$ yields (for sufficiently large~$\snr$)
  \begin{equation*}
    \mse \ge c \snr^{-n+(n-1)\e} Q\left(1/2\gamma\right)
    \int_{\psi(\Delta)} ds.
  \end{equation*}

  The proof is almost complete, but we still have to show that
  $\int_{\psi(\Delta)}ds$ can be bounded below by a constant for large SNR. The
  length of a single interval~$\I_j^\Delta$ is $\beta^{-(n-1)} - \Delta$. Within
  $[A,B-\Delta)$ there are $(B-A-\Delta)\beta^{n-1}$ such
  intervals. The total length of all intervals~$\I_j^\Delta$ in $[A, B-\Delta)$
  is therefore
  \[ \int_{\psi(\Delta)} ds = (B-A-\Delta)
  (1 - \beta^{n-1}\Delta), \]
  which, for the given values of~$\beta$ and~$\Delta$, 
  converges to $B-A$ for $\snr \ra \infty$ and thus can be lower bounded by a
  constant for $\snr$ greater than some $\snr_0$. With this, the proof is
  complete.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:lowerbound2}]
  Observe first that~\eqref{eq:QEdef} implies $Q_1(s + \beta^{-1}) = Q_1(s) +
  \beta^{-1}$ and $E_1(s + \beta^{-1}) = E_1(s)$. Since all $Q_i$ and $E_i$ for
  $i \ge 2$ are by recursion a function of $E_1$ only, $Q_i(s) = Q_i(s +
  \beta^{-1})$ for $i = 2$, \dots, $n-1$, and $E_{n-1}(s) = E_{n-1}(s +
  \beta^{-1})$. Consequently,  $X_i(s) = X_i(s + \beta^{-1})$ for all $i =
  2$, \dots, $n$. By~\eqref{eq:Xdef} and the above, the Euclidean distance
  between $\X(s)$ and~$\X(s+\beta^{-1})$ is therefore
  \begin{equation}
    \label{eq:xbetadist}
    \frac{\sqrt P}{\gamma} |Q_1(s) - Q_1(s+\beta^{-1})| 
    = \frac{\sqrt P}{\gamma\beta}.
  \end{equation}

  We now apply Lemma~\ref{lem:zivbound} with $\Delta = \beta^{-1}$. The
  parameter $\beta$ will be chosen to increase with the SNR, therefore $\Delta
  \in [0, B-A)$ holds for sufficiently large values of~$\snr$.

  Using~\eqref{eq:xbetadist}, the resulting bound on the mean squared error is
  \begin{equation*}
    \mse \ge \frac{\pmin}{4} \beta^{-2}
    Q\left (\frac{\sqrt{\snr}}{2\gamma\beta}  \right) (B-A-\beta^{-1}
    ).
  \end{equation*}
  Because $\beta^2 \in \Theta(\snr^{1-\e})$ (see \remref{betaepswlog}),
  $\sqrt{\snr}/\beta \in \Theta(\snr^{\e/2})$. If $\e(\snr)$ is such that
  $\limtoinf{\snr} \snr^{\e(\snr)} = \infty$, use the fact that $Q(x)$~converges
  to $e^{-x^2/2}/\sqrt{2\pi}x$ (cf.~\cite[\Spg 26.2.12]{AbramowitzS1964}).
  Otherwise $\snr^{\e(\snr)}$ is upper bounded by a constant, in which case
  $Q(x)$ is lower bounded and so is $e^{-x^2/2}/x$ for $x = \snr^{\e(\snr)}$.
  In any case, for sufficiently large values of~$\snr$,
  \begin{equation*}
    \mse \ge c_1 \snr^{-1 + \e/2} \exp\{-c_2\snr^\e\},
  \end{equation*}
  with $c_1$ and $c_2$ positive constants that do not depend on~$\snr$, thus
  proving the lemma.
\end{proof}

The following lemma will be used to prove Theorem~\ref{thm:scalinglb}, the main
result of this section.

\begin{lemma}
  \label{lem:epssolution}
  Define $W(x)$ to be the function that satisfies $W(x)e^{W(x)} = x$ for $x >
  0$.  This function is well defined and is sometimes called the \emph{Lambert
  $W$-function}~\textnormal{\cite{CorlessGHJK1996}}. Then for $\snr > 1$ and
  arbitrary real constants $a$, $b>0$, and $c > 0$, 
  \begin{equation}
    \label{eq:epsequation}
    \snr^{a+b\e} = \exp\{-c\snr^\e\},
  \end{equation}
  if and only if
  \begin{equation}
    \label{eq:epssolution}
    \snr^\e = (b/c) W(c\snr^{-a/b} / b).
  \end{equation}
\end{lemma}

\begin{proof}
  Let $\snr>1$. Since $\snr^{a+b\e}$ is strictly increasing and
  $\exp\{-c \snr^\e\}$ is strictly decreasing in~$\e$, there is at most one
  solution to~\eqref{eq:epsequation}.  Assume now $\snr^\e$ is as
  in~\eqref{eq:epssolution}. Then
  \begin{equation*}
    \exp\{-c \snr^\e\} = \exp\{-b W(c \snr^{-a/b}/b)\}.
  \end{equation*}
  On the other hand,
  \begin{align*}
    \snr^{a+b\e} &= \snr^a \left( (b/c) W(c\snr^{-a/b}/b) \right)^b \\
    &= \left( W(c\snr^{-a/b}/b) / (c\snr^{-a/b}/b) \right)^b.
  \end{align*}
  By definition, $W(x)/x = e^{-W(x)}$, so the above is equal to
  \begin{equation*}
    \snr^{a+b\e} = \exp\{-bW(c \snr^{-a/b}/b)\},
  \end{equation*}
  which proves the claim.
\end{proof}

The following is the main result of this section.
\begin{theorem}
  \label{thm:scalinglb}
  For any parameter~$\beta$ and for any decoder, the mean squared error of the
  transmission strategy described in Section~\ref{sec:commscheme} satisfies
  \begin{equation*}
    \mse \in \Omega(\snr^{-n}(\log\snr)^{n-1}).
  \end{equation*}
\end{theorem}

% [Marius] Removed the following paragraph, since in this chapter the
% achievability result comes _after_ the bound.
%\emph{Discussion:} The asymptotic lower bound on the mean squared error given
%by the theorem coincides with the asymptotic performance achieved by the
%suboptimal decoder in Section~\ref{sec:achievable}; the bound is therefore
%asymptotically tight. 

\begin{proof}
  For notational simplicity define
  \begin{align*}
    l_1(\snr, \e) &= \snr^{-n+(n-1)\e} \quad\text{and} \\
    l_2(\snr,\e) &= \snr^{-1+\e/2} \exp\{-c\snr^\e\}.
  \end{align*}
  By Lemmata~\ref{lem:lowerbound1} and~\ref{lem:lowerbound2},
  \begin{equation*}
    \mse \in \Omega\big(\max \left( l_1(\snr,\e), l_2(\snr,\e) \right) \big).
  \end{equation*}
  The optimal parameter $\e(\snr)$ is therefore such that for
  any~$\snr$
  \begin{equation}
    \label{eq:epsmax}
    \max\left( l_1(\snr,\e), l_2(\snr,\e) \right)
  \end{equation}
  is minimized. Now for any fixed~$\snr$, $l_1(\snr,\e)$ is increasing in~$\e$,
  and $l_2(\snr,\e)$ is increasing in~$\e$ for $0 \le \e < \xi =
  \log(1/2c)/\log\snr$ and decreasing in~$\e$ for $\e \ge \xi$.  The maximum
  in~\eqref{eq:epsmax} is therefore minimized either for $\e = 0$ or for $\e \ge
  \xi$ such that $l_1(\e) = l_2(\e)$. As remarked before, $\e = 0$ leads to
  $\mse \in \Omega(\snr^{-1})$. In the following, let thus $\e(\snr)$ be such
  that $l_1(\snr,\e) = l_2(\snr, \e)$, to see whether this gives a better lower
  bound.  Inserting the definitions of $l_1$ and $l_2$ and rearranging the terms
  yields
  \begin{equation*}
    \snr^{-(n-1) + (n-3/2)\e} = \exp\{-c\snr^\e\},
  \end{equation*}
  which is of the form~\eqref{eq:epsequation} with $a = -(n-1)$ and $b = n-3/2$.
  By Lemma~\ref{lem:epssolution}, for $\snr > 1$,
  \begin{equation*}
    \snr^\e = \frac{n-3/2}{c}
    W\left( \frac{c\snr^{\frac{2(n-1)}{2n-3}}}{n-3/2} \right).
  \end{equation*}
  Using L'H\^opital's rule and because the derivative of
  $W(x)$ is $W(x)/[x(1 + W(x))]$ (cf.~\cite{CorlessGHJK1996}), it is
  straightforward to check that $W(x)/\log x$ converges to~$1$ for $x \ra
  \infty$.  For sufficiently large $\snr$, therefore, there exists a constant
  $c_1 > 0$ such that
  \begin{equation*}
    \snr^\e \ge c_1 \frac{n-3/2}{c} \left[ \frac{2(n-1)}{2n-3}\log\snr -
    \log\left(\frac{n-3/2}{c}\right)
    \right],
  \end{equation*}
  and so $\snr^\e \in \Omega(\log\snr)$. Plugging this into the bound of
  Lemma~\ref{lem:lowerbound1} finally results in\footnote{If $a(x) \in
  \Omega(f(x))$ and $b(x) \in \Omega(g(x))$, then $a(x)b(x)^m \in
  \Omega(f(x)g(x)^m)$.}
  \begin{equation*}
    \mse \in \Omega(\snr^{-n}(\log\snr)^{n-1}),
  \end{equation*}
  and no choice of $\e(\snr)$ can improve this bound.
\end{proof}


\subsection{Asymptotical Achievability of Lower Bound}
\label{sec:achievable}

The previous section showed that the hybrid transmission strategy studied here
achieves an SDR scaling of at best $\snr^n/(\log\snr)^{n-1}$. Using a simple
decoder, this scaling is in fact achievable, as will now be shown.

\subsubsection{A Suboptimal Decoder}

The $X_i$ are transmitted across the channel, producing at the channel output
the symbols
\begin{equation*}
  Y_i = X_i + Z_i, \quad i = 1, \dots, n,
\end{equation*}
where the $Z_i$ are iid Gaussian random variables of variance~$\szq$. 
To estimate $S$ from  $Y_1$, \dots, $Y_n$, the decoder first
computes separate estimates $\Qh_1$, \dots, $\Qh_{n-1}$ and $\Eh_{n-1}$, and
then combines them to obtain the final estimate~$\Sh$.  While this strategy is
suboptimal in terms of achieving a small MSE, it will turn out to be good enough
to achieve the desired SDR scaling.

The $Q_i$ are estimated using a maximum likelihood (ML) decoder, which yields
the minimum distance estimate
\begin{equation}
  \label{eq:mldecoder}
  \Qh_i = \frac{1}{\beta} \arg \min_{j\in \Z} \left| \frac{j\sqrt{P}}
  {\gamma\beta} - Y_i \right|.
\end{equation}
To estimate $E_{n-1}$, a linear minimum mean-square error (LMMSE)
estimator is used (see \eg~\cite[Section~8.3]{Scharf1990}), which computes
\begin{equation}
  \label{eq:lmmse}
  \Eh_{n-1} = \frac{\E[E_{n-1} Y_n]}{\E[Y_n^2]} Y_n.
\end{equation}
Finally, using~\eqref{eq:unwraprec}, the estimate of~$S$ is computed as
\begin{equation}
  \label{eq:unwrapestim}
  \Sh = \sum_{i=1}^{n-1} \frac{1}{\beta^{i-1}} \Qh_i + \frac{1}{\beta^{n-1}}
  \Eh_{n-1}.
\end{equation}


\subsubsection{Error Analysis}

The overall MSE $\E[(S-\Sh)^2]$ can be broken up into contributions due to the
errors in decoding $Q_i$ and $E_{n-1}$ as follows. From~\eqref{eq:unwraprec}
and~\eqref{eq:unwrapestim}, the difference between $S$ and $\Sh$ is
\begin{equation*}
  S - \Sh = \sum_{i=1}^{n-1} \frac1{\beta^{i-1}} (Q_i - \Qh_i) + \frac1{\beta^{n-1}}
  (E_{n-1} - \Eh_{n-1}).
\end{equation*}
The error terms $Q_i - \Qh_i$ depend only on the noise of the respective channel
uses and are therefore independent of each other and of $E_{n-1} - \Eh_{n-1}$,
so the error variance can be written componentwise as
\begin{equation}
  \label{eq:totalerror}
  \E[(S-\Sh)^2] = \sum_{i=1}^{n-1} \frac{1}{\beta^{2(i-1)}} \Eqi +
  \frac{1}{\beta^{2(n-1)}} \Ee, 
\end{equation}
where $\Eqi \deq \E[(Q_i - \Qh_i)^2]$ and $\Ee \deq \E[(E_{n-1} -
\Eh_{n-1})^2]$.

\begin{lemma}
  \label{lem:eqbound}
  For each $i = 1$, \dots, $n-1$, 
  \begin{equation}
    \label{eq:eqidecay}
    \Eqi \in O\left(\exp\{-c \snr/\beta^2\}\right),
  \end{equation}
  where $\snr = P/\szq$ and $c > 0$~is a constant.
\end{lemma}
(The $O$-notation is defined in Appendix~\ref{app:asymptotic}.)

\begin{proof}
  Define the interval
  \begin{equation*}
    \I_j = \left[ \frac{(j - \frac12) \sqrt{P}}{\gamma\beta},
    \frac{(j + \frac12) \sqrt{P}}{\gamma\beta } \right).
  \end{equation*}
  According to the minimum distance decoder~\eqref{eq:mldecoder}, $\Qh_i - Q_i
  \le j/\beta$ whenever $Z_i \in \I_j$.  The error $\Eqi$ satisfies thus
  \begin{align}
    \E[(Q_i - \Qh_i)^2] &\le \frac{1}{\beta^2} \sum_{j \in \Z} j^2 \Pr[Z_i \in
    \I_j]  \nonumber \\
    &= \frac{2}{\beta^2} \sum_{j = 1}^\infty j^2 \Pr[Z_i \in \I_j],
    \label{eq:eqexact}
  \end{align}
  where the second equality follows from the symmetry of the distribution
  of~$Z_i$. Now,
  \begin{equation*}
    \Pr[Z_i \in \I_j] = Q\left( \frac{(j - \frac12) \sqrt{\snr}}{\gamma\beta}
    \right) - Q\left( \frac{(j + \frac12) \sqrt{\snr}}{\gamma\beta } \right),
  \end{equation*}
  where
  \begin{equation*}
    Q(x) = \frac{1}{\sqrt{2\pi}} \int_x^\infty e^{-\xi^2/2} d\xi,
  \end{equation*}
  which can be bounded from above for $x \ge 0$ as
  \begin{equation*}
    Q(x) \le \frac12 e^{-x^2/2}.
  \end{equation*}
  Since $\beta \ge 1$, \eqref{eq:eqexact} is then upper bounded by
  \begin{equation*}
    \Eqi \le \sum_{j=1}^\infty j^2 \exp\left\{ - \frac{(j - 1/2)^2
    \snr}{2\gamma^2\beta^2} \right\}.
  \end{equation*}
  Note that for $j \ge 2$, $(j - 1/2)^2 > j$.  Thus
  \begin{eqnarray}
    \Eqi &\le & \exp \left \{ - \frac{\snr}{8 \gamma^2 \beta^2} \right\}
    \nonumber \\
    & & \mbox{} + 
    \sum_{j = 2}^\infty j^2 \exp \left \{ - \frac{j \snr}{2\gamma^2 \beta^2}
    \right\}. \label{eq:eqibound}
  \end{eqnarray}
  To bound the infinite sum, use 
  \begin{equation}
    \label{eq:geomsum}
    \sum_{j=2}^\infty j^2 p^j \le \sum_{j=1}^\infty j^2 p^j = 
    \frac{p^2+p}{(1-p)^3}
  \end{equation}
  with $p = \exp\{-\snr/2 \gamma^2 \beta^2\}$. The first term
  of~\eqref{eq:eqibound} thus dominates for large values of
  $\snr/\beta^2$ and
  \begin{equation*}
    \Eqi \le c_1\exp\left\{ - \frac{\snr}{c_2 \beta^2} \right\}
  \end{equation*}
  for some~$c_1 > 0$ and $c_2 = 8 \gamma^2$, which completes the proof. 
\end{proof}

\begin{lemma}
  \label{lem:eedecay}
  $\Ee \in O(\snr^{-1})$. 
\end{lemma}
\begin{proof}
  The mean-squared error that results from the LMMSE estimation~\eqref{eq:lmmse}
  is
  \begin{equation}
    \label{eq:lmmse-error}
    \Ee = \seq - \frac{(\E[E_{n-1}
    Y_n])^2}{\E[Y_n^2]}. 
  \end{equation}
  Since
  \begin{equation*}
    Y_n = X_n + Z_n = \frac{\sqrt{P}}{\gamma} E_{n-1} + Z_n,
  \end{equation*}
  we have $\E[E_{n-1}Y_n] = \sqrt{P}\seq/\gamma$. Moreover, $\E[Y_n^2] = \E[X^2]
  + \E[Z^2] = P\seq / \gamma^2 +\szq$.  Inserting this
  into~\eqref{eq:lmmse-error} we obtain
  \begin{align*}
    \Ee &= \seq - \frac{P \se^4/\gamma^2}{P\seq/\gamma^2 + \szq} \\
    &= \seq \left( 1 - \frac{P\seq/\gamma^2}{P\seq/\gamma^2 + \szq} \right) \\
    &= \frac{\seq}{1 + \snr\seq/\gamma^2} \\
    & < \frac{\gamma^2}{\snr}.
  \end{align*}
  Since $\gamma$ is independent of~SNR (cf.\
  Proposition~\ref{prop:qeproperties}), $\Ee \in O(\snr^{-1})$ as claimed.
\end{proof}


\subsubsection{Optimizing the Quantization Resolution}

Recall the formula for the overall error
\begin{equation*}
  \E[(S-\Sh)^2] = \sum_{i=1}^{n-1} \frac{1}{\beta^{2(i-1)}} \Eqi +
  \frac{1}{\beta^{2(n-1)}} \Ee.
\end{equation*}
According to Lemma~\ref{lem:eqbound}, $\Eqi$ decreases exponentially
when $\snr/\beta^2$ goes to infinity. This happens for increasing SNR if
$\beta$~is set \eg\ to
\begin{equation*}
  \beta = \lceil \snr^{(1-\e)/2} \rceil
\end{equation*}
for some $\e > 0$, in which case $\Eqi \in O\left(\exp(-c \snr^\e) \right)$.
From this and Lemma~\ref{lem:eedecay}, the overall error satisfies
\begin{equation}
  \label{eq:overallO}
  \E[(S-\Sh)^2] \in O(\snr^{-(n - \e')}),
\end{equation}
where $\e' = (n-1)\e$ can be made as small as desired.
%The scaling exponent for
%a fixed $\e$ satisfies therefore
%\begin{equation}
%  \label{eq:sdrepsilon}
%  \limtoinf{\snr} \frac{\log\sdr}{\log\snr} \ge
%  \limtoinf{\snr} \frac{\log \ssq + (n - \e') \log\snr}{\log \snr} = n - \e'. 
%\end{equation}

Note that the choice of $\e$ represents a tradeoff: for small $\e$ the error due
to the ``discrete'' part vanishes only slowly, but the scaling exponent in the
limit is larger. For larger $\e$, $\Eq$ vanishes quickly but the resulting
exponent is smaller. The remainder of this section shows how to choose $\e$ as a
function of~$\snr$ to optimize SDR scaling. 

Let
\begin{equation}
  \label{eq:esnrdecay}
  \e = \e(\snr) = \frac{\log(n \log\snr / c)}{\log\snr},
\end{equation}
where $c$~is the constant indicating the decay of $\Eqi$ in~\eqref{eq:eqidecay}.
With this choice of $\e$,
\begin{align*}
  \Eqi &\in O\left( \exp\left( - c \snr^\e \right) \right) \\
 % &= O\left( \exp\left( - \exp\left( \e \log \snr \right) \right) \right) \\
 &= O(\snr^{-n}),
\end{align*}
hence the overall error is still dominated as in~\eqref{eq:overallO}.
Inserting~\eqref{eq:esnrdecay} in~\eqref{eq:overallO},
\begin{equation*}
  \mse \in \snr^n (\log\snr)^{-(n-1)},
\end{equation*}
which coincides with the lower bound on the MSE scaling of \thmref{scalinglb}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: GENERAL BANDWIDTH EXPANSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{General Bandwidth Expansion}\label{sec:genbwexp}

For simplicity assume for now that the support of the source is limited to
the interval~$[-1/2, 1/2]$; the general case will be treated afterwards.

\subsection{Transmission Strategy}

The strategy used to encode the source symbols $S_1$, \dots, $S_k$ into the
channel inputs $X_1$, \dots, $X_n$ is displayed schematically on
\figref{knencoding} for~$k=3$ and~$n=5$. 
\begin{figure}[tbph]
  \begin{center}
    \input{figures/knencoding.tex_t}
  \end{center}
  \caption{Schematic display of the transmission strategy of \secref{genbwexp}
  for~$k = 3$ and~$n=5$. The boxes labeled~\textsf{Q} are uniform scalar
  quantizers with resolution~$\beta$ as described in~\eqref{eq:QEdefkn}. The
  part marked in thick lines corresponds exactly to the encoding scheme
  described in \secref{scalarquant} to encode one source symbol into three
  channel uses; compare this to \figref{1nencoding}.}
  \label{fig:knencoding}
\end{figure}
The encoder consists of $k$~parallel scalar encoders that encode each~$S_i$ into
$n-k$~quantizer outputs $Q_{i,1}$, \dots, $Q_{i,n-k}$ and a quantization
error~$E_{j,n-k}$, just like for the case~$k=1$. The quantizer outputs at each
level are then combined into a single value $Q_{\tot,j}$
(cf.~\figref{knencoding}). More precisely, letting $i \in \{1, \dots, k\}$ be
the index of the source symbols, the encoder defines $E_{i,0} = S_i$ for
each~$i$ and computes
\begin{align}
  Q_{i,j} &= \frac{1}{\beta} \Int(\beta E_{i,j-1}) \quad\text{and} \nonumber \\
  E_{i,j} &= \beta(E_{i,j-1} - Q_{i,j}). \label{eq:QEdefkn}
\end{align}
(This is exactly the same as~\eqref{eq:QEdef}, except for the addition of the
subscript~$i$; see also \figref{knencoding}.) For each quantization level~$j =
1$, \dots, $n-k$, the $k$~parallel quantizer outputs are combined into
\begin{equation}
  \label{eq:Qtot}
  Q_{\tot,j} = \sum_{i=1}^k \frac{1}{\beta^{i-1}} Q_{i,j}.
\end{equation}
Since each $Q_{i,j}$ is a multiple of $1/\beta$ and is assumed to lie in $[-1/2,
1/2]$, the mapping from the $Q_{i,j}$ into~$Q_{\tot,j}$ is invertible, and the
inverse can be recursively computed as
\begin{align}
  Q_{k,j} &= \beta^{k-1} Q_{\tot,j} \bmod 1 \nonumber \\
  Q_{k-i,j} &= \beta^{k-1-i} Q_{\tot,j} - \sum_{l = 1}^i \beta^{-l} Q_{k - i +
  l} \bmod 1, \quad\text{$i = 1$, \dots, $k-1$,} 
  \label{eq:Qtotinv}
\end{align}
where $x \bmod 1 \deq x - \Int(x)$.

Point~2 of \propref{qeproperties} from \secref{scalarquant} applies here as
well, so there exists a constant~$\gamma^2$ that upper bounds the variances of
all $Q_{i,j}$ and~$E_{i,j}$ for all~$\beta$. The channel inputs are thus
\begin{align*}
  X_j &= (\sqrt{P}/\gamma) Q_{\tot,j} &&\quad\text{for $j = 1$, \dots, $n-k$ and}
  \\
  X_j &= (\sqrt{P}/\gamma) E_{j-n+k,n-k} &&\quad\text{for $j = n-k+1$, \dots,
  $n$.}
\end{align*}


\subsection{Decoder}

Just like when $k=1$, the decoder first computes separate estimates~$\Qh_{i,j}$
and~$E_{i,n-k}$ and then combines them into the estimates~$\Sh_i$. The
$\Qh_{i,j}$ are obtained using a maximum likelihood (minimum distance) estimate
of~$Q_{\tot,j}$ and then applying~\eqref{eq:Qtotinv}. The estimate of
$Q_{\tot,j}$ is
\begin{equation*}
  \Qh_{\tot,j} = \frac{1}{\beta^k} \arg\min_{l\in\Z}
  \left| \frac{l\sqrt P}{\gamma \beta^k} - Y_j \right|,
\end{equation*}
and the quantization errors~$E_{i,n-k}$ are estimated as
\begin{equation*}
  E_{i,n-k} = \frac{\E[E_{i,n-k} Y_{n-k+i}]}{\E[Y_{n-k+i}^2]}.
\end{equation*}
Using~\eqref{eq:unwraprec} and~\eqref{eq:Qtotinv}, the final estimate for $i =
1$, \dots, $k$ is
\begin{equation*}
  \Sh_i = \sum_{j=1}^{n-k} \frac{1}{\beta^{j-1}} \Qh_{i,j} +
  \frac{1}{\beta^{n-k}} \Eh_{i,n-k}.
\end{equation*}


\subsection{Error Analysis}

As in the case~$k=1$, the overall mean squared error can be written as
\begin{equation*}
  \mse = \sum_{j=1}^{n-k} \frac{1}{\beta^{2(j-1)}} \Err_{Q,i,j} + 
  \frac{1}{\beta^{2(n-k)}} \Err_{E,i}
\end{equation*}
where $\Err_{Q,i,j} \deq \E[(\Qh_{i,j} - Q_{i,j})^2]$ and $\Err_{E,i} \deq
\E[(\Eh_{i,n-k} - E_{i,n-k})^2]$. The behavior of the~$\Err_{E,i}$ is exactly
the same as when~$k=1$; the following lemma is therefore given without proof.

\begin{lemma}
  \label{lem:Eedecayk}
  The estimation error of the $E_{i,n-k}$ satisfies
  \begin{equation*}
    \Err_{E,i} \in O(\snr^{-1})
  \end{equation*}
  for all $i = 1$, \dots, $k$. 
\end{lemma}

The main difference to the case~$k=1$ concerns the behavior of~$\Err_{Q,i,j}$.
Because $k$~quantizer outputs are packed into a single channel input as
described by~\eqref{eq:Qtot}, $\beta^2$ is raised to the exponent~$k$ in the
following lemma (compare with \lemref{eqbound}).

\begin{lemma}
  \label{lem:eqboundk}
  For each $i = 1$, \dots, $k$ and for each $j = 1$, \dots, $n-k$,
  \begin{equation*}
    \Err_{Q,i,j} \in O(\exp\{-c \snr/\beta^{2k} \})
  \end{equation*}
  where~$c > 0$ is a constant.
\end{lemma}

\begin{proof}
  For any $i$ and~$j$, $| \Qh_{i,j} - Q_{i,j}| \ne 0$ only if $|Z_j| \ge
  \sqrt{P} / 2 \gamma \beta^k$. Furthermore, since $\Qh_{i,j}, Q_{i,j} \in
  [-1/2, 1/2)$, $|\Qh_{i,j} - Q_{i,j}| \le 1$. The error~$\Err_{Q,i,j}$ can thus
  be upper bounded as
  \begin{align*}
    \E[(\Qh_{i,j} - Q_{i,j})^2] &\le \Pr\left[|Z_j| \ge \frac{\sqrt{P}}{2 \gamma
    \beta^k} \right] \\
    &= 2 Q\left( \frac{\sqrt{\snr}}{2\gamma\beta^k} \right) \\
    &\le \exp\{-c \snr / \beta^{2k} \}
  \end{align*}
  with $c = 1/8\gamma$.
\end{proof}


Let now $\beta = \lceil \snr^{(1-\e)/2k} \rceil$. Then $\beta^{2k} \in
O(\snr^{1-\e})$, and the bound from \lemref{eqboundk} becomes
\begin{equation}
  \label{eq:qboundkepsilon}
  \Err_{Q,i,j} \in O(\exp\{-c \snr^\e\})
\end{equation}
(where $c > 0$ is not necessarily the same constant as in \lemref{eqboundk}).
Moreover, using \lemref{Eedecayk},
\begin{equation}
  \label{eq:Eeboundkepsilon}
  \frac{\Err_{E,i}}{\beta^{2(n-k)}} \in O(\snr^{\frac{n}{k} - \e
  \frac{n-k}{k}}).
\end{equation}

The final step is to choose $\e$ as a function of~$\snr$ (again just as
for~$k=1$). Let
\begin{equation*}
  \e(\snr) = \frac{\log(n \log\snr / c)}{\log\snr}.
\end{equation*}
Inserting this in~\eqref{eq:qboundkepsilon} and~\eqref{eq:Eeboundkepsilon}, 
\begin{align*}
  \Err_{Q,i,j} &\in O(\snr^{-n}) \quad\text{and} \\
  \Err_{E,i} &\in O(\snr^{-n/k} (\log\snr)^{(n-k)/k}).
\end{align*}
The overall MSE scales thus as
\begin{equation*}
  \mse \in O(\snr^{-n/k} (\log\snr)^{(n-k)/k}).
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: LATTICE QUANTIZERS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Extension to General Sources}

The assumption in \secref{genbwexp} has so far been that the support of the
source is limited to~$[-1/2,1/2]$. If a source~$S$ has support outside this
interval but its support still lies within a bounded set, just define $S' =
S/\alpha$, with $\alpha > 1$ such that $S' \in [-1/2, 1/2]$. Then use
the described scheme to transmit~$S'$ and let $\Sh = \alpha \Sh'$. The
incurred distortion is $\E[(S - \Sh)^2] = \alpha^2 \E[(S' - \Sh')^2]$; the SDR
therefore still scales in the same way as when~$S \in [-1/2, 1/2]$.



\section{Lattice Quantizers}\label{sec:latticequant}

The scalar quantizer scheme described in the previous section can be extended
quite easily to treat blocks of $m$ source symbols and to use lattice
quantizers. For those unfamiliar with lattices or in need of a refresher, a
concise presentation of the necessary concepts and results can be found in
\appref{latticebasics}.

The conclusion of this section is that for no finite~$m$ a scaling better than
that for $m=1$ can be obtained. Choosing a high dimensional lattice does result,
however, in larger multiplying constants.


\subsection{Transmission Strategy}

The procedure to encode a vector~$\S$ of $m$~source symbols into $mn$~channel
input vectors $\X_1$, \ldots, $\X_n$ using the lattice~$\Lambda$ for
quantization is analog to that in \secref{scalarquant}, except that now all
involved quantities are $m$-dimensional vectors.

Let $\Lambda$ be some fixed lattice of dimension~$m$ and define $\Evc_0 = \S$.
For $i = 1$, \ldots, $n-1$ define
\begin{align}
  \Q_i &= \QLb(\Evc_{i-1})  \nonumber \\
  \Evc_i &= \beta ( \Evc_{i-1} - \Q_i), \label{eq:latticeQE}
\end{align}
where $\beta \in \NN$. According to \lemref{latquantvar}, $\E[\|\Q_i\|^2]$~is
upper bounded by a constant independent of~$\beta$. Moreover, $\Evc_i$
is contained within the Voronoi region of~$\Lambda$ around the origin, so by
\lemref{voronoivarbound} $\E[\|\Evc_i\|^2]$ is also upper bounded independent
of~$\beta$. Let $\gamma$~denote the upper bound on~$\E[\|\Q_i\|^2]$
and~$\E[\|\Evc_i\|^2]$.  The channel inputs are then given by
\begin{align*}
  \X_i &= \frac{\sqrt{mP}}{\gamma} \Q_i \quad \text{for $i = 1$, \dots, $n-1$
  and} \\
  \X_n &= \frac{\sqrt{mP}}{\gamma} \Evc_{n-1}.
\end{align*}
By the above argument, this ensures that $\E[\X_i^2]/m \le P$ for all~$i$.



\subsection{Error Lower Bound}

Ziv's lower bound on the mean squared error, used in \secref{scalarquant},
allows a straightforward extension to vector sources. With this, essentially the
same argument sequence as in \secref{scalarquant} can be used to lower bound the
squared error, independent of the particular decoder used.
(We again write the quantization resolution in terms of~$\snr$
as $\beta = \lceil \snr^{(1-\e)/2} \rceil$ without loss of generality
(cf.~\remref{betaepswlog}).)

\begin{lemma}
  \label{lem:zivboundvec}
  Consider a communication system where a continuous-valued source vector~$\S$
  is encoded into a vector~$\X(\S)$, sent across independent parallel scalar
  AWGN channels with noise variance~$\szq$, and decoded at the receiver to
  produce an estimate~$\Shv$. If the density $f_{\S}(\s)$ of the source is such
  that there exists a set~$\Xi$ and a number $\pmin > 0$ such that $f_{\S}(\s)
  \ge \pmin$ whenever $s \in \Xi$, then for any vector~$\Dv$ the mean squared
  error incurred by the communication system satisfies
  \begin{equation*}
    \msev \ge \pmin  \left( \frac{\|\Dv\|}{2}\right)^2 \int_{\Xi \cap (\Xi -
    \Dv)} Q(d(\s, \Dv)/2 \sz) d\s,
  \end{equation*}
  where $d(\s,\Dv) = \|\X(\s) - \X(\s + \Dv)\|$. (The addition of a set $A$ and
  a vector~$\x$ is defined as $A + \x = \{\vect{a} + \x : \vect{a} \in A\}$.)
\end{lemma}

\begin{remark}
  \label{rem:zivboundvec}
  The set $\Xi \cap (\Xi - \Dv)$ is the equivalent of the interval $[A,
  B-\Delta]$ in the scalar case. It has the property that for every $\s \in \Xi
  \cap (\Xi - \Dv)$, $\s + \Dv \in \Xi$. To get a meaningful lower bound,
  $\Dv$~should of course be chosen such that $\Xi \cap (\Xi - \Dv)$ is nonempty.
  Note also that when $\|\Dv\| \goesto 0$, the volume (or area) of $\Xi \cap
  (\Xi - \Dv)$ converges to the volume of~$\Xi$.
\end{remark}

\begin{proof}[Proof of \lemref{zivboundvec}]
  The proof is essentially the same as that for the scalar case
  (\lemref{zivbound}). The only difference is that the
  integrals~$\int_A^{B-\Delta}$ and~$\int_{A+\Delta}^B$ are replaced,
  respectively, with~$\int_{\Xi \cap (\Xi - \Dv)}$ and~$\int_{\Xi \cap (\Xi +
  \Dv)}$.
\end{proof}

Using \lemref{zivboundvec}, Lemmata~\ref{lem:lowerbound1}
and~\ref{lem:lowerbound2} can be rederived for the case of lattices; the
statements are in fact identical to the scalar case.

\begin{lemma}
  \label{lem:lowerbound1vec}
  For an arbitrary function~$\e(\snr)$, the mean squared error of the lattice
  quantizer transmission scheme of \secref{latticequant} satisfies
  \begin{equation*}
    \msev \in \Omega(\snr^{-n + (n-1)\e}).
  \end{equation*}
\end{lemma}

\begin{lemma}
  \label{lem:lowerbound2vec}
  For an arbitrary function $\e(\snr) \ge 0$, the mean squared error of the
  strategy of \secref{latticequant} satisfies
  \begin{equation*}
    \msev \in \Omega(\snr^{-1 + \e/2} \exp\{- c \snr^\e \},
  \end{equation*}
  where $c > 0$ does not depend on~$\snr$.
\end{lemma}

See \appref{lbvecproofs} for the proofs.

Since Lemmata~\ref{lem:lowerbound1vec} and~\ref{lem:lowerbound2vec} are
identical to Lemmata~\ref{lem:lowerbound1} and~\ref{lem:lowerbound2}, it is a
direct consequence that \thmref{scalinglb} also applies to lattice quantizers.
It is restated here for completeness.

\begin{theorem}
  \label{thm:scalinglbvec}
  For any choice of the parameter~$\beta$ (as a function of the SNR) and for any
  decoder, the mean squared error of the lattice quantizer transmission strategy
  of \secref{latticequant} satisfies
  \begin{equation*}
    \msev \in \Omega(\snr^{-n} (\log\snr)^{n-1}).
  \end{equation*}
\end{theorem}

\begin{proof}
  The proof is identical to that of \thmref{scalinglb}.
\end{proof}


\subsection{Achievability}

The scalar quantizer scheme of \secref{scalarquant} is nothing but a lattice
scheme using a onedimensional rectangular lattice. The achievability proof of
that section therefore applies to general lattices as well, and the bound of
\thmref{scalinglbvec} is trivially achievable using a lattice quantizer.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: HISTORICAL REMARKS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Historical Remarks}

Schemes similar to the one proposed here have been considered before. Indeed,
one of the first schemes to transmit an analog source across two uses of a
Gaussian channel was suggested by Shannon~\cite{Shannon1949}. Notice its
resemblance to the constellation studied here, shown in
Figure~\ref{fig:shannoncomparison}.
\begin{figure}
  \centerline{\subfloat[Shannon's original proposition.]{\input{figures/shannonline.tex_t}}
  \hfil
  \subfloat[Mapping proposed in this paper
  (for~$n=2$).]{\input{figures/ourconstellation.tex_t}} }% end centerline
  \caption{A minimum-delay source-channel code for $n=2$ can be visualized as a
  curve in $\R^2$ parametrized by the source. Here we compare the mapping
  presented in this chapter (right) to Shannon's original suggestion (left).}
  \label{fig:shannoncomparison}
\end{figure}

After Shannon, Wozencraft and Jacobs~\cite{WozencraftJ1965} were among the first
to study source-channel mappings as curves in $n$-dimensional space.
Ziv~\cite{Ziv1970} found important theoretical limitations of such mappings.
Much of the later work is due to Ramstad and his coauthors
(see~\cite{Ramstad2002}, \cite{FloorR2006}, \cite{CowardR2000,CowardR2000a},
\cite{WernerssonSR2007}, \cite{HeklandFR2009}). A proof that the performance of
minimal-delay codes is strictly smaller than that of codes with unrestricted
delay when $n>1$ was given in 2008 by Ingber et al.~\cite{IngberLZF2008}.

For $n=2$, the presented scheme is almost identical to the HSQLC scheme by
Coward~\cite{Coward2001}, which uses a numerically optimized quantizer,
transmitter and receiver to minimize the mean-squared error (MSE) for finite
values of the SNR. Coward correctly conjectured that the right strategy for $n >
2$ would be to repeatedly quantize the quantization error from the previous
step, which is exactly what we do here.

Another closely related communication scheme is the \emph{shift-map} scheme due
to Chen and Wornell~\cite{ChenW1998}.  Vaishampayan and
Costa~\cite{VaishampayanC2003} showed in their analysis that it achieves the
scaling exponent $n-\e$ for any $\e > 0$ if the relevant parameters are chosen
correctly as a function of the SNR. Up to rotation and a different constellation
shaping, the shift-map scheme is in fact virtually identical to the one
presented here, a fact that was pointed out recently by Taherzadeh and
Khandani~\cite{TaherzadehK2008}. In their own paper they develop a scheme that
achieves the optimal scaling exponent exactly and is in addition robust to SNR
estimation errors; their scheme, however, is based on rearranging the digits of
the binary expansion of the source and is thus quite different from the one
presented here.

Shamai, Verd\'u and Zamir~\cite{ShamaiVZ1998} used Wyner-Ziv coding to extend an
existing analog system with a digital code when additional bandwidth is
available. Mittal and Phamdo~\cite{MittalP2002} (see also the paper by Skoglund,
Phamdo and Alajaji~\cite{SkoglundPA2002}) split up the source into a quantized
part and a quantization error, much like we do here, but they use a
separation-based code (or ``tandem'' code) to transmit the quantization symbols.
Reznic et al.~\cite{ReznicFZ2006} use both quantization and Wyner-Ziv coding,
and their scheme includes Shamai et al.\ and Mittal \& Phamdo as extreme cases.
All three schemes, however, use long block codes for the digital phase and incur
correspondingly large delays, so they are not directly comparable with minimum
delay schemes.














%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                              %
% CHAPTER APPENDICES                                                           %
%                                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{subappendices}
  % [Marius] This is no longer needed, since we use the more general lemma that
  % gives the upper bound on the quantization error for all lattices.
%  \section{Proof of Lemma~\ref{lem:qvarconvergence}}
%  \label{app:lemma1proof}
%
%  Since all involved distributions are symmetric, $\E[Q_i] = 0$.  Writing $Q_i$
%  as a function of $E_{i-1}$, we have
%  \begin{equation}
%    \var(Q_i) = \E[Q_i^2] = \int_{-\infty}^\infty Q_i(\xi)^2 f(\xi) d\xi,
%    \label{eq:varqintegral}
%  \end{equation}
%  %\begin{align}
%  %  \var(Q_i) &= \E[Q_i^2] \nonumber\\
%  %  &= \int_{-\infty}^\infty Q_i(\xi)^2 f(\xi) d\xi,
%  %  \label{eq:varqintegral}
%  %\end{align}
%  where $f(\xi)$ is the pdf\footnote{probability density function} of $E_{i-1}$.
%  Now, $Q_i(\xi) = j/\beta$ whenever
%  \begin{equation*}
%    \xi \in \left[ \frac{j - 1/2}{\beta}, \frac{j + 1/2}{\beta} \right).
%  \end{equation*}
%  With this, the integral~\eqref{eq:varqintegral} becomes
%  \begin{align*}
%    \var(Q_i) &= \frac{1}{\beta^2} \sum_{j \in \Z} j^2 
%    \int_{\frac{j - 1/2}{\beta}}^{\frac{j + 1/2}{\beta}} f(\xi) d\xi \\
%    &= \sum_{j\in \Z} \left( \frac{j}{\beta} \right)^2 \left[ F\left( 
%    { \textstyle
%    \frac{j + 1/2}{\beta} }\right) - F \left( { \textstyle \frac{j - 1/2}{\beta}
%    } \right) \right],
%  \end{align*}
%  where $F(\xi)$ is the cdf\footnote{cumulative distribution function} of
%  $E_{i-1}$. As $\beta$ goes to infinity, this sum converges to a
%  Riemann-Stieltjes integral:
%  \begin{equation*}
%    \var(Q_i) \longrightarrow \int \xi^2 dF(\xi) = \var(E_{i-1}) \quad
%    \text{as $\beta \goesto \infty$.}
%  \end{equation*}
%  \qed


\section{Proof of \lemref{Qvarbound}}\label{app:qvarboundproof}

\begin{proof}
  Let $f(\xi)$ be the probability density function of~$E_{i-1}$. Then
  \begin{align*}
    \Var Q_i &= \frac{1}{\beta^2} \int_{\R} \Int(\beta E_{i-1})^2 f(\xi) d\xi \\
    &= \frac{1}{\beta^2} \sum_{i\in\Z} i^2
    \int_{\frac{i-1/2}{\beta}}^{\frac{i+1/2}{\beta}} f(\xi) d\xi.
  \end{align*}
  Whenever $\xi \in [(i-1/2)/\beta, (i+1/2)/\beta)$, $|i| \le \beta|\xi| + 1/2$,
  so
  \begin{align*}
    \Var Q_i &\le \frac{1}{\beta^2} \sum_{i\in \Z}
    \int_{\frac{i-1/2}{\beta}}^{\frac{i+1/2}{\beta}} (\beta|\xi| + 1/2)^2 
    f(\xi) d\xi \\
    &= \frac{1}{\beta^2} \int_{\R} (\beta|\xi| + 1/2)^2 f(\xi) d\xi \\
    &= \Var E_{i-1} + \frac{1}{4\beta^2} + \frac{1}{\beta} \int_{\R} |\xi|
    f(\xi)d\xi.
  \end{align*}
  To bound the last integral, use the fact that $\E[|E_{i-1}|] \le \sqrt{\Var
  E_{i-1}}$ to finally obtain
  \begin{equation*}
    \Var Q_i \le \Var E_{i-1} + \frac{\sqrt{\Var E_{i-1}}}{\beta} +
    \frac{1}{\beta^2}.
  \end{equation*}
\end{proof}
  

  \section{Proof of Ziv's Lower Bound (Lemma~\ref{lem:zivbound})}
  \label{app:zivboundproof}

  If we condition the mean squared error on~$S$ and use the assumption on~$p_S$
  we obtain
  \begin{equation*}
    \mse \ge \pmin \int_A^B \msecond ds.
  \end{equation*}
  For $\Delta \in [0, B-A]$ we can further bound this in two ways:
  \begin{align*}
    \mse &\ge \pmin \intabd \msecond ds \\
    \mse &\ge \pmin \int_{A+\Delta}^B \msecond ds \\
    &= \pmin \intabd \msecondd ds.
  \end{align*}
  Averaging the two lower bounds yields
  \begin{equation}
    \label{eq:avglbd}
    \mse \ge \frac{\pmin}{2} \intabd \bigg( \msecond + \\
    \msecondd \bigg) ds,
  \end{equation}
  and applying Markov's inequality to the expectation terms leads to
  \begin{equation}
    \label{eq:markov1}
    \msecond \ge \dhsq \Pr[|\Sh - S| \ge \Delta/2 \mid s]
  \end{equation}
  and
  \begin{equation}
    \label{eq:markov2}
    \msecondd \\
    \ge \dhsq \Pr[|\Sh - S - \Delta| \ge \Delta/2 \mid s+\Delta].
  \end{equation}

  Now suppose that we use the communication system in question for binary
  signaling. We want to send either $s$ or $s+\Delta$; at the decoder we use the
  estimate~$\Sh$ to decide for~$s$ or $s + \Delta$ depending on which one $\Sh$
  is closer to. When $s$ is sent, the decoder makes an error only if $|\Sh - s|
  \ge \Delta/2$; when $s + \Delta$ is sent, it makes an error only if $|\Sh - s
  - \Delta| \ge \Delta/2$. The conditional error probabilities therefore satisfy
  $\Pr[\text{error} | s] \le \Pr[|\Sh - S| \ge \Delta/2 \mid s]$ and
  $\Pr[\text{error} | s + \Delta] \le \Pr[|\Sh - S - \Delta| \ge \Delta/2 \mid s +
  \Delta]$. Applying this to~\eqref{eq:markov1} and~\eqref{eq:markov2} and
  inserting the result in~\eqref{eq:avglbd} yields
  \begin{equation}
    \label{eq:zivalmostproved}
    \mse \ge \pmin \dhsq \intabd \pe(s, \Delta) ds,
  \end{equation}
  where $\pe(s, \Delta) = \left(\Pr[\text{error}|s] + \Pr[\text{error}|s +
  \Delta] \right)/2$ is the average error probability.

  If $s$ and $s+\Delta$ are picked with equal probability and transmitted across
  $n$~parallel Gaussian channels as $\X(s)$ and $\X(s+\Delta)$, and if $d(s,
  \Delta) = \| \X(s) - \X(s + \Delta)\|$, then the error probability of the MAP
  decoder is $Q(d(s,\Delta) / 2 \sz)$, a standard result of communication theory
  (see \eg~\cite[Section~4.5]{WozencraftJ1965}). Because the MAP decoder minimizes
  the error probability, $Q(d(s,\Delta)/2\sz) \le \pe(s,\Delta)$, which, when
  inserted into~\eqref{eq:zivalmostproved}, completes the proof. \hfill\qed



  \section{Lattice Basics}\label{app:latticebasics}

  This section contains the very basics on lattices and lattice quantization
  needed in the remainer of the chapter. For a comprehensive treatment of
  lattices and/or quantization the reader is referred to the books by Conway and
  Sloane~\cite{ConwayS1988} and by Gersho and Gray~\cite{GershoG1992}.

  \begin{definition}
    An $n$-dimensional \emph{lattice} $\Lambda$ is a discrete set of vectors
    (points) in~$\R^n$ such that for any $\x$, $\y \in \Lambda$, $\x + \y \in
    \Lambda$. A sublattice of~$\Lambda$ is a subset $\Lambda' \subseteq \Lambda$
    that is itself a lattice. 
  \end{definition}

  \begin{example}
    \label{ex:scalarlattice}
    In~$\R$ there exists only a single lattice (up to scaling), the scalar
    lattice~$\Z$. Two examples of lattices in~$\R^2$ are displayed in
    \figref{r2lattices}.
  \end{example}
  \begin{figure}[tbph]
    \centerline{%
    \subfloat[Rectangular lattice.]{%
    \label{fig:rectlattice}\input{figures/rectlattice.tex_t}}%
    \hfil
    \subfloat[Hexagonal lattice.]{%
    \label{fig:hexlattice}\input{figures/hexlattice.tex_t}}%
    }
    \caption{Two lattices in~$\R^2$ and the corresponding partition into Voronoi
    regions.}
    \label{fig:r2lattices}
  \end{figure}
  \begin{proposition}
    \label{prop:intsublattice}
    If $\Lambda$ is a lattice and $\beta \in \NN$, then $\beta \Lambda$ is a
    sublattice of~$\Lambda$. (The set $\beta \Lambda$ is defined as $\{ \beta \x
    : \x \in \Lambda\}$.)
  \end{proposition}

  \begin{proof}
    By definition of a lattice, $\beta \Lambda \subseteq \Lambda$. Moreover, if
    $\x, \y \in \beta \Lambda$, then $\x = \beta \x'$ and $\y = \beta \y'$ with
    $\x', \y' \in \Lambda$. It follows that $\x + \y = \beta (\x' + \y') \in \beta
    \Lambda$, so $\beta\Lambda$ is itself a lattice.
  \end{proof}

  \begin{definition}
    The \emph{Voronoi region} $V(\p)$ of a lattice point $\p \in \Lambda$ is
    defined as \begin{equation*} V(\p) = \{\x \in \R^n : \|\x - \p\| \le \|\x -
      \q\|, \forall \q \in \Lambda\}, \end{equation*} \ie, $V(\p)$ is the set of
      points in $\R^n$ that at least as close to $\p$ as to any other lattice
      point.
  \end{definition}
  See \figref{r2lattices} for an illustration of the Voronoi region.

  \begin{definition}
    The \emph{packing radius} $\rho$ of a lattice is half the minimal distance
    between lattice points. Thus, $\rho$~is the largest radius of spheres that
    can be packed in~$\R^n$ by placing them at the lattice points.
  \end{definition}

  \begin{definition}
    The \emph{covering radius}~$R$ of a lattice~$\Lambda$ is the least upper
    bound for the distance from any point of~$\R^n$ to the closest point $\x \in
    \Lambda$. Thus, spheres of radius~$\rho$ around each lattice point will
    cover~$\R^n$, and no smaller radius will do.~\cite{ConwayS1988}
  \end{definition}

  The packing radius and the covering radius are illustrated on
  \figref{packingcoveringr} for a hexagonal lattice.
  \begin{figure}[tbph]
    \begin{center}
      \input{figures/packingcoveringr.tex_t}
    \end{center}
    \caption{The packing radius~$\rho$ and the covering radius~$R$ for the
    rectangular lattice and the hexagonal lattice.}
    \label{fig:packingcoveringr}
  \end{figure}

  \begin{definition}
    \label{def:latticequant}
    A \emph{lattice quantizer} $\QL : \R^n \ra \Lambda$ maps each point
    of~$\R^n$ to the closest lattice point. Thus, for any $\x \in \R^n$, $\y \in
    \Lambda$,
    \begin{equation*}
      \| \x - \QL(\x) \| \le \|\x - \y\|.
    \end{equation*}
  \end{definition}

  \begin{remark}
    \label{rem:latticequant}
    \defref{latticequant} does not unambiguously specify $\QL(\x)$ if $\x$~lies
    on the boundary between the Voronoi regions of two adjacent lattice points.
    Since quantization is only applied to continuous-valued random variables in
    this chapter, however, the probability of this happening is zero, and this
    ambiguity can be left alone without causing any trouble.
  \end{remark}

  \begin{example}
    Let $\Lambda = \Z / \beta$ for some $\beta >0$. The associated
    quantizer~$\QL$ maps each real number to the closest multiple of $1/\beta$.
  \end{example}

  %The following simple property will be useful later.
  %\begin{proposition}
  %  Let $\Lambda = \Z / \beta$ with $\beta \in \NN$ and let $x \in [-1/2, 1/2]$.
  %  Then
  %  \begin{equation*}
  %    \QL(x) \in
  %    \begin{cases}
  %      \left\{ -\frac{\beta/2}{\beta}, -\frac{\beta/2 - 1}{\beta}, \dots, 
  %      -\frac{1}{\beta}, 0, \frac1\beta, \dots, \frac{\beta/2}{\beta} \right\}
  %      & \text{if $\beta$ is even} \\
  %      \left\{ -\frac{(\beta - 1)/2}{\beta}, -\frac{(\beta-1)/2 - 1}{\beta}, 
  %      \dots, -\frac1\beta, 0, \frac1\beta, \dots, \frac{(\beta-1)/2}{\beta}
  %      \right\}
  %      & \text{if $\beta$ is odd}.
  %    \end{cases}
  %  \end{equation*}
  %\end{proposition}
  %
  %\begin{proof}
  %  The proof is left as an exercise for the reviewers.
  %\end{proof}
  %
  The next two lemmata are useful to bound the transmit power when transmitting
  a quantized random vector.
  \begin{lemma}
    \label{lem:latquantvar}
    Let $\X$ be a random vector satisfying $\E[\|\X\|^2]
    = \sq < \infty$. Let $\Y = \QL(\X)$. Then $\E[\|\Y\|^2] \le \sq + 2R\sigma +
    R^2$, where $R$~is the covering radius of~$\Lambda$.
  \end{lemma}

  \begin{proof}
    The power of~$\Y$ is given by
    \begin{equation*}
      \E[\|\QL(\X)\|^2] = \int_{R^n} \|\QL(\x)\|^2 \fXv(\x) d\x
      = \sum_{\p \in \Lambda} \|\p\|^2 \int_{V(\p)} \fXv(\x) d\x.
    \end{equation*}
    By definition of the covering radius, $\|\p\| \le \|\x\| + R$ for all $\x
    \in V(\p)$. Thus,
    \begin{align*}
      \E[\|\QL(\X)\|^2] &\le \sum_{\p \in \Lambda} \int_{V(\p)} (\|\x\| + R)^2
      \fXv(\x) d\x \\
      &= \int_{R^n} (\|\x\| + R)^2 \fXv(\x) d\x.
    \end{align*}
    By assumption, $\int_{\R^n} \|\x\|^2 \fXv(\x) d\x = \sq$. Moreover, by the
    positivity of the variance, $\E[\xi] \le (\E[\xi^2])^{1/2}$, and so
    $\int_{\R^n} \|\x\| \fXv(\x) d\x \le \sigma$. Applying this to the above
    yields
    \begin{equation*}
      \E[\|\QL(\X)\|^2] \le \sq + 2R\sigma + R^2,
    \end{equation*}
    thus completing the proof.
  \end{proof}

  \begin{example}
    Let $X$ be a scalar zero-mean random variable of variance~$\sq$ and let
    $\Lambda = \Z / \beta$, for some $\beta > 0$. The covering radius of this
    lattice is $R = 1/\beta$, so $\E[\QL(X)^2] \le \sq + 2 \beta^{-1} \sigma +
    \beta^{-2}$.
  \end{example}

  \begin{lemma}
    \label{lem:voronoivarbound}
    Let $\X$~be a random vector whose support is limited to the Voronoi
    region~$V(\vz)$ of a lattice~$\Lambda$. Then $\E[\|\X\|^2] \le
    R\Vol(V(\vz))$, where $R$~is the covering
    radius of~$\Lambda$ and $\Vol(V(\vz))$ is the volume of a Voronoi region.
  \end{lemma}

  \begin{proof}
    \begin{align*}
      \E[\|\X\|^2] & = \int_{V(\vz)} \|x\|^2 \fXv(\x) d\x \\
      & \le R \int_{V(\vz)} \fXv(\x) d\x = R \Vol(V(\vz)).
    \end{align*}
  \end{proof}




  \section{Proof of Lemmata~\ref{lem:lowerbound1vec}
  and~\ref{lem:lowerbound2vec}}\label{app:lbvecproofs}

  The following auxiliary result and its corollary will be useful for the proofs
  to come. See \figref{voronoiintersect} for an illustration.

  \begin{lemma}
    \label{lem:subvoronoi}
    Let $\Lambda$ be an arbitrary $m$-dimensional lattice and let $\Lambda' =
    \Lambda/\beta$, where $\beta \in \NN$. (Clearly $\Lambda$ is a sublattice of
    $\Lambda'$.) Then the fraction of Voronoi cells of $\Lambda$ that do not lie
    on the boundary between two Voronoi cells of $\Lambda'$ is bounded away from
    zero as $\beta$~grows large.
  \end{lemma}

  \begin{proof}
    Let $V(\Lambda)$ be a Voronoi region of $\Lambda$ and let $V(\Lambda')$ be a
    Voronoi region of~$\Lambda'$.  Consider a sphere of radius $\rho - 2R'$
    around the center of $V(\Lambda)$, where $\rho$ is the
    packing radius of $\Lambda$ and $R' = R/\beta$ is
    the covering radius of~$\Lambda'$. By definition of the packing radius, this
    sphere is completely contained within $V(\Lambda)$. Furthermore, the
    distance from the border of the sphere to the border of $V(\Lambda)$ is
    at least~$2R'$. Any Voronoi cell of $\Lambda'$ that intersects
    with the boundary of $V(\Lambda)$ lies therefore outside the sphere. 
    
    The fraction of Voronoi cells of $\Lambda'$ that do not lie on the boundary
    of $V(\Lambda)$ is therefore lower bounded by
    \begin{align*}
      \frac{(\rho - 2R')^m V^{(m)}(1)}{\Vol V(\Lambda)} &= \frac{(\rho -
      2R/\beta)^n V^{(m)}(1)}{\Vol V(\Lambda)},
    \end{align*}
    where $V^{(m)}(1)$ is the volume of the $m$-dimensional unit sphere. As beta
    grows large, this converges to $\rho V^{(n)}(1) / \Vol V(\Lambda) > 0$, and
    the proof is complete.
  \end{proof}

  \begin{corollary}
    Let $\Lambda$ be a fixed lattice and consider the sequence of lattices
    $\Lambda/\beta$, $\Lambda/\beta^2$, \dots, $\Lambda/\beta^{n-1}$, where
    $\beta \in \NN$. Then the fraction of Voronoi regions of
    $\Lambda/\beta^{n-1}$ that do not lie on the boundary between two Voronoi
    regions of any of its sublattices $\Lambda/\beta^i$, $i < n-1$, is bounded
    away from zero as $\beta$ grows large.
  \end{corollary}

  \begin{proof}
    Apply \lemref{subvoronoi} successively to the pairs $\Lambda/\beta^i$,
    $\Lambda/\beta^{i+1}$, for $i = 1$, \dots, $n-2$.
  \end{proof}

  \begin{figure}[tbph]
    \centerline{
    \subfloat[$\beta = 3$]{\input{figures/hierhex3.tex_t}}
    \hfil
    \subfloat[$\beta = 4$]{\input{figures/hierhex4.tex_t}}
    }
    \caption{Illustration for \lemref{subvoronoi} and the corollary thereof.
    Note how in the right picture the fraction of small Voronoi cells that are
    not ``cut'' by a Voronoi boundary of the sublattice is larger.}
    \label{fig:voronoiintersect}
  \end{figure}

  \begin{proof}[Proof of \lemref{lowerbound1vec}]
    Consider the lattice $\Ln \deq \Lambda/\beta^{n-1}$. For $\p \in
    \Ln$, let $V(\p)$ be the Voronoi region of~$\p$.
    For some $\vxi \in V(\vz)$ define $\Dv = \Delta \vxi$ for some $\Delta \le
    1$ and define $\Vd(\p) = V(\p) \cap (V(\p) - \Dv)$. This set has the
    property that $\x + \Dv \in V(\p)$ whenever $\x \in \Vd(\p)$.\footnote{For
    reference, the sets $\Vd(\p)$ are the equivalent of the intervals
    $\I_j^{\Delta}$ in the proof of \lemref{lowerbound1}.}

    If $\p$ is such that $V(\p)$ does not lie on the boundary of the Voronoi
    region of a sublattice, then $\Vd(\p)$ defined this way has the property
    that if $\s \in \Vd(\p)$,
    \begin{align*}
      d(\s,\Dv) &= \frac{\sqrt{mP}}{\gamma} \| \Evc_{n-1}(\s) - \Evc_{n-1}(\s +
      \Dv) \| \\
      &= \frac{\sqrt{mP}}{\gamma} \| \vxi \| \beta^{n-1} \Delta.
    \end{align*}
    We can now apply \lemref{zivboundvec} and restrict the integral to the set
    \[ \Psi(\Dv) \deq \Xi \cap (\Xi - \Dv) \cap \bigcup_{\p \in \Ln'} \Vd(\p),
    \]
    where $\Ln'$ is defined to be the subset of those points in $\Ln$ whose
    Voronoi region does not lie on the boundary between Voronoi regions of a
    sublattice. By the corollary to \lemref{subvoronoi}, this set has positive
    probability.

    The lower bound of \lemref{zivboundvec} is then relaxed to give
    \begin{equation*}
      \msev \ge c_1 \Delta^2 Q(c_2 \sqrt{\snr} \beta^{n-1} \Delta) 
      \int_{\Psi(\Dv)} d\s
    \end{equation*}
    for some positive constants~$c_1$ and~$c_2$. Letting $\Delta =
    1/(\sqrt{\snr} \beta^{n-1})$ and $\beta^2 = \snr^{1-\e}$ yields
    \begin{equation*}
      \msev \ge c_3 \snr^{-n+(n-1)\e} \int_{\Psi(\Dv)} d\s,
    \end{equation*}
    with $c_3 > 0$ a constant independent of~$\snr$.

    It remains to prove the convergence of $\int_{\Psi(\Dv)} d\s$ to a constant.
    Since $\Delta$ goes to zero as $\snr \goesto \infty$, the set $\Psi(\Dv)$
    converges to the set $\Xi \cap \bigcup_{\p \in \Ln'} V(\p)$. Since both
    $\Xi$ and~$\bigcup_{\p \in \Ln'} V(\p)$ have positive volume, the overall
    set also has positive volume.
  \end{proof}


  \begin{proof}[Proof of \lemref{lowerbound2vec}]
    Let $\Lambda$~be the normalized lattice used for quantization. Let $\vxi \in
    \Lambda$ be arbitrary but fixed. Let $\Dv = \beta^{-1} \vxi$. Using the same
    reasoning as in the proof of \lemref{lowerbound2}, it follows that $\Q_1(\s
    + \Dv) = \Q_1(\s) + \Dv$, and also $\Q_i(\s + \Dv) = \Q_i(\s)$ for $i = 2$,
    \ldots, $n-1$, and $\Evc_{n-1}(\s+\Dv) = \Evc_{n-1}(\s)$. Consequently,
    \begin{equation*}
      \| \X(\s) - \X(\s +\Dv) \| = \frac{\sqrt{mP}}{\gamma}\|\vxi\| \beta^{-1}.
    \end{equation*}
    According to \lemref{zivboundvec}, therefore,
    \begin{equation*}
      \msev \ge c_1 \beta^{-2} Q(c_2 \sqrt{\snr} \beta^{-1}) \int_{\Xi \cap (\Xi
      - \Dv)} d\s,
    \end{equation*}
    where $c_1$ and~$c_2$ are positive constants independent of~$\snr$.  The
    rest of the proof is essentially identical to that of
    \lemref{lowerbound2}. 
  \end{proof}

\end{subappendices}
