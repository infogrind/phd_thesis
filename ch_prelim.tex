\chapter{Preliminaries}
\label{ch:prelim}

\begin{notebox}
  I have used boxes like these to add notes in the text wherever I felt
  something was missing or not right, but did not find a proper way to write it
  or was lacking the necessary results.
\end{notebox}
The problem treated in this chapter and the next is the transmission of
discrete-time memoryless sources across discrete-time memoryless channels, where
the channel is used $n$~times for each $k$~source symbols.  The goal of the
present chapter is to establish the playing field by providing all necessary
definitions and by deriving the known theoretical limits.

The most fundamental boundary to the problem at hand comes from Shannon's
separation theorem\footnote{What is the best reference for this?}. It states
that the average cost and distortion $P$ and~$D$ of any point-to-point
communication system are related by
\begin{equation}
  \label{eq:shannonlimit}
  k R(D) \le n C(P),
\end{equation}
where $R(D)$ and $C(P)$ are the rate distortion function and the capacity of the
source and the channel, respectively. This puts (depending on the perspective) a
lower bound on the incurred distortion for a fixed cost or a lower bound on the
cost needed to achieve a given distortion.

Shannon also showed that there exists a sequence of source and channel codes of
increasing blocklength that, when put together, can come arbitrarily close to
equality in~\eqref{eq:shannonlimit}, establishing the operational significance
of the bound. In many practical situations, however, the delays implied by large
blocklengths are punitive; imagine for example the case of a telephone or video
call.

%Under a limited blocklength, the performance of a code is clearly at most as
%good as when the blocklength is unrestricted, and in many cases it is worse.
%The shortcoming of the separation theorem is that it does not take into account
%blocklength limitations. 
%
This and the next chapter focus on codes with the smallest possible delays. Such
minimal delay codes encode $k$~source symbols into $n$~channel inputs, with
$\gcd(k,n) = 1$.  The first question to ask is: are there any communication
systems employing minimal delay codes that satisfy~\eqref{eq:shannonlimit} with
equality? It turns out that when $k=n$, there is in fact an infinity of such
communication systems, as Gastpar's excellent treatment of the problem
showed~\cite{GastparRV2003}. If $k \ne n$,
\cite{GastparRV2003,GastparThesis}~give necessary and sufficient conditions for
a communication system to be optimal but do not in fact make any statement about
the \emph{existence} of such an optimal system; this question is still open. 


This part of the thesis does not try to find optimal communication systems in
the sense of equality in~\eqref{eq:shannonlimit}. Instead, with the focus
limited on the additive white Gaussian noise channel, the goal is to find
communication strategies that perform well in the loose sense that the
achievable mean squared error decays fast with increasing SNR. Nevertheless, the
conditions for optimality in the strict sense may give helpful indications
towards this goal. Hence, after \secref{defs} introduces the necessary
definitions and \secref{limits} formally states the problem,
Sections~\ref{sec:optmindel} and~\ref{sec:optmindelfb} review the optimality
conditions from~\cite{GastparThesis} for channels without and with feedback,
respectively, in the hope of getting insights that will help to solve the
problem of interest. 


\section{Basic Definitions and Notation}\label{sec:defs}

At the heart of the problem are the fundamental concepts of source and channel,
modeling the information to be communicated and the noisy medium over which it
is to be transmitted. 

\begin{definition}[Source]
  \label{def:analogsource}
  A discrete-time memoryless source is specified by a probability distribution
  $\pS(s)$ on~$\R$. An \emph{analog} source is a source that admits a
  probability density function \emph{(\pdf)}.
\end{definition}

\begin{definition}[Channel]
  \label{def:dtmlc}
  A discrete-time memoryless channel is specified by a conditional probability
  distribution $\pyx(y|x)$, assigning for each channel input~$x$ an output
  distribution $p_{Y|X=x}$. 
\end{definition}

Of particular interest is the Gaussian noise channel. 
\begin{definition}
  \label{def:awgn}
  An additive white Gaussian noise channel (AWGN channel) with noise
  variance~$\sq$ is a discrete-time memoryless channel whose output, conditioned
  on the input $X=x$, is a Gaussian random variable $Y$ with mean~$x$ and
  variance~$\sq$. An AWGN channel is sometimes also specified by relating the
  output $Y$ to the input $X$ through $Y = X + Z$, where $Z$ is zero-mean
  Gaussian with variance~$\sq$. 
\end{definition}

The next few definitions concern joint source/channel codes. 
\begin{definition}[Joint source/channel code]
  \label{def:knsccode}
  A $(k,n)$~\emph{joint source/channel code} $(f,g)$ consists of an
  \emph{encoder} function $f : \R^k \ra \R^n$ and a \emph{decoder}
  function $g:\R^n \ra \R^k$. The \emph{rate} of a $(k,n)$~joint
  source/channel code is $\k = k/n$. If $\k < 1$, the code is called
  a \emph{bandwidth expansion} code; if $k > 1$, the code is called a
  \emph{bandwidth compression} code. If $\k = 1$, the code is called a
  \emph{bandwidth matched} code.
\end{definition}

The following definition of the blocklength differs from the conventional use of
the term in source coding and channel coding. Nevertheless, it has the
intuitively appealing property that the ``shortest'' joint source/channel code
for a given system, \ie, the one with the smallest possible delay, has
blocklength one. 
\begin{definition}
  \label{def:blocklength}
  The blocklength of a $(k,n)$ joint source/channel code is defined
  as~$\gcd(k,n)$.
\end{definition}

\begin{definition}[Minimal-delay codes]
  \label{def:mindelcode}
  A $(k,n)$~\emph{minimal-delay} joint source\slash channel code is a
  $(k,n)$~joint source/channel code of blocklength~$1$. A minimal-delay code of
  rate~$1$ is also called a \emph{single letter} code.
\end{definition}

\begin{definition}
  \label{def:jointsccommsys}
  A $(k,n)$~\emph{source/channel communication system} consists of
  a discrete-time memoryless source~$\pS$, a discrete memoryless channel~$\pyx$,
  and a $(k,n)$~joint source/channel code~$(f,g)$ of rate~$\k = k/n$. The
  encoder maps $k$~source symbols $S^k$ into $n$~channel input symbols~$X^n =
  f(S^k)$, and the decoder maps $n$~channel output symbols $Y^n$ into $k$~source
  estimates $\Sh^k = g(Y^n)$.
\end{definition}

\figref{gensccommsys} shows the components of a source/channel communication
system and their connections.
\begin{figure}[tbp]
  \begin{center}
    \input{figures/sc_gen.tex_t}
  \end{center}
  \caption{A general source/channel communication system of rate $\k = k/n$
  according to \defref{jointsccommsys}.}
  \label{fig:gensccommsys}
\end{figure}

The performance of a general source/channel communication system can be measured
by the tradeoff between average cost and distortion, according to the following
definition.
\begin{definition}
  \label{def:PDgen}
  For a given \emph{channel input cost measure}~$\rho(x)$ and a given
  \emph{distortion measure}~$d(s,\sh)$, the \emph{expected cost}~$P$ and
  \emph{expected distortion}~$D$ incurred by a $(k,n)$ source/channel
  communication system are given by
  \begin{equation*}
    P = \frac1n \sn \E[\rho(X_i)] \quad \text{and} \quad
    D = \frac1k \sk \E[d(S_i - \Sh_i)],
  \end{equation*}
  respectively.
\end{definition}

Using the concepts of \sdr\ and \snr, the performance tradeoff can be succinctly
expressed in a single formula that is valid independent of the source variance
and the channel noise variance. 
\begin{definition}
  \label{def:sdrsnr}
  Consider a source/channel communication system with source
  variance~$\ssq$ and whose channel is Gaussian with noise variance~$\szq$. If
  the cost measure is the input power, \ie, $\rho(x) = x^2$, and the distortion
  measure is the squared error, \ie, $d(s,\sh) = (s - \sh)^2$, then the
  \emph{source-to-distortion ratio} \sdr\ and the \emph{signal-to-noise ratio}
  \snr\ are defined respectively as
  \begin{equation*}
    \sdr = \frac{\ssq}{D} \quad \text{and} \quad
    \snr = \frac{P}{\szq}.
  \end{equation*}
\end{definition}

Often a whole family of codes for Gaussian channels is defined implicitly by
making the encoder and decoder depend on the~\snr. To accommodate this,
\defref{knsccode} needs to be slightly extended as follows.
\begin{definition}
  \label{def:codingscheme}
  A \emph{coding scheme} for the Gaussian channel is a code whose encoder and
  decoder not only depend on the source and the channel output, respectively,
  but also on the value of~\snr. The notation used is usually just $f(s^k)$ and
  $g(y^n)$ instead of the technically correct $f(s^k,\snr)$ and $g(y^n,\snr)$.
\end{definition}


\section{Theoretical Limits and Problem Statement\looseness=-1}\label{sec:limits}

The most general bound on the performance of a joint source/channel coding
system is Shannon's separation theorem, which was already stated at the
beginning of this chapter.  For completeness, it is given here again formally.

\begin{theorem}
  \label{thm:sepconverse}
  When a memoryless source with rate distortion function~$R(D)$ is transmitted
  across a memoryless channel with capacity~$C(P)$ using a code of rate~$\k$,
  the expected distortion~$D$ and the expected cost~$P$ are related by
  \begin{equation}
    \label{eq:sepconverse}
    \k R(D) \le C(P).
  \end{equation}
\end{theorem}

\begin{proof}
  For a proof, see~\cite[Theorem~9.6.1]{Gallager1968}.
\end{proof}

\begin{definition}
  \label{def:optimalcode}
  A source/channel communication system that satisfies the bound of
  \thmref{sepconverse} with equality is said to be \emph{optimal}, or
  \emph{strictly optimal} where necessary to distinguish this definition from
  weaker forms of optimality.
\end{definition}

For Gaussian channels, \thmref{sepconverse} leads to the following upper bound
on the~\sdr.
\begin{theorem}
  \label{thm:sdrub}
  If a discrete-time analog source $\pS$ of zero mean and variance~$\ssq$ is
  transmitted across an AWGN channel with noise variance~$\szq$ using a code of
  rate~$\k$, the \sdr\ is bounded by
  \begin{equation}
    \label{eq:sdrub}
    \sdr \le 2^{2 D(\pS \| \phi_{\ssq})} (1 + \snr)^{1/\k},
  \end{equation}
  where $D(\cdot \| \cdot)$ is the relative entropy or Kullback-Leibler distance
  between two distributions (see \eg~\cite{CoverT1991}) and $\phi_{\ssq}$ is a
  centered Gaussian distribution of variance~$\ssq$.
\end{theorem}

\begin{remark}
  \label{rem:perflimitgaussiansource}
  For a Gaussian source the bound of \thmref{sdrub} simplifies to $\sdr \le (1 +
  \snr)^{1/\k}$ and is simply \thmref{sepconverse} applied to a Gaussian
  source and channel. This bound is therefore tight, in the sense that there
  exists a sequence of communication systems of rate~$\k$ (with increasing
  blocklength) that can come arbitrarily close to equality. 
  For other sources the bound is generally not tight, but it becomes tight as
  $\snr \ra \infty$~\cite{LinderZ1994}.
\end{remark}

\begin{proof}[Proof of \thmref{sdrub}]
  The rate distortion function of an analog source of finite entropy~$h(S)$
  with squared error distortion satisfies~\cite{Shannon1959}
  \begin{equation*}
    R(D) \ge h(S) - \frac12 \log_2(2\pi e D).
  \end{equation*}
  Inserting the capacity of the Gaussian channel into~\eqref{eq:sepconverse}
  results in
  \begin{equation*}
    \k R(D) \le \frac12 \log_2(1 + \snr)
  \end{equation*}
  and combining the two above inequalities yields
  \begin{equation*}
    \frac1D \le 2^{-2h(S)} 2\pi e (1 + \snr)^{1/\k}.
  \end{equation*}
  Multiplying both sides with~$\ssq$ and noting that $0.5 \log_2(2\pi e \ssq) =
  h(\phi_{\ssq})$ leads to
  \begin{equation*}
    \sdr \le 2^{2(h(\phi_{\ssq}) - h(S))} (1 + \snr)^{1/\k}.
  \end{equation*}
  Applying the property $h(\phi_{\ssq}) - h(S) = D(\pS \|
  \phi_{\ssq})$~\cite[Theorem~8.6.5]{CoverT1991} completes the proof.
\end{proof}


\subsection{Formal Problem Statement}

The problem considered in the first part of this thesis is to develop and
analyze coding schemes for the Gaussian channel that have a good asymptotic
performance. According to \thmref{sdrub}, the \sdr\ of any coding scheme scales
at most as $\snr^{1/\k}$. Expressed in the asymptotic notation explained in
\appref{asymptotic}, this is written as
\begin{equation*}
  \sdr \in O(\snr^{1/\k}).
\end{equation*}
In view of this limit, the following looser definition of optimality can be
formulated. 
\begin{definition}
  A coding scheme for the Gaussian channel is called \emph{asymptotically
  optimal} if it satisfies $\sdr \in \Omega(\snr^{1/\k})$.
\end{definition}
\secref{optmindelfb} will show that if the channel has noiseless feedback then a
simple asymptotically optimal coding scheme exists. In the absence of feedback,
however, the existence of an asymptotically optimal coding scheme is as yet
unknown.

The next two sections take again a step back and look at the conditions for
strict optimality in communication systems made up of general sources and
channels.


\section{Optimal Minimal-Delay Codes}\label{sec:optmindel}

\subsection{Bandwith Matched Codes}

By definition, minimal-delay bandwidth matched codes are single letter codes. A
particular example of an optimal single letter code is that of a Gaussian source
connected to a Gaussian channel, as given in the following example.
\begin{example}
  \label{ex:gausssingle}
  Let the source be zero-mean Gaussian with variance~$\ssq$ and let the channel
  be AWGN with noise variance~$\szq$. The encoder is given by $X = f(S) =
  \sqrt{P/\ssq} S$ and the decoder is given by $\Sh = g(Y) = \sqrt{P} \ssq Y /
  (P + \szq)$. Using $Y = X + Z$ it is quickly verified that 
  \begin{equation*}
    D = \E[(S - \Sh)^2] = \ssq / (1 + P/\szq),
  \end{equation*}
  which is indeed the optimal distortion according to \thmref{sdrub}.
\end{example}

\exref{gausssingle} is a particular instance of a more general paradigm called
\emph{measure matching}. Measure matching provides conditions under which a code
of finite blocklength achieves the bound of \thmref{sepconverse} with equality.
For an excellent treatment of measure matching the reader is referred to
Gastpar~\cite{GastparRV2003,GastparThesis}; in the sequel only the results
relevant to this thesis are quoted.

The first relevant result from~\cite{GastparRV2003} says that \emph{any}
bandwidth matched code is optimal for \emph{some} way of measuring the cost and
distortion.

\begin{theorem}
  \label{thm:tcntcbwmatch}
  A a source/channel communication system of rate one is optimal if and only if
  the following conditions are met.
  \begin{enumerate}[(i)]
    \item The cost measure~$\rho(x)$ satisfies
      \begin{equation}
        \label{eq:optcost}
        \rho(x)
        \begin{cases}
          = c_1 D(p_{Y|X=x} \| p_Y) + \beta & \text{if $p_X(x) > 0$} \\
          \ge c_1 D(p_{Y|X=x} \| p_Y) + \beta & \text{if $p_X(x) = 0$},
        \end{cases}
      \end{equation}
      where $c_1 > 0$ and $\beta$ are arbitrary real constants.

    \item The distortion measure~$d(s,\sh)$ satisfies
      \begin{equation}
        \label{eq:optdist}
        d(s,\sh) = - c_2 \log_2 \frac{p(\sh|s)}{p(\sh)} + d_0(s),
      \end{equation}
      where $c_2 > 0$ and $d_0(\cdot)$ is an arbitrary function.

    \item The code is information lossless in the sense that~$I(S;\Sh) =
      I(X;Y)$.
  \end{enumerate}
\end{theorem}

\begin{remark}
  \label{rem:tcntcrug}
  The statement of \thmref{tcntcbwmatch} ignores a few special cases,
  notably the case $I(S;\Sh) = 0$ and the case $I(X;Y) = \max_P C(P)$; these are
  treated in detail in~\cite{GastparRV2003} and are not relevant here.
\end{remark}

\begin{proof}[Proof of \thmref{tcntcbwmatch}]
  The proof hinges on the inequality chain
  \begin{equation*}
    R(D) \le I(S;\Sh) \le I(X;Y) \le C(P).
  \end{equation*}
  The complete proof, found in~\cite{GastparRV2003}, shows that $I(X;Y) = C(P)$
  if and only if condition~(i) is satisfied and that $R(D) = I(S;\Sh)$ if and
  only if condition~(ii) is satisfied. It is then clear that condition~(iii) is
  the third condition needed for $R(D) = C(P)$.
\end{proof}

The single letter code of \exref{gausssingle} is optimal precisely because
$\rho(x) = x^2$ satisfies condition~(i) and $d(s,\sh) = (s - \sh)^2$ satisfies
condition~(ii) for the communication system at hand. Moreover, since the encoder
and decoder are both one-to-one maps, condition~(iii) is trivially satisfied.

According to \thmref{tcntcbwmatch} there exists an infinity of optimal
single letter codes. For systems with bandwith expansion or compression,
however, optimal minimal delay codes may not exist, as the following section
argues.


\subsection{Bandwidth Expansion/Compression Codes}

If $k \ne n$, it is not known whether optimal minimal delay codes exist.  For
the particular case of a Gaussian source and a Gaussian channel with input power
constraint and squared error distortion, it has in fact been proven that no
optimal $(k,1)$ and $(1,n)$~codes exist~\cite{IngberLZF2008}.

For codes that are not bandwidth matched, optimality is no longer only a matter
of measure matching in the sense of \thmref{tcntcbwmatch}; additional conditions
need to be fulfilled. The following theorem lists these conditions. It is
essentially Theorem~3.9 from~\cite{GastparThesis}, adapted to the case without
feedback and with a slightly expanded definition of the ``information lossless''
property of the code. 

\begin{theorem}
  \label{thm:tcntc1n}
  A $(k, n)$~source/channel communication system is optimal if and only if the
  following conditions are met.
  \begin{enumerate}[(i)]
    \item
      \begin{enumerate}[(a)]
        \item The conditional distribution of the source symbols given the
          estimates can be factored as $p(s^k|\sh^k) = \pk p(s_i|\sh_i)$, and
        \item each $p(s_i|\sh_i)$ achieves the rate distortion function at the
          same average distortion.
      \end{enumerate}

    \item The estimates $\Sh^k$ form a sufficient statistic for $S^k$ given the
      outputs~$Y^n$.

    \item The encoder is information lossless, \ie,  $I(S^k; Y^n) =
      I(X^n; Y^n)$. 

    \item The channel outputs $Y_1$, \ldots, $Y_n$ are mutually independent.

    \item The marginal distributions $p(x_i)$ of the channel inputs all achieve
      the capacity at the same average cost.
  \end{enumerate}
\end{theorem}

\begin{discussion}
  If $k = 1$, condition~(i)(a) of the theorem is trivially satisfied. If $n =
  1$, condition~(iv) is trivially satisfied. Furthermore, if condition~(i)(a) is
  satisfied then condition~(i)(b) is only a matter of matching the distortion
  measure. Similarly, if condition~(iv) is satisfied then condition~(v) is only
  a matter of matching the cost measure. 
\end{discussion}

\begin{proof}
  The proof is based on the following chain of inequalities.
  \begin{align}
    \label{eq:14step1}
    k R(D) &\le I(S^k; \Sh^k) \\
    \label{eq:14step2}
    &\le I(S^k; Y^n) \\
    \label{eq:14step3}
    &\le I(X^n; Y^n) \\
    \label{eq:14step4}
    &\le \sn I(X_i; Y_i)  \\
    \label{eq:14step5}
    &\le n C(P).
  \end{align}
  Inequality \eqref{eq:14step1} follows from the definition of~$R(D)$; it
  becomes an equality if and only if condition~(i) is satisfied.
  \eqref{eq:14step2} is the data processing inequality, it becomes an inequality
  if and only if condition~(ii) is satisfied. Next, \eqref{eq:14step3} is again
  the data processing inequality and it is satisfied with equality if and only
  if condition~(iii) is satisfied.  Inequality~\eqref{eq:14step4} follows
  because the channel is memoryless and because conditioning can only decrease
  the entropy. It becomes an inequality if and only if condition~(iv) is
  satisfied. Inequality~\eqref{eq:14step5}, finally, follows from the definition
  of~$C(P)$ and becomes an equality if and only if condition~(v) is satisfied.
\end{proof}

% [Marius] Disabled the following lemma, since it might not be true. To be
% resolved. 

%The following lemma gives a condition equivalent to conditions~(i)(a) and~(ii).
%\begin{lemma}
%  \label{lem:ssil}
%  Adapt here the lemma from the feedback case to the feedbackfree case.
%\end{lemma}
%\begin{proof}
%  \todo.
%\end{proof}
%
\begin{remark}
  \label{rem:inflosslessenc}
  A deterministic encoder is sufficient but not necessary for condition~(iii).
  Nevertheless, any nondeterministic encoder satisfying condition~(iii) can be
  turned into a deterministic encoder that also satisfies the condition as the
  following argument shows. 

  Assume $I(S^k; Y^n) = I(X^n;Y^n)$. For $s^k$, $x^n$ such that $p(s^k, x^n) >
  0$, we can then factor the joint distribution of $s^k$, $x^n$, $y^n$ as
  \begin{equation*}
    p(s^k, x^n, y^n) = p(y^n | s^k) p(x^n|s^k) p(s^k),
  \end{equation*}
  where $p(x^n|s^k)$ specifies the nondeterministic encoder. Suppose we replace
  $p(x^n|s^k)$ with another distribution $\pt(x^n|s^k)$ with the property that
  $\pt(x^n|s^k) = 0$ whenever $p(x^n|s^k) = 0$. Then the joint distribution
  becomes
  \begin{equation*}
    \pt(s^k,x^n,y^n) = p(y^n|s^k) \pt(x^n|s^k) p(s^k).
  \end{equation*}
  Consequently, $\pt(x^n,y^n|s^k) = p(y^n|s^k)\pt(x^n|s^k)$, so
  $\It(X^n;Y^n|S^k) = 0$. Furthermore,
  \begin{equation*}
    \pt(s^k,y^n) = \sum_{x^n} \pt(s^k, x^n, y^n) = p(y^n|s^k) p(s^k),
  \end{equation*}
  and so $\It(S^k;Y^n) = I(S^k; Y^n)$. In particular, we can choose for
  $\pt(x^n|s^k)$ a distribution that has all its mass in a single $x^n$,
  which is nothing but a deterministic encoder. 
\end{remark}

The consequence of \thmref{tcntc1n}, and the discussion following it, is this:
when $k \ne n$, finding an optimal communication system is no longer only a
matter of matching cost and distortion measures. More precisely, only
conditions~(i)(b) and~(v) can be fulfilled by choosing a matching cost and
distortion measure, and only provided that the remaining conditions are
satisfied. These remaining conditions are absolute: they depend only on the
statistical properties of the relevant quantities. These findings are summarized
in the following lemma.

\begin{lemma}
  \label{lem:optcodenkexist}
  An optimal minimal delay $(k,n)$~source/channel communication system exists if
  and only if there exists an $(k,n)$~communication system satisfying
  conditions~(i)(a) and (ii)--(iv) of \thmref{tcntc1n}.
\end{lemma}

In order for a Gaussian communication system to be asymptotically optimal, the
channel output sequence need neither be independent nor have the capacity
achieving distribution, as the following result shows. 

\begin{lemma}
  \label{lem:asymptoptinputs}
  Consider a sequence of zero-mean input distributions $\{p(x^n)\}$ on the
  Gaussian channel with covariance matrix sequence~$\{K\}$ satisfying
  \begin{equation*}
    \frac{\Tr K}{n \szq} = \snr
  \end{equation*}
  and 
  \begin{equation*}
    \limsup_{\snr \goesto \infty} D(p(y^n) \| \phi_{K + \szq I}) < \infty,
  \end{equation*}
  where $\phi_K$ is the distribution of a centered Gaussian random vector with
  covariance matrix~$K$. If the distributions $p(x^n)$ are nondegenerate then
  $2^{2 (X^n; Y^n)} \in \Omega(\snr^n)$. 
\end{lemma}

\begin{proof}
  Write the mutual information as $I(X^n; Y^n) = h(Y^n) - h(Z^n)$. Since $Y^n$
  has covariance matrix $K + \szq I$, its entropy can be written
  as~\cite[Theorem~8.6.5]{CoverT1991}
  \begin{equation*}
    h(Y^n) = h(\phi_{K + \szq I}) - D(p(y^n) \| \phi_{K + \szq I}).
  \end{equation*}
  Next, 
  \begin{align*}
    h(\phi_{K + \szq I}) - h(Z^n) &= \frac12 \log_2 \left(
    \frac{\det(K + \szq I)}{(\szq)^n} \right) \\
    &= \frac12 \log_2 \left( \frac{\pn (\lambda_i + \szq)}{(\szq)^n} \right),
  \end{align*}
  where the~$\lambda_i$ are the eigenvalues of~$K$. Write now $K = P \Kt$, where
  $\Tr \Kt = n$. Then $\lambda_i = P \lt_i$, where the $\lt_i$ are the
  eigenvalues of~$\Kt$. With this and the above, we have
  \begin{equation*}
    I(X^n; Y^n) = \frac12 \log_2 \left(
    \frac{ \pn(P \lt_i + \szq)}{(\szq)^n} \right) - D(p(y^n) \| \phi_{K + \szq
    I}).
  \end{equation*}
  Using the assumption on the divergence, there exists a constant~$c$ such that
  \begin{equation*}
    2^{2 I(X^n; Y^n)} \ge c \frac{ \pn(P \lt_i + \szq)}{(\szq)^n}.
  \end{equation*}
  If the distributions~$p(x^n)$ are nondegenerate, all the $\lt_i$ are strictly
  positive and thus $2^{2 I(X^n; Y^n)} \in \Omega((P/\szq)^n) = \Omega(\snr^n)$.
\end{proof}

\begin{notebox}
  It would appear that the condition of an nondegenerate input distribution is
  not only sufficient but also necessary. However, since we are talking about
  \emph{sequences} of distributions here, I am not sure how to formulate the
  conditions exactly. 
\end{notebox}

\section{Optimal Minimal-Delay Codes with Feedback}\label{sec:optmindelfb}

While the problem studied here concerns channels without feedback, the case of
feedback channels is nevertheless of interest, since it provides the only known
instance of a strictly optimal $(1,n)$~communication system. 

For channels with feedback the definitions of a code and of a communication
system must be slightly adapted. 

\begin{definition}
  \label{def:fbcode}
  A $(k,n)$ \emph{joint source/channel feedback code} $(f,g)$ consists of
  $n$~\emph{encoding functions} $f_i : \R^k \times \R^{i-1} \ra \R$, $i = 1$,
  \dots, $n$, and a decoding function $g: \R^n \ra \R^k$. 
\end{definition}

\begin{definition}
  \label{def:fbcommsys}
  A $(k,n)$ \emph{source/channel communication system with feedback} is a
  $(k,n)$ communication system where the encoder has noiseless feedback from the
  channel output, and a $(k,n)$ feedback code computes the $i^{\text{th}}$
  channel input as $X_i = f_i(S^k, Y^{i-1})$ and the source estimate as $\Sh^k =
  g(Y^n)$. 
\end{definition}

\begin{figure}[tbp]
  \begin{center}
    \input{figures/sc_gen_feedback.tex_t}
  \end{center}
  \caption{A general source/channel communication system with feedback.}
  \label{fig:fbcommsys}
\end{figure}
\figref{fbcommsys} has a schematic depiction of a feedback communication system.

Since feedback does not increase the capacity of memoryless channels, the bound
of \thmref{sepconverse} also applies to systems with feedback. It is therefore
clear that an optimal single letter code for a system without feedback is also
optimal for a system with feedback. While there are no known optimal
communication systems without feedback when $k \ne n$, there is at least one
known optimal feedback system with $k = 1$ and $n > 1$, as the following example
shows.

\begin{example}
  \label{ex:gaussfb}
  In this example, a Gaussian source is transmitted across an AWGN channel, and
  the channel is used $n$~times per source symbol.  Define $E_0 = S$. In the
  $i^{\text{th}}$ channel use, the encoder produces
  \begin{equation}
    \label{eq:gaussfbxi}
    X_i = \sqrt{\frac{P}{\Var E_{i-1}}} E_{i-1}.
  \end{equation}
  Both the receiver and the sender now compute the minimum mean-squared
  error (MMSE) estimator $\Eh_{i-1}$ of $E_{i-1}$ given $Y_i$. The sender
  computes $E_i = \Eh_{i-1} - E_{i-1}$ and proceeds to the next round.

  After $n$~rounds of transmission, the receiver has $n$~estimates $\Eh_0$
  to~$\Eh_{n-1}$. Using these, it computes the final estimate~$\Sh$ as
  \begin{equation}
    \label{eq:shdecom1}
    \Sh = \Eh_0 - \Eh_1 + \Eh_2 - \cdots \pm \Eh_{n-1}.
  \end{equation}
  (The sign of the last term is $+$ if $n$~is even and $-$ if $n$~is odd.)

  To compute the overall distortion $\mse$, note that $\Eh_{i-1} = E_{i-1} +
  E_i$ by definition, so \eqref{eq:shdecom1}~can be written as
  \begin{align*}
    \Sh &= (E_0 + E_1) - (E_1 + E_2) + (E_2 + E_3) - \cdots \pm (E_{n-1} + E_n)
    \\
    &= E_0 \pm E_n,
  \end{align*}
  and since we have defined $E_0 = S$, we have $\mse = \E[E_n^2]$, where $E_n$
  is the remaining error after the last round of transmission.

  To compute $\E[E_n^2]$, note that since $\Eh_i$ is the MMSE estimator of
  $E_i$, the estimation error variance is given by (see
  \eg~\cite[Section~8.3]{Scharf1990})
  \begin{equation}
    \label{eq:gaussvardec}
    \E[(\Eh_i - E_i)^2] = \frac{\E[E_i^2]}{1 + P/\szq}.
  \end{equation}
  Using $\E[E_0^2] = \E[S^2] = \ssq$ and recursively applying the above, we find
  that
  \begin{equation*}
    \frac{\ssq}{\E[E_n^2]} = \frac{\ssq}{\E[(\Eh_{n-1} - E_{n-1})^2]} =
    (1 + P/\szq)^n,
  \end{equation*}
  which is indeed the largest possible \sdr\ according
  to~\remref{perflimitgaussiansource}.
\end{example}

To see why this example works, let us restate \thmref{tcntc1n} for the case with
feedback. 
\begin{theorem}
  \label{thm:tcntcfb}
  For $k \ne n$, a $(k, n)$~source/channel communication system with feedback is
  optimal if and only if the following conditions are met.
  \begin{enumerate}[(i)]
    \item
      \begin{enumerate}[(a)]
        \item The conditional distribution of the source symbols given the
          estimates can be factored as $p(s^k|\sh^k) = \pk p(s_i|\sh_i)$, and
        \item each $p(s_i|\sh_i)$ achieves the rate distortion function at the
          same average distortion.
      \end{enumerate}

    \item The estimates $\Sh^k$ form a sufficient statistic for $S^k$ given the
      outputs~$Y^n$.

    \item The encoder is information lossless in the sense that $I(S^k; Y^n) =
      I(X^n \ra Y^n)$. 

    \item The channel outputs $Y_1$, \ldots, $Y_n$ are mutually independent.

    \item The marginal distributions $p(x_i)$ of the channel inputs all achieve
      the capacity at the same average cost.
  \end{enumerate}
\end{theorem}

\begin{proof}
  The proof is essentially the same as that of \thmref{tcntc1n}, except that the
  inequality $I(X^n;Y^n) \le \sn I(X_i; Y_i)$ is replaced by $I(X^n \ra Y^n) \le
  \sn I(X_i; Y_i)$, where $I(X^n \ra Y^n)$ is the \emph{directed information}
  from~$X^n$ to~$Y^n$ (see~\cite{Massey1990,Kramer1998}). A necessary and
  sufficient condition for equality is condition~(iv) of the
  theorem~\cite[Theorem~2]{Massey1990}.
\end{proof}

Let us now revisit Example~\ref{ex:gaussfb} and see why it achieves the optimal
distortion. We will go through the conditions of \thmref{tcntcfb}
in reverse order, starting at condition~(v). Since the source and the noise are
jointly Gaussian and the encoder and decoder are linear, all channel inputs
$X_i$ are Gaussian; since they are scaled to have variance~$P$ they all achieve
the capacity at the same average cost. Next, because the estimation error of an
MMSE estimator is uncorrelated with the observation and because in the Gaussian
case uncorrelated implies independent, $E_i$ is independent of $Y_{i-1}$ and
thus so are~$X_i$ and~$Y_i$, satisfying condition~(iv). Condition~(iii) is
satisfied because the encoder is deterministic. For conditions~(i) and~(ii),
observe that the final estimate $\Sh$ is such that $S = \Sh + E_n$, where $E_n$
is independent of $\Sh$ and of $Y^n$. Given $\Sh$, $S$ is therefore independent
of~$Y^n$, which makes $\Sh$ a sufficient statistic, satisfying condition~(ii).
Moreover, the relationship between $S$ and $\Sh$ is exactly the one leading to
the distribution that achieves the rate distortion function (see
\eg~\cite[Theorem~10.3.2]{CoverT1991}), fulfilling condition~(i)(b).
Condition~(i)(a) is trivially satisfied since~$k=1$.

It appears as though this example works only because of the particular
properties of the Gaussian distribution: preservation of distribution under
linear transformation, linear MMSE decoder, equivalence of uncorrelatedness and
independence, and so on. From this example alone, one would therefore not
conclude that there exist optimal codes for other sources and channels. As the
next section shows, though, at least conditions~(iii)--(v) of \thmref{tcntcfb}
can be achieved with a minimal delay code for any source and channel if perfect
feedback is available.

\begin{remark}
  \label{rem:lmmse}
  Note that the \sdr\ achievable by the scheme of \exref{gaussfb} does not
  depend on the distribution of the source. Comparing the performance of this
  scheme with the bound of \thmref{sdrub}, it follows that the scheme can be
  used to construct an asymptotically optimal Gaussian communication system for
  arbitrary analog sources.
\end{remark}

\subsection{Posterior Matching}

Example~\ref{ex:gaussfb} showed how a simple transmission scheme can achieve the
optimal distortion using noiseless feedback. The discussion in the previous
section showed how the particular properties of the Gaussian distribution helped
in achieving this. In the sequel we show that conditions~(iii) to~(v) of
\thmref{tcntcfb} can be satisfied for arbitrary sources and channels. The
underlying idea, called \emph{posterior matching}, was used by Shayevitz and
Feder in~2007~\cite{ShayevitzF2007,ShayevitzF2008} to generalize the capacity
achieving channel coding schemes of Schalkwijk and
Kailath~\cite{SchalkwijkK1966} and Horstein~\cite{Horstein1963} to arbitrary
channels with feedback.

Before continuing we prove some properties of cumulative distribution functions
(\cdf s).

\begin{lemma}
  \label{lem:cdfunif}
  Let $X$ be a continuous random variable with density $f(x)$ and \cdf\ $F_X$,
  \ie,
  \begin{equation*}
    F_X(x) = \Pr[X \le x].
  \end{equation*}
  Then the random variable $F_X(X)$ is uniformly distributed on~$[0,1]$.
\end{lemma}

\begin{proof}
  Let $Y = F_X(X)$. Then
  \begin{align*}
    \Pr[Y \le y] &= \Pr[F_X(X) \le y] \\
    &= \Pr[X \le F_X^{-1}(y)] \\
    &= \int_{-\infty}^{F_X^{-1}(y)} f(x) dx \\
    &= F_X(F_X^{-1}(y)) = y,
  \end{align*}
  which is the \cdf\ of a uniform random variable on $[0,1]$.
\end{proof}


\begin{lemma}
  \label{lem:invcdf}
  Let $U$ be a uniform random variable on~$[0,1]$ and let $F$ be the \cdf\ of an
  arbitrary random variable~$X$. If $F$~is not invertible, define $F^{-1}$ with
  a slight abuse of notation as
  \begin{equation}
    \label{eq:invcdf}
    F^{-1}(y) = \sup \{x : F(x) \le y\}.
  \end{equation}
  Then the random variable $F^{-1}(U)$ has the same distribution as~$X$.
\end{lemma}

\begin{proof}
  The definition of $F^{-1}$ according to~\eqref{eq:invcdf} is such
  that $\Pr[F^{-1}(U) \le x] = \Pr[U \le F(x)]$. Thus,
  \begin{align*}
    \Pr[F^{-1}(U) \le x] &= \Pr[U \le F(x)] \\
    &= \int_0^{F(x)} d\xi = F(x).
  \end{align*}
\end{proof}

Consider now a channel~$\pyx$ and let $\pi(x)$ be the capacity achieving
distribution at average cost~$P$, \ie, 
\begin{equation*}
  \pi(x) = \arg\max_{p(x): \E[\rho(X)] \le P} I(X;Y).
\end{equation*}
The problem is to encode one source symbol of an \emph{analog} source into~$n$
channel inputs, making use of the feedback.

Let $\Fpi$ be the cumulative distribution function (\cdf) of the distribution
$\pi(x)$, and let $F_S$ be the \cdf\ of the source. In the first channel
use, the encoder produces
\begin{equation}
  \label{eq:posteriorx1}
  X_1 = \Fpi^{-1}(F_S(S)),
\end{equation}
where $\Fpi^{-1}$ is the inverse of $\Fpi$ according to~\eqref{eq:invcdf}. By
Lemma~\ref{lem:cdfunif}, $F_S(S)$ has uniform distribution on $[0,1]$, and so by
Lemma~\ref{lem:invcdf}, $\Fpi^{-1}(F_S(S))$ is a random variable with \cdf\
$\Fpi$.

After $i-1$ rounds of transmission, the encoder knows $y_1$, \ldots, $y_{i-1}$
and can compute the conditional
\cdf\ $F_{S|y_1, \ldots, y_{i-1}}$. It then sends
\begin{equation}
  \label{eq:posteriorxi}
  X_i = \Fpi^{-1}(F_{S|y_1,\dots,y_{i-1}}(S)).
\end{equation}
For any $y_1$, \ldots, $y_{i-1}$, therefore,
\begin{equation*}
  p(x_i|s, y_1, \dots, y_{i-1}) = \pi(x)
\end{equation*}
and so $X_i$ is independent of $Y_1$, \ldots, $Y_{i-1}$. 

Using this strategy the encoder produces an \iid\ sequence of inputs $X_i$
with the capacity achieving distribution~$\pi(x)$, satisfying conditions~(iv)
and~(v) of \thmref{tcntcfb}; condition~(iii) of the theorem is
trivially satisfied because the encoder is deterministic.

Let us now derive the posterior matching encoder for the communication system of
\exref{gaussfb}.

\begin{example}
  \label{ex:gaussfbpost}
  First, a few properties of Gaussian \cdf s are given. Let $F_{\N(\mu, \sq)}$
  be the \cdf\ of a Gaussian random variable of mean~$\mu$ and variance~$\sq$
  and let $F_\N \deq F_{\N(0,1)}$. Then $F_{\N(\mu,\sq)}(x) =
  F_\N((x-\mu)/\sigma)$. Furthermore, the inverse \cdf\ is 
  \begin{equation*}
    F_{\N(\mu,\sq)}^{-1}(y) = \sigma F_\N^{-1}(y) + \mu.
  \end{equation*}

  Let $\pi(x) = \N(0,P)$. According to~\eqref{eq:posteriorx1}, the first channel
  input is
  \begin{equation*}
    X_1 = \sqrt{P} F_{\N}^{-1}(F_{\N}(S/\sigma_S)) = \sqrt{\frac{P}{\ssq}} S,
  \end{equation*}
  which coincides with~\eqref{eq:gaussfbxi} in \exref{gaussfb} when~$i=1$.

  Given $Y_1$, $S$ is Gaussian with mean $\E[S|Y_1]$ and variance $\Var(S -
  \E[S|Y_1])$. Following~\eqref{eq:posteriorxi}, the second channel input is
  thus
  \begin{align*}
    X_2 &= \sqrt P F_{\N}^{-1} \left( F_{\N} \left( \frac{S - \E[S|Y_1]}
    {\sqrt{\Var(S-\E[S|Y_1])}} \right) \right) \\
    &= \sqrt{P} \frac{S - \E[S|Y_1]}{\sqrt{\Var(S-\E[S|Y_1])}}.
  \end{align*}
  Continuing this way, the $i^{\text{th}}$ channel input is found to be
  \begin{equation}
    \label{eq:gausspmenc}
    X_i = \sqrt{P} \frac{S - \E[S|Y_1^{i-1}]}{\sqrt{\Var(S-\E[S|Y_1^{i-1}])}}.
  \end{equation}
  That this is equal to~\eqref{eq:gaussfbxi} can be seen as follows. In
  \exref{gaussfb}, write
  \begin{align*}
    S &= E_0 + (E_1 - E_1) - (E_2 - E_2) + \dots \pm (E_{i-2} -
    E_{i-2}) \\
    &= (E_0 + E_1) - (E_1 + E_2) + (E_2 + E_3) - \dots - E_{i-2} \\
    &= \Eh_0 - \Eh_1 + \Eh_2 - \dots - E_{i-2}.
  \end{align*}
  Since $\E[\Eh_j | Y_1^{i-1}] = \Eh_j$ for $j = 1$, \dots,~$i-2$, and
  $\E[E_{i-2}|Y_1^{i-1}] = \Eh_{i-2}$, 
  \begin{equation*}
    \E[S|Y_1^{i-1}] = \Eh_0 - \Eh_1 + \dots  - \Eh_{i-2}
  \end{equation*}
  and so $S - \E[S|Y_1^{i-1}] = \Eh_{i-2} - E_{i-2} = E_{i-1}$. Plugging this
  into~\eqref{eq:gausspmenc} yields exactly the encoder~\eqref{eq:gaussfbxi} of
  Example~\ref{ex:gaussfb}.
\end{example}

\begin{remark}
  \label{rem:alwaysdistmatch}
  A question of interest is whether the encoder of \emph{any} optimal
  source/channel communication system can be expressed as a posterior matching
  encoder. The answer is in fact negative. Since by definition cumulative
  distribution functions are increasing, the posterior matching
  encoders~\eqref{eq:posteriorx1} and~\eqref{eq:posteriorxi} are always
  increasing functions. Yet, using measure matching it is straightforward to
  come up with an optimal $(1,1)$~communication system that has a nonincreasing
  encoder: just take the encoder to be an arbitrary function, and select the
  cost and distortion measures accordingly, following \thmref{tcntcbwmatch}.
\end{remark}


\subsection{Achieving $R(D)$ using Posterior Matching?}

The previous section showed how one can turn an arbitrary distribution into the
capacity achieving distribution. Can the same trick be used to make the
conditional distribution of~$\Sh$ given~$S$ achieve the rate distortion
function? 

For simplicity consider a $(1,1)$ communication system (whether it has feedback
or not is irrelevant). For a fixed~$D$, let
\begin{equation*}
  \Phi_s(\sh) = \arg\min_{p(\sh|s): \E[d(S,\Sh)] \le D} I(S;\Sh),
\end{equation*}
\ie, $\Phi_s(\sh)$ is the conditional distribution of~$\Sh$ given~$S$ that
achieves the rate distortion function at expected distortion~$D$. Let the
decoder be
\begin{equation}
  \label{eq:distmatchdec}
  g(y) = F_{\Phi_s}^{-1}(F_{Y|S=s}(y)).
\end{equation}
According to the previous section, the resulting joint distribution of $\Sh =
g(Y)$ and~$S$ satisfies thus $I(S;\Sh) = R(D)$. 

It is immediately clear, however, that this approach cannot work -- both \cdf s
needed to implement this decoder depend on the actual value of~$s$, which is
obviously not known at the decoder (there would not really be a communication
problem otherise). Interestingly, though, in the Gaussian case the dependence
on~$s$ of $F_{\Phi_s}$ and of $F_{Y|S=s}$ cancel each other out, and the
decoder~\eqref{eq:distmatchdec} yields again the MMSE decoder, as the following
example shows.

\begin{example}
  Let the source $S$ be distributed as $\N(0,1)$ and let the channel be AWGN
  with noise variance~$1$ and input constraint $\E[X^2] \le P$.
  The distortion is the squared error. The smallest achievable distortion is
  \begin{equation}
    \label{eq:exmindist}
    D_{\min} = \frac{1}{1 + P}.
  \end{equation}
  The capacity-achieving input distribution is $\N(0,P)$, and the conditional
  distribution of $\Sh$ given $S=s$ that achieves the rate distortion function
  at distortion~$D$ is $\N((1-D)s, D(1-D))$.
  Let $X = \sqrt{P}S$.  The decoder from~\eqref{eq:distmatchdec} is
  \begin{align*}
    g(y) &= F_{\Phi_s}^{-1} (F_{Y|S=s}(y)) \\
    &= \sqrt{D(1-D)} F_{\N}^{-1} \left( F_{\N}\left( y-\sqrt{P}s
    \right) \right) + (1-D)s \\
    &= \sqrt{D(1-D)} \left( y-\sqrt{P}s \right) + (1-D)s.
  \end{align*}
  This expression still depends on~$s$. If we plug in the optimal distortion
  $D_{\min}$ from~\eqref{eq:exmindist}, however, the decoder becomes
  \begin{align*}
    g(y) &= \frac{\sqrt{P}}{P+1} (y - \sqrt{P}s) + \frac{P}{P +
    1}s \\ 
    &= \frac{\sqrt{P}y}{P + 1},
  \end{align*}
  which no longer depends on~$s$. Furthermore, this decoder is the MMSE decoder.
\end{example}

\subsection{Statistically Sufficient Decoder}

\begin{notebox}
According to the Wikipedia article on \emph{Exponential
Families}\urldef{\wikiexp}\url{http://en.wikipedia.org/wiki/Exponential_family}\footnote{\wikiexp},
the \emph{Pitman-Koopman-Darmois theorem} says that ``only in exponential
families is there a sufficient statistic whose dimension remains bounded as
sample size increases.'' This seems to mean that unless the distribution
of~$Y^n$ given~$S$ is an exponential family (in which category the Gaussian
distribution falls), the estimate~$\Sh$ cannot be a sufficient statistic for~$n
> 1$. Unfortunately, the relevant papers are quite mathematical and it may take
some time to figure out the exact consequences for our problem. 
\end{notebox}


\subsection{Geometric Interpretation}

\begin{notebox}
  While the derivation in this section is nice and appealing, we don't actually
  use it to prove anything or get any particular insights. Furthermore, it seems
  to be a well known fact that random variables of finite variance form a vector
  space. Hence I am not sure if there is a good reason to keep this section,
  other than the fact that it takes up some space (and that I independently came
  up with it).
\end{notebox}

If the channel is an AWGN channel, the distortion measure is the squared error,
and the decoder computes the LMMSE estimator, there is a nice geometrical
interpretation for the feedback transmission problem. (The derivation of the
inner product space follows Cramér and
Leadbetter~\cite[Section~5.6]{CramerL1967}.)

\begin{definition}
  An \emph{inner product space} $H$ is a set of elements (points, vectors) $x$,
  $y$, \dots satisfying the following properties.
  \begin{enumerate}
    \item There is an operation of \emph{addition}, assigning to each two
      elements $x, y \in H$ a unique element $z \in H$, denoted $x + y$.
      The unique element $y$ satisfying $x + y = z$ is denoted $y = z - x$. The
      element $0 = x-x$ is unique. 
    \item For every $c \in \R$, the operation of \emph{scalar multiplication}
      maps each $x \in H$ to $cx \in H$. 
    \item To every two elements $x,y \in H$ corresponds a unique scalar $\sp x
      y$ called the \emph{inner product} of $x$ and $y$ with the properties that
      for all $c \in \R$ and $x,y, z\in H$, $\sp{cx+y}{z} = c\sp xz + \sp yz$,
      and $\sp xx \ge 0$, with equality if and only if $x = 0$. The
      \emph{norm} of an element $x \in H$ is defined as $\|x\| = \sqrt{\sp xx}$.
      If $\sp xy = 0$ for some $x$ and $y$, then $x$ and $y$ are called
      \emph{orthogonal}.
  \end{enumerate}
\end{definition}

The following easily verified lemma allows us to view the Gaussian feedback
communication problem in a geometric setting.
\begin{lemma}
  \label{lem:rvinprodsp}
  Any set of jointly Gaussian random variables with finite variance forms an
  inner product space under ordinary addition and with $\sp XY = \Cov(X,Y)$. 
\end{lemma}



% [Marius] Disabled the following lemma and proof for now, it doesn't seem to be
% quite solid.
%\begin{lemma}
%  \label{lem:inprodspcond}
%  Given a set of random variables in an inner product space~$H$ as defined
%  above, the space of the random variables conditioned on $X \in H$ is
%  equivalent to the projection of $H$ on the subspace orthogonal to~$X$.
%\end{lemma}
%\begin{proof}
%  Conditioned on~$X$, we have $\sp YZ = \E[YZ|X]$. The covariance between $Y$
%  and~$Z$ given~$X$ is
%  \begin{align*}
%    \Cov(Y, Z \mid X) &= \E[YZ|X] - \E[Y|X]\E[Z|X] \\
%    &= \E[YZ|X] - \frac{\E[XY|X]\E[XZ|X]}{\E[X^2|X]} \\
%    &= \sp YZ - \frac{\sp XY \sp XZ}{\|X\|^2} \\
%    &= \left\bra Y - \frac{\sp XY}{\|X\|^2} X, Z - \frac{\sp XZ}{\|X\|^2} X
%    \right\ket \\
%    &= \sp{Y_{\perp X}}{Z_{\perp X}},
%  \end{align*}
%  where the subscript $\perp X$ denotes the projection onto the subspace
%  orthogonal to~$X$.
%\end{proof}

From the point of view of this inner product space, the communication strategy
of Example~\ref{ex:gaussfb} is illustrated in Figure~\ref{fig:gaussfb1}.
\begin{figure}[tbp]
  \begin{center}
    \input{figures/gaussfb1.tex_t}
  \end{center}
  \caption{Geometrical interpretation of the feedback communication scheme of
  Example~\ref{ex:gaussfb}. The estimate $\Eh_0$ of the first transmission is
  the \emph{projection} of $S = E_0$ onto $Y_1$, hence the estimation error
  $E_1$ is orthogonal to~$Y_1$. Note in particular that $X_2$ is \emph{not}
  independent of $X_1$ (as would be a condition for optimality in the case
  without feedback).}
  \label{fig:gaussfb1}
\end{figure}
The figure shows that while $X_1$ and $X_2$ are not independent, $X_2$ is
independent of $Y_1$. Since all future rounds of communication involve only
linear combinations of~$E_1$ and the noise components $Z_2$, $Z_3$, \dots, all
future communication takes place in a subspace orthogonal to~$Y_1$. This
communication strategy thus produces a projection of $S$ onto the subspace
spanned by the orthogonal vectors $Y_1$, \ldots, $Y_n$.

%Since the estimate $\Eh_i$ is the projection of $E_i$ onto $Y_{i+1}$, 
%\begin{equation*}
%  \Eh_i = \frac{\sp{E_i}{Y_{i+1}}}{\|Y_{i+1}\|^2} Y_{i+1}.
%\end{equation*}
%Writing $Y_{i+1} = \alpha E_i + Z_{i+1}$, with $\alpha = \sqrt{P}/\|E_i\|$,
%yields
%\begin{equation*}
%  \|E_{i+1}\|^2 = \|\Eh_i - E_i\|^2 = \frac{\|E_i\|^2}{1 + P/\|Z_{i+1}\|^2},
%\end{equation*}
%which is nothing else than Equation~\ref{eq:gaussvardec} expressed in the inner
%product space formalism.



%\subsection{Other Results}
%
%\subsubsection{Noisy Feedback}
%
%\subsubsection{Minimal-Delay Codes for Arbitrary Rates}


\section{Lessons for the Feedbackfree Case}

\begin{notebox}
  I have been trying to support the intuitive arguments in this paragraph with
  solid formulas, but so far without success. I would like to formulate the
  statement ``the encoder knows the decoder's state'' using mutual information.

  Is it possible to express a particular mutual information
  once for the feedback case and once for the case without feedback (using
  Massey's definition $p(x_i|x^{i-1},y^{i-1}) = p(x_i|x^{i-1})$ for ``used
  without feedback''), and then show for which condition the two mutual
  informations are equal? This condition would be the formal equivalent of ``the
  encoder knows the state of the decoder''.
\end{notebox}
The two preceding sections show that when $n > 1$, the only known optimal
$(1,n)$~communication system requires feedback. The same is true for the only
known \emph{asymptotically optimal} $(1,n)$~communication system.  To state the
very obvious: the difference between the case with and without feedback is that
if the channel has feedback, the encoder knows exactly the state of the decoder
before sending each subsequent channel input. The encoder can thus exactly
determine the additional information needed to refine the decoder's estimate and
can avoid sending anything redundant. 

If there is no feedback, by the very nature of the Gaussian channel, the encoder
does not know the state of the decoder after transmitting a symbol across the
channel. If it were possible, though, by using appropriate transformations at
the channel input and output to turn the Gaussian channel into an error free
channel, then this would in essence bring about the same situation as in the
case with feedback. 

Probably the most well known way to turn a channel into an error free channel is
the channel coding theorem. As was already stated on several occasions
throughout this chapter, however, this requires large blocklenghts, which are
unsuitable for the problem of finding good \emph{minimal delay} codes. The
establishment of minimal delay codes that turn a Gaussian channel into a channel
that is with high probability error free is the subject of the next chapter. 

\begin{subappendices}
  \section{Asymptotic Notation}\label{app:asymptotic}

  \begin{definition}
    \label{def:bigo}
    Let $f(x)$ and $g(x)$ be two functions defined on~$\R$. The set $O(g(x))$ is
    defined as
    \begin{equation*}
      f(x) \in O(g(x))
    \end{equation*}
    if and only if there exists an $x_0$ and a constant~$c$ such that
    \begin{equation*}
      f(x) \le c g(x)
    \end{equation*}
    for all $x > x_0$. 
    Similarly, $f(x) \in \Omega(g(x))$ if $\le$ is replaced by $\ge$ in
    the above definition. Finally, $\Theta(g(x)) \deq O(g(x)) \cap
    \Omega(g(x))$.
  \end{definition}

\end{subappendices}
