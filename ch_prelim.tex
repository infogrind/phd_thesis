\chapter{Preliminaries}
\label{ch:prelim}

The problem considered in this and the next chapter is the transmission of
analog sources across Gaussian channels. The present chapter provides the
necessary definitions and notation, and recalls previous results.

\section{Basic Definitions and Notation}
\label{sec:defs}

\begin{definition}[Source]
  \label{def:analogsource}
  A discrete-time memoryless source is specified by a probability distribution
  $\pS(s)$ on~$\R$. An \emph{analog} source is a source that admits a
  probability density function \emph{(\pdf)}.
\end{definition}

\begin{definition}[Channel]
  \label{def:dtmlc}
  A discrete-time memoryless channel is specified by a conditional probability
  distribution $\pyx(y|x)$, assigning for each channel input~$x$ an output
  distribution $p_{Y|X=x}$. 
\end{definition}

\begin{definition}
  \label{def:awgn}
  An additive white Gaussian noise channel (AWGN channel) with noise
  variance~$\sq$ is a discrete-time memoryless channel whose output, conditioned
  on the input $X=x$, is a Gaussian random variable $Y$ with mean~$x$ and
  variance~$\sq$. An AWGN channel is sometimes also specified by relating the
  output $Y$ to the input $X$ through $Y = X + Z$, where $Z$ is zero-mean
  Gaussian with variance~$\sq$. 
\end{definition}

\begin{definition}[Joint source/channel code]
  \label{def:knsccode}
  A $(k,n)$~\emph{joint source/channel code} $(f,g)$ consists of an
  \emph{encoder} function $f : \R^k \ra \R^n$ and a \emph{decoder}
  function $g:\R^n \ra \R^k$. The \emph{rate} of a $(k,n)$~joint
  source/channel code is $\k = k/n$. If $\k < 1$, the code is called
  a \emph{bandwidth expansion} code; if $k > 1$, the code is called a
  \emph{bandwidth compression} code. If $\k = 1$, the code is called a
  \emph{bandwidth matched} code.
\end{definition}

\begin{definition}[Minimal-delay codes]
  \label{def:mindelcode}
  A $(k,n)$~\emph{minimal-delay} joint source\slash channel code is a
  $(k,n)$~joint source/channel code with $\gcd(k,n) = 1$. A minimal-delay joint
  source/channel code of rate~$1$ is also called a \emph{single letter} code.
\end{definition}

\begin{definition}
  \label{def:jointsccommsys}
  A $(k,n)$~\emph{source/channel communication system} consists of
  a discrete-time memoryless source~$\pS$, a discrete memoryless channel~$\pyx$,
  and a $(k,n)$~joint source/channel code~$(f,g)$ of rate~$\k = k/n$. The
  encoder maps $k$~source symbols $S^k$ into $n$~channel input symbols~$X^n =
  f(S^k)$, and the decoder maps $n$~channel output symbols $Y^n$ into $k$~source
  estimates $\Sh^k = g(Y^n)$.
\end{definition}

For an illustration of a general source/channel communication system, see
\figref{gensccommsys}.
\begin{figure}[tbp]
  \begin{center}
    \input{figures/sc_gen.tex_t}
  \end{center}
  \caption{A general source/channel communication system of rate $\k = k/n$
  according to \defref{jointsccommsys}.}
  \label{fig:gensccommsys}
\end{figure}

\begin{definition}
  \label{def:PDgen}
  For a given \emph{channel input cost measure}~$\rho(x)$ and a given
  \emph{distortion measure}~$d(s,\sh)$, the \emph{expected cost}~$P$ and
  \emph{expected distortion}~$D$ incurred by a $(k,n)$ source/channel
  communication system are given by
  \begin{equation*}
    P = \frac1n \sn \E[\rho(X_i)] \quad \text{and} \quad
    D = \frac1k \sk \E[d(S_i - \Sh_i)],
  \end{equation*}
  respectively.
\end{definition}

\begin{definition}
  \label{def:sdrsnr}
  Consider a source/channel communication system whose source has
  variance~$\ssq$ and whose channel is Gaussian with noise variance~$\szq$. If
  the cost measure is the input power, \ie, $\rho(x) = x^2$, and the distortion
  measure is the squared error, \ie, $d(s,\sh) = (s - \sh)^2$, then the
  \emph{source-to-distortion ratio} \sdr\ and the \emph{signal-to-noise ratio}
  \snr\ are defined respectively as
  \begin{equation*}
    \sdr = \frac{\ssq}{D} \quad \text{and} \quad
    \snr = \frac{P}{\szq}.
  \end{equation*}
\end{definition}


\begin{definition}
  \label{def:codingscheme}
  A $(k,n)$ \emph{coding scheme} consists of a function $\tilde{f} : \R^k \times
  \R_+ \ra \R^n$ and a function $\tilde{g}: \R^n \times \R_+ \ra \R^k$, such
  that for every value of $\snr$, $(f(\cdot, \snr), g(\cdot, \snr))$ is a
  $(k,n)$~joint source/channel code that when used in a Gaussian communication
  system leads to $P/\szq = \snr$.
\end{definition}


\section{Fundamental Limits of Performance}
\label{sec:limits}

The most general bound on the performance of a joint source/channel coding
system is Shannon's separation theorem, stated below.

\begin{theorem}
  \label{thm:sepconverse}
  The expected distortion~$D$ and expected cost~$P$ of a source/channel
  communication system of rate~$\k$ are related by
  \begin{equation}
    \label{eq:sepconverse}
    \k R(D) \le C(P), 
  \end{equation}
  where $R(D)$ is the rate-distortion function of the source and $C(P)$ is the
  capacity-cost function of the channel.
\end{theorem}

\begin{proof}
  For a proof, see~\cite[Theorem~9.6.1]{Gallager1968}.
\end{proof}

\begin{definition}
  \label{def:optimalcode}
  A source/channel communication system that satisfies the bound of
  \thmref{sepconverse} with equality is said to be \emph{optimal}.
\end{definition}

For Gaussian channels, \thmref{sepconverse} leads to the following upper bound
on the~\sdr.
\begin{theorem}
  \label{thm:sdrub}
  If a discrete-time analog source $\pS$ of zero mean and variance~$\ssq$ is
  transmitted across an AWGN channel with noise variance~$\szq$ using a code of
  rate~$\k$, the \sdr\ is bounded by
  \begin{equation}
    \label{eq:sdrub}
    \sdr \le 2^{2 D(\pS \| \phi_{\ssq})} (1 + \snr)^{1/\k},
  \end{equation}
  where $D(\cdot \| \cdot)$ is the relative entropy or Kullback-Leibler distance
  between two distributions (see \eg~\cite{CoverT1991}) and $\phi_{\ssq}$ is a
  centered Gaussian distribution of variance~$\ssq$.
\end{theorem}

\begin{remark}
  \label{rem:perflimitgaussiansource}
  For a Gaussian source the bound of \thmref{sdrub} simplifies to $\sdr \le (1 +
  \snr)^{1/\k}$ and is simply \thmref{sepconverse} applied to a Gaussian
  source and channel. This bound is therefore tight, in the sense that there
  exist codes that can come arbitrarily close to equality. 
  For other sources the bound is generally not tight, but it becomes tight as
  $\snr \ra \infty$~\cite{LinderZ1994}.
\end{remark}

\begin{proof}[Proof of \thmref{sdrub}]
  The rate distortion function of an analog source of finite entropy~$h(S)$
  with squared error distortion satisfies~\cite{Shannon1959}
  \begin{equation*}
    R(D) \ge h(S) - \frac12 \log_2(2\pi e D).
  \end{equation*}
  Inserting the capacity of the Gaussian channel into~\eqref{eq:sepconverse}
  results in
  \begin{equation*}
    \k R(D) \le \frac12 \log_2(1 + \snr)
  \end{equation*}
  and combining the two above inequalities yields
  \begin{equation*}
    \frac1D \le 2^{-2h(S)} 2\pi e (1 + \snr)^{1/\k}.
  \end{equation*}
  Multiplying both sides with~$\ssq$ and noting that $0.5 \log_2(2\pi e \ssq) =
  h(\phi_{\ssq})$ leads to
  \begin{equation*}
    \sdr \le 2^{2(h(\phi_{\ssq}) - h(S))} (1 + \snr)^{1/\k}.
  \end{equation*}
  Applying the property $h(\phi_{\ssq}) - h(S) = D(\pS \|
  \phi_{\ssq})$~\cite[Theorem~8.6.5]{CoverT1991} completes the proof.
\end{proof}

\begin{remark}
  \label{rem:asympbound}
  In asymptotic terms, \thmref{sdrub} says that the \sdr\ scales at best as
  $\snr^{1/\k}$, or $\sdr \in O(\snr^{1/\k})$. (See \appref{asymptotic} for the
  definition of the $O$-notation.)
\end{remark}

\begin{definition}
  A coding scheme is called \emph{asymptotically optimal} if it satisfies $\sdr
  \in \Omega(\snr^{1/\k})$.
\end{definition}


According to the source coding and channel coding theorems, there exist
asymptotically optimal coding schemes. The codes whose existence is proved by
these theorems, however, are of unbounded blocklength (\ie, unbounded delay) and
of unbounded complexity. For minimal delay codes, bounds based on
\thmref{sepconverse} are in general not tight; the remainder of this chapter
studies certain exceptions.


\section{Optimal Minimal-Delay Codes}
\label{sec:optmindel}

\subsection{Bandwith Matched Codes}

By definition, minimal-delay bandwidth matched codes are single letter codes. A
particular example of an optimal single letter code is that of a Gaussian source
connected to a Gaussian channel, as given in the following example.
\begin{example}
  \label{ex:gausssingle}
  Let the source be zero-mean Gaussian with variance~$\ssq$ and let the channel
  be AWGN with noise variance~$\szq$. The encoder is given by $X = f(S) =
  \sqrt{P/\ssq} S$ and the decoder is given by $\Sh = g(Y) = \sqrt{P} \ssq Y /
  (P + \szq)$. Using $Y = X + Z$ it is quickly verified that 
  \begin{equation*}
    D = \E[(S - \Sh)^2] = \ssq / (1 + P/\szq),
  \end{equation*}
  which is indeed the optimal distortion according to
  \remref{perflimitgaussiansource}.
\end{example}

\exref{gausssingle} is a particular instance of a more general paradigm called
\emph{measure matching}. Measure matching provides conditions under which a code
of finite blocklength achieves the bound of \thmref{sepconverse} with equality.
For an excellent treatment of measure matching the reader is referred to
Gastpar~\cite{GastparRV2003,GastparThesis}; in the sequel only the results
relevant to this thesis are quoted.

The first relevant result from~\cite{GastparRV2003} says that any bandwidth
matched code is optimal for \emph{some} way of measuring the cost and
distortion.

\begin{theorem}
  \label{thm:tcntcbwmatch}
  A single letter code is optimal for a source/channel communication system of
  rate one, \ie, $R(D) = C(P)$, if and only if the following conditions are
  met.
  \begin{enumerate}[(i)]
    \item The cost measure~$\rho(x)$ satisfies
      \begin{equation}
        \label{eq:optcost}
        \rho(x)
        \begin{cases}
          = c_1 D(p_{Y|X=x} \| p_Y) + \beta & \text{if $p_X(x) > 0$} \\
          \ge c_1 D(p_{Y|X=x} \| p_Y) + \beta & \text{if $p_X(x) = 0$},
        \end{cases}
      \end{equation}
      where $c_1 > 0$ and $\beta$ are arbitrary real constants.

    \item The distortion measure~$d(s,\sh)$ satisfies
      \begin{equation}
        \label{eq:optdist}
        d(s,\sh) = - c_2 \log_2 \frac{p(\sh|s)}{p(\sh)} + d_0(s),
      \end{equation}
      where $c_2 > 0$ and $d_0(\cdot)$ is an arbitrary function.

    \item The code is information lossless in the sense that~$I(S;\Sh) =
      I(X;Y)$.
  \end{enumerate}
\end{theorem}

\begin{remark}
  \label{rem:tcntcrug}
  The statement of \thmref{tcntcbwmatch} ignores a few special cases,
  notably the case $I(S;\Sh) = 0$ and the case $I(X;Y) = \max_P C(P)$; these are
  treated in detail in~\cite{GastparRV2003} and are not relevant here.
\end{remark}

\begin{proof}[Proof of \thmref{tcntcbwmatch}]
  The proof hinges on the inequality chain $R(D) \le I(S;\Sh) \le I(X;Y) \le
  C(P)$. The complete proof, found in~\cite{GastparRV2003}, shows that $I(X;Y) =
  C(P)$ if and only if condition~(i) is satisfied and that $R(D) = I(S;\Sh)$ if
  and only if condition~(ii) is satisfied. It is then clear that condition~(iii)
  is the third condition needed for $R(D) = C(P)$.
\end{proof}

The single letter code of \exref{gausssingle} is optimal precisely because
$\rho(x) = x^2$ satisfies condition~(i) and $d(s,\sh) = (s - \sh)^2$ satisfies
condition~(ii) for the communication system at hand. Moreover, since the encoder
and decoder are both one-to-one maps, condition~(iii) is trivially satisfied.

According to \thmref{tcntcbwmatch} there exists an infinity of optimal
single letter codes. For systems with bandwith expansion or compression,
however, optimal minimal delay codes may not exist, as the following section
argues.


\subsection{Bandwidth Expansion/Compression Codes}

If $k \ne n$ it is not known whether optimal minimal delay codes exist.  For the
particular case of a Gaussian source and a Gaussian channel with input power
constraint and squared error distortion, it has in fact been proven that no
optimal $(k,1)$ and $(1,n)$~codes exist~\cite{IngberLZF2008}.

For codes that are not bandwidth matched, optimality is no longer only a matter
of measure matching in the sense of \thmref{tcntcbwmatch}; additional conditions
need to be fulfilled. The following theorem lists these conditions. It is
essentially Theorem~3.9 from~\cite{GastparThesis}, adapted to the case without
feedback and with a slightly more extended discussion on the ``information
lossless'' property of the code. 

\begin{theorem}
  \label{thm:tcntc1n}
  For $k \ne n$, a $(k, n)$~source/channel communication system is optimal if
  and only if the following conditions are met.
  \begin{enumerate}[(i)]
    \item
      \begin{enumerate}[(a)]
        \item The conditional distribution of the source symbols given the
          estimates can be factored as $p(s^k|\sh^k) = \pk p(s_i|\sh_i)$, and
        \item each $p(s_i|\sh_i)$ achieves the rate distortion function at the
          same average distortion.
      \end{enumerate}

    \item The estimates $\Sh^k$ form a sufficient statistic for $S^k$ given the
      outputs~$Y^n$.

    \item The encoder is information lossless in the sense that $I(S^k; Y^n) =
      I(X^n; Y^n)$. 

    \item The channel outputs $Y_1$, \ldots, $Y_n$ are mutually independent.

    \item The marginal distributions $p(x_i)$ of the channel inputs all achieve
      the capacity at the same average cost.
  \end{enumerate}
\end{theorem}

\begin{discussion}
  If $k = 1$, condition~(i)(a) of the theorem is trivially satisfied. If $n =
  1$, condition~(iv) is trivially satisfied. Furthermore, if condition~(i)(a) is
  satisfied then condition~(i)(b) is only a matter of matching the distortion
  measure. Similarly, if condition~(iv) is satisfied then condition~(v) is only
  a matter of matching the cost measure. 
\end{discussion}

\begin{proof}
  The proof is based on the following chain of inequalities.
  \begin{align}
    \label{eq:14step1}
    k R(D) &\le I(S^k; \Sh^k) \\
    \label{eq:14step2}
    &\le I(S^k; Y^n) \\
    \label{eq:14step3}
    &\le I(X^n; Y^n) \\
    \label{eq:14step4}
    &\le \sn I(X_i; Y_i)  \\
    \label{eq:14step5}
    &\le n C(P).
  \end{align}
  Inequality \eqref{eq:14step1} follows from the definition of~$R(D)$; it
  becomes an equality if and only if condition~(i) is satisfied.
  \eqref{eq:14step2} is the data processing inequality, it becomes an inequality
  if and only if condition~(ii) is satisfied. Next, \eqref{eq:14step3} is again
  the data processing inequality and it is satisfied with equality if and only
  if condition~(iii) is satisfied.  Inequality~\eqref{eq:14step4} follows
  because the channel is memoryless and because conditioning can only decrease
  the entropy. It becomes an inequality if and only if condition~(iv) is
  satisfied. Inequality~\eqref{eq:14step5}, finally, follows from the definition
  of~$C(P)$ and becomes an equality if and only if condition~(v) is satisfied.
\end{proof}

The following lemma gives a condition equivalent to conditions~(i)(a) and~(ii).
\begin{lemma}
  \label{lem:ssil}
  Adapt here the lemma from the feedback case to the feedbackfree case.
\end{lemma}
\begin{proof}
  \todo.
\end{proof}

\begin{remark}
  \label{rem:inflosslessenc}
  A deterministic encoder is sufficient but not necessary for condition~(iii).
  Nevertheless, any nondeterministic encoder satisfying condition~(iii) can be
  turned into a deterministic encoder that also satisfies the condition as the
  following argument shows. 

  \todo: Complete this argument.
\end{remark}

The consequence of \thmref{tcntc1n}, and the discussion following it, is this:
when $k \ne n$, finding an optimal communication system is no longer only a
matter of matching cost and distortion measures. More precisely, only
conditions~(i)(b) and~(v) can be fulfilled by choosing a matching cost and
distortion measure, and only provided that the remaining conditions are
satisfied. These remaining conditions are absolute: they depend only on the
statistical properties of the relevant quantities. These findings are summarized
in the following lemma.

\begin{lemma}
  \label{lem:optcodenkexist}
  For the communication system of \figref{gensccommsys} with $k \ne n$, an
  optimal minimal delay $(k,n)$~source/channel communication system exists if
  and only if there exists an $(k,n)$~communication system satisfying
  conditions~(i)(a) and (ii)--(iv) of \thmref{tcntc1n}.
\end{lemma}


\section{Optimal Minimal-Delay Codes with Feedback}
\label{sec:optmindelfb}

\begin{definition}
  \label{def:fbcode}
  A $(k,n)$ \emph{joint source/channel feedback code} $(f,g)$ consists of
  $n$~\emph{encoding functions} $f_i : \R^k \times \R^{i-1} \ra \R$, $i = 1$,
  \dots, $n$, and a decoding function $g: \R^n \ra \R^k$. 
\end{definition}

\begin{definition}
  \label{def:fbcommsys}
  A $(k,n)$ \emph{source/channel communication system with feedback} is a
  $(k,n)$ communication system where the encoder has noiseless feedback from the
  channel output, and a $(k,n)$ feedback code computes the $i^{\text{th}}$
  channel input as $X_i = f_i(S^k, Y^{i-1})$ and the source estimate as $\Sh^k =
  g(Y^n)$. 
\end{definition}

\figref{fbcommsys} has a schematic depiction of a feedback communication system.
\begin{figure}[tbp]
  \begin{center}
    \input{figures/sc_gen_feedback.tex_t}
  \end{center}
  \caption{A general source/channel communication system with feedback.}
  \label{fig:fbcommsys}
\end{figure}

Since feedback does not increase the capacity of memoryless channels, the bound
of \thmref{sepconverse} also applies to systems with feedback. It is therefore
clear that an optimal single letter code for a system without feedback is also
optimal for a system with feedback. While there are no known optimal
communication systems without feedback when $k \ne n$, there is at least one
known optimal feedback system with $k = 1$ and $n > 1$, as the following example
shows.

\begin{example}
  \label{ex:gaussfb}
  In this example, a Gaussian source is transmitted across an AWGN channel, and
  the channel is used $n$~times per source symbol.  Define $E_0 = S$. In the
  $i^{\text{th}}$ channel use, the encoder produces
  \begin{equation}
    \label{eq:gaussfbxi}
    X_i = \sqrt{\frac{P}{\Var E_{i-1}}} E_{i-1}.
  \end{equation}
  Both the receiver and the sender now compute the minimum mean-squared
  error (MMSE) estimator $\Eh_{i-1}$ of $E_{i-1}$ given $Y_i$. The sender
  computes $E_i = \Eh_{i-1} - E_{i-1}$ and proceeds to the next round.

  After $n$~rounds of transmission, the receiver has $n$~estimates $\Eh_0$
  to~$\Eh_{n-1}$. Using these, it computes the final estimate~$\Sh$ as
  \begin{equation}
    \label{eq:shdecom1}
    \Sh = \Eh_0 - \Eh_1 + \Eh_2 - \cdots \pm \Eh_{n-1}.
  \end{equation}
  (The sign of the last term is $+$ if $n$~is even and $-$ if $n$~is odd.)

  To compute the overall distortion $\mse$, note that by definition $\Eh_{i-1} =
  E_{i-1} + E_i$, so \eqref{eq:shdecom1}~can be written as
  \begin{align*}
    \Sh &= (E_0 + E_1) - (E_1 + E_2) + (E_2 + E_3) - \cdots \pm (E_{n-1} + E_n)
    \\
    &= E_0 \pm E_n,
  \end{align*}
  and since we have defined $E_0 = S$, we have $\mse = \E[E_n^2]$, where $E_n$
  is the remaining error after the last round of transmission.

  To compute $\E[E_n^2]$, note that since $\Eh_i$ is the MMSE estimator of
  $E_i$, the estimation error variance is given by (see
  \eg~\cite[Section~8.3]{Scharf1990})
  \begin{equation}
    \label{eq:gaussvardec}
    \E[(\Eh_i - E_i)^2] = \frac{\E[E_i^2]}{1 + P/\szq}.
  \end{equation}
  Using $\E[E_0^2] = \E[S^2] = \ssq$ and recursively applying the above, we find
  that
  \begin{equation*}
    \frac{\ssq}{\E[E_n^2]} = \frac{\ssq}{\E[(\Eh_{n-1} - E_{n-1})^2]} =
    (1 + P/\szq)^n,
  \end{equation*}
  which is indeed the largest possible \sdr\ according
  to~\remref{perflimitgaussiansource}.
\end{example}

To see why this example works, let us restate \thmref{tcntc1n} for the case with
feedback. 
\begin{theorem}
  \label{thm:tcntcfb}
  For $k \ne n$, a $(k, n)$~source/channel communication system with feedback is
  optimal if and only if the following conditions are met.
  \begin{enumerate}[(i)]
    \item
      \begin{enumerate}[(a)]
        \item The conditional distribution of the source symbols given the
          estimates can be factored as $p(s^k|\sh^k) = \pk p(s_i|\sh_i)$, and
        \item each $p(s_i|\sh_i)$ achieves the rate distortion function at the
          same average distortion.
      \end{enumerate}

    \item The estimates $\Sh^k$ form a sufficient statistic for $S^k$ given the
      outputs~$Y^n$.

    \item The encoder is information lossless in the sense that $I(S^k; Y^n) =
      I(X^n \ra Y^n)$. 

    \item The channel outputs $Y_1$, \ldots, $Y_n$ are mutually independent.

    \item The marginal distributions $p(x_i)$ of the channel inputs all achieve
      the capacity at the same average cost.
  \end{enumerate}
\end{theorem}

\begin{proof}
  The proof is essentially the same as that of \thmref{tcntc1n}, except that the
  inequality $I(X^n;Y^n) \le \sn I(X_i; Y_i)$ is replaced by $I(X^n \ra Y^n) \le
  \sn I(X_i; Y_i)$, where $I(X^n \ra Y^n)$ is the \emph{directed information}
  from~$X^n$ to~$Y^n$ (see~\cite{Massey1990,Kramer1998}). A necessary and
  sufficient condition for $I(X^n \ra Y^n) = \sn I(X_i; Y_i)$ is condition~(iv)
  of the theorem~\cite[Theorem~2]{Massey1990}.
\end{proof}

Let us now revisit Example~\ref{ex:gaussfb} and see why it achieves the optimal
distortion. We will go through the conditions of \thmref{tcntcfb}
in reverse order, starting at condition~(v). Since the source and the noise are
jointly Gaussian and the encoder and decoder are linear, all channel inputs
$X_i$ are Gaussian; since they are scaled to have variance~$P$ they all achieve
the capacity at the same average cost. Next, because the estimation error of an
MMSE estimator is uncorrelated with the observation and because in the Gaussian
case uncorrelated implies independent, $E_i$ is independent of $Y_{i-1}$ and
thus so are~$X_i$ and~$Y_i$, satisfying condition~(iv). Condition~(iii) is
satisfied because the encoder is deterministic. For conditions~(i) and~(ii),
observe that the final estimate $\Sh$ is such that $S = \Sh + E_n$, where $E_n$
is independent of $\Sh$ and of $Y^n$. Given $\Sh$, $S$ is therefore independent
of~$Y^n$, which makes $\Sh$ a sufficient statistic, satisfying condition~(ii).
Moreover, the relationship between $S$ and $\Sh$ is exactly the one leading to
the distribution that achieves the rate distortion function (see
\eg~\cite[Theorem~10.3.2]{CoverT1991}), fulfilling condition~(i)(b).
Condition~(i)(a) is trivially satisfied since~$k=1$.

It appears as though this example works only because of the particular
properties of the Gaussian distribution: preservation of distribution under
linear transformation, linear MMSE decoder, equivalence of uncorrelatedness and
independence, and so on. From this example alone, one would therefore not
conclude that there exist optimal codes for other sources and channels. As the
next section shows, though, at least conditions~(iii)--(v) of \thmref{tcntcfb}
can be achieved with a minimal delay code for any source and channel if perfect
feedback is available.


\subsection{Posterior Matching}

Example~\ref{ex:gaussfb} showed how a simple transmission scheme can achieve the
optimal distortion using noiseless feedback. The discussion in the previous
section showed how the particular properties of the Gaussian distribution helped
in achieving this. In the sequel we show that conditions~(iii) to~(v) of
\thmref{tcntcfb} can be satisfied for arbitrary sources and channels. The
underlying idea, called \emph{posterior matching}, was used by Shayevitz and
Feder in~2007~\cite{ShayevitzF2007,ShayevitzF2008} to generalize the capacity
achieving channel coding schemes of Schalkwijk and
Kailath~\cite{SchalkwijkK1966} and Horstein~\cite{Horstein1963} to arbitrary
channels with feedback.

Before continuing we prove some properties of cumulative distribution functions
(\cdf s).

\begin{lemma}
  \label{lem:cdfunif}
  Let $X$ be a continuous random variable with density $f(x)$ and \cdf\ $F_X$,
  \ie,
  \begin{equation*}
    F_X(x) = \Pr[X \le x].
  \end{equation*}
  Then the random variable $F_X(X)$ is uniformly distributed on~$[0,1]$.
\end{lemma}

\begin{proof}
  Let $Y = F_X(X)$. Then
  \begin{align*}
    \Pr[Y \le y] &= \Pr[F_X(X) \le y] \\
    &= \Pr[X \le F_X^{-1}(y)] \\
    &= \int_{-\infty}^{F_X^{-1}(y)} f(x) dx \\
    &= F_X(F_X^{-1}(y)) = y,
  \end{align*}
  which is the \cdf\ of a uniform random variable on $[0,1]$.
\end{proof}


\begin{lemma}
  \label{lem:invcdf}
  Let $U$ be a uniform random variable on~$[0,1]$ and let $F$ be the \cdf\ of an
  arbitrary random variable~$X$. If $F$~is not invertible in the strict sense,
  define $F^{-1}$ as
  \begin{equation}
    \label{eq:invcdf}
    F^{-1}(y) = \sup \{x : F(x) \le y\}.
  \end{equation}
  Then the random variable $F^{-1}(U)$ has the same distribution as~$X$.
\end{lemma}

\begin{proof}
  The definition of $F^{-1}$ according to~\eqref{eq:invcdf} has the property
  that $\Pr[F^{-1}(U) \le x] = \Pr[U \le F(x)]$. Thus,
  \begin{align*}
    \Pr[F^{-1}(U) \le x] &= \Pr[U \le F(x)] \\
    &= \int_0^{F(x)} d\xi = F(x).
  \end{align*}
\end{proof}

Consider now a channel~$\pyx$ and let $\pi(x)$ be the capacity achieving
distribution at average cost~$P$, \ie, 
\begin{equation*}
  \pi(x) = \arg\max_{p(x): \E[\rho(X)] \le P} I(X;Y).
\end{equation*}
The problem is to encode one source symbol of an \emph{analog} source into~$n$
channel inputs, making use of the feedback.

Let $\Fpi$ be the cumulative distribution function (\cdf) of the distribution
$\pi(x)$, and let $F_S$ be the \cdf\ of the source. In the first channel
use, the encoder produces
\begin{equation}
  \label{eq:posteriorx1}
  X_1 = \Fpi^{-1}(F_S(S)),
\end{equation}
where $\Fpi^{-1}$ is the inverse of $\Fpi$ according to~\eqref{eq:invcdf}. By
Lemma~\ref{lem:cdfunif}, $F_S(S)$ has uniform distribution on $[0,1]$, and so by
Lemma~\ref{lem:invcdf}, $\Fpi^{-1}(F_S(S))$ is a random variable with \cdf\
$\Fpi$.

After $i-1$ rounds of transmission, the encoder knows $y_1$, \ldots, $y_{i-1}$
and can compute the conditional
\cdf\ $F_{S|y_1, \ldots, y_{i-1}}$. It then sends
\begin{equation}
  \label{eq:posteriorxi}
  X_i = \Fpi^{-1}(F_{S|y_1,\dots,y_{i-1}}(S)).
\end{equation}
For any $y_1$, \ldots, $y_{i-1}$, therefore,
\begin{equation*}
  p(x_i|s, y_1, \dots, y_{i-1}) = \pi(x)
\end{equation*}
and so $X_i$ is independent of $Y_1$, \ldots, $Y_{i-1}$. 

Using this strategy the encoder produces an \iid\ sequence of inputs $X_i$
with the capacity achieving distribution~$\pi(x)$, satisfying conditions~(iv)
and~(v) of \thmref{tcntcfb}; condition~(iii) of the theorem is
trivially satisfied because the encoder is deterministic.

Let us now derive the posterior matching encoder for the communication system of
\exref{gaussfb}.

\begin{example}
  \label{ex:gaussfbpost}
  First, a few properties of Gaussian \cdf s are given. Let $F_{\N(\mu, \sq)}$
  be the \cdf\ of a Gaussian random variable of mean~$\mu$ and variance~$\sq$
  and let $F_\N \deq F_{\N(0,1)}$. Then $F_{\N(\mu,\sq)}(x) =
  F_\N((x-\mu)/\sigma)$. Furthermore, the inverse \cdf\ is 
  \begin{equation*}
    F_{\N(\mu,\sq)}^{-1}(y) = \sigma F_\N^{-1}(y) + \mu.
  \end{equation*}

  Let $\pi(x) = \N(0,P)$. According to~\eqref{eq:posteriorx1}, the first channel
  input is
  \begin{equation*}
    X_1 = \sqrt{P} F_{\N}^{-1}(F_{\N}(S/\sigma_S)) = \sqrt{\frac{P}{\ssq}} S,
  \end{equation*}
  which coincides with~\eqref{eq:gaussfbxi} in \exref{gaussfb} when~$i=1$.

  Given $Y_1$, $S$ is Gaussian with mean $\E[S|Y_1]$ and variance $\Var(S -
  \E[S|Y_1])$. Following~\eqref{eq:posteriorxi}, the second channel input is
  thus
  \begin{align*}
    X_2 &= \sqrt P F_{\N}^{-1} \left( F_{\N} \left( \frac{S - \E[S|Y_1]}
    {\sqrt{\Var(S-\E[S|Y_1])}} \right) \right) \\
    &= \sqrt{P} \frac{S - \E[S|Y_1]}{\sqrt{\Var(S-\E[S|Y_1])}}.
  \end{align*}
  Continuing this way, the $i^{\text{th}}$ channel input is found to be
  \begin{equation}
    \label{eq:gausspmenc}
    X_i = \sqrt{P} \frac{S - \E[S|Y_1^{i-1}]}{\sqrt{\Var(S-\E[S|Y_1^{i-1}])}}.
  \end{equation}
  That this is equal to~\eqref{eq:gaussfbxi} can be seen as follows. In
  \exref{gaussfb}, write
  \begin{align*}
    S &= E_0 + (E_1 - E_1) - (E_2 - E_2) + \dots \pm (E_{i-2} -
    E_{i-2}) \\
    &= (E_0 + E_1) - (E_1 + E_2) + (E_2 + E_3) - \dots - E_{i-2} \\
    &= \Eh_0 - \Eh_1 + \Eh_2 - \dots - E_{i-2}.
  \end{align*}
  Since $\E[\Eh_j | Y_1^{i-1}] = \Eh_j$ for $j = 1$, \dots,~$i-2$, and
  $\E[E_{i-2}|Y_1^{i-1}] = \Eh_{i-2}$, 
  \begin{equation*}
    \E[S|Y_1^{i-1}] = \Eh_0 - \Eh_1 + \dots  - \Eh_{i-2}
  \end{equation*}
  and so $S - \E[S|Y_1^{i-1}] = \Eh_{i-2} - E_{i-2} = E_{i-1}$. Plugging this
  into~\eqref{eq:gausspmenc} yields exactly the encoder~\eqref{eq:gaussfbxi} of
  Example~\ref{ex:gaussfb}.
\end{example}

\begin{remark}
  \label{rem:alwaysdistmatch}
  A question of interest is whether the encoder of \emph{any} optimal
  source/channel communication system can be expressed as a posterior matching
  encoder. The answer is in fact negative. Since by definition cumulative
  distribution functions are increasing, the posterior matching
  encoders~\eqref{eq:posteriorx1} and~\eqref{eq:posteriorxi} are always
  increasing functions. Yet, using measure matching it is straightforward to
  come up with an optimal $(1,1)$~communication system that has a nonincreasing
  encoder: just take the encoder to be an arbitrary function, and select the
  cost and distortion measures accordingly following \thmref{tcntcbwmatch}.
\end{remark}


\subsection{Achieving $R(D)$ using Posterior Matching?}

The previous section showed how one can turn an arbitrary distribution into the
capacity achieving distribution. Can the same trick be used to make the
conditional distribution of~$\Sh$ given~$S$ achieve the rate distortion
function? 

For simplicity consider a $(1,1)$ communication system (whether it has feedback
or not is irrelevant). For a fixed~$D$, let
\begin{equation*}
  \Phi_s(\sh) = \arg\min_{p(\sh|s): \E[d(S,\Sh)] \le D} I(S;\Sh),
\end{equation*}
\ie, $\Phi_s(\sh)$ is the conditional distribution of~$\Sh$ given~$S$ that
achieves the rate distortion function at expected distortion~$D$. Let the
decoder be
\begin{equation}
  \label{eq:distmatchdec}
  g(y) = F_{\Phi_s}^{-1}(F_{Y|S=s}(y)).
\end{equation}
According to the previous section, the resulting joint distribution of $\Sh =
g(Y)$ and~$S$ satisfies thus $I(S;\Sh) = R(D)$. 

It is immediately clear, however, that this approach cannot work -- both \cdf s
needed to implement this decoder depend on the actual value of~$s$, which is
obviously not known at the decoder (there would not really be a communication
problem otherise). Interestingly, though, in the Gaussian case the dependence
on~$s$ of $F_{\Phi_s}$ and of $F_{Y|S=s}$ cancel each other out, and the
decoder~\eqref{eq:distmatchdec} yields again the MMSE decoder, as the following
example shows.

\begin{example}
  Let the source $S$ be distributed as $\N(0,1)$ and let the channel be AWGN
  with noise variance~$1$ and input constraint $\E[X^2] \le P$.
  The distortion is the squared error. The smallest achievable distortion is
  \begin{equation}
    \label{eq:exmindist}
    D_{\min} = \frac{1}{1 + P}.
  \end{equation}
  The capacity-achieving input distribution is $\N(0,P)$, and the conditional
  distribution of $\Sh$ given $S=s$ that achieves the rate distortion function
  at distortion~$D$ is $\N((1-D)s, D(1-D))$.
  Let $X = \sqrt{P}S$.  The decoder from~\eqref{eq:distmatchdec} is
  \begin{align*}
    g(y) &= F_{\Phi_s}^{-1} (F_{Y|S=s}(y)) \\
    &= \sqrt{D(1-D)} F_{\N}^{-1} \left( F_{\N}\left( y-\sqrt{P}s
    \right) \right) + (1-D)s \\
    &= \sqrt{D(1-D)} \left( y-\sqrt{P}s \right) + (1-D)s.
  \end{align*}
  This expression still depends on~$s$. If we plug in the optimal distortion
  $D_{\min}$ from~\eqref{eq:exmindist}, however, the decoder becomes
  \begin{align*}
    g(y) &= \frac{\sqrt{P}}{P+1} (y - \sqrt{P}s) + \frac{P}{P +
    1}s \\ 
    &= \frac{\sqrt{P}y}{P + 1},
  \end{align*}
  which no longer depends on~$s$. Furthermore, this decoder is the MMSE decoder.
\end{example}

\subsection{Statisticically Sufficient Decoder}


\subsection{Geometric Interpretation}

If the channel is an AWGN channel, the distortion measure is the squared error,
and the decoder computes the LMMSE estimator, there is a nice geometrical
interpretation for the feedback transmission problem. (The derivation of the
inner product space follows Cramér and
Leadbetter~\cite[Section~5.6]{CramerL1967}.)

\begin{definition}
  An \emph{inner product space} $H$ is a set of elements (points, vectors) $x$,
  $y$, \dots satisfying the following properties.
  \begin{enumerate}
    \item There is an operation of \emph{addition}, assigning to each two
      elements $x, y \in H$ a unique element $z \in H$, denoted $x + y$.
      The unique element $y$ satisfying $x + y = z$ is denoted $y = z - x$. The
      element $0 = x-x$ is unique. 
    \item For every $c \in \R$, the operation of \emph{scalar multiplication}
      maps each $x \in H$ to $cx \in H$. 
    \item To every two elements $x,y \in H$ corresponds a unique scalar $\sp x
      y$ called the \emph{inner product} of $x$ and $y$ with the properties that
      for all $c \in \R$ and $x,y, z\in H$, $\sp{cx+y}{z} = c\sp xz + \sp yz$,
      and $\sp xx \ge 0$, with equality if and only if $x = 0$. The
      \emph{norm} of an element $x \in H$ is defined as $\|x\| = \sqrt{\sp xx}$.
      If $\sp xy = 0$ for some $x$ and $y$, then $x$ and $y$ are called
      \emph{orthogonal}.
  \end{enumerate}
\end{definition}

The following lemma, which can easily be verified, allows us to view the
Gaussian feedback communication problem in a geometric setting.
\begin{lemma}
  \label{lem:rvinprodsp}
  Any set of jointly Gaussian random variables with finite variance forms an
  inner product space under ordinary addition and with $\sp XY = \Cov(X,Y)$. 
\end{lemma}



% [Marius] Disabled the following lemma and proof for now, it doesn't seem to be
% quite solid.
%\begin{lemma}
%  \label{lem:inprodspcond}
%  Given a set of random variables in an inner product space~$H$ as defined
%  above, the space of the random variables conditioned on $X \in H$ is
%  equivalent to the projection of $H$ on the subspace orthogonal to~$X$.
%\end{lemma}
%\begin{proof}
%  Conditioned on~$X$, we have $\sp YZ = \E[YZ|X]$. The covariance between $Y$
%  and~$Z$ given~$X$ is
%  \begin{align*}
%    \Cov(Y, Z \mid X) &= \E[YZ|X] - \E[Y|X]\E[Z|X] \\
%    &= \E[YZ|X] - \frac{\E[XY|X]\E[XZ|X]}{\E[X^2|X]} \\
%    &= \sp YZ - \frac{\sp XY \sp XZ}{\|X\|^2} \\
%    &= \left\bra Y - \frac{\sp XY}{\|X\|^2} X, Z - \frac{\sp XZ}{\|X\|^2} X
%    \right\ket \\
%    &= \sp{Y_{\perp X}}{Z_{\perp X}},
%  \end{align*}
%  where the subscript $\perp X$ denotes the projection onto the subspace
%  orthogonal to~$X$.
%\end{proof}

From the point of view of this inner product space, the communication strategy
of Example~\ref{ex:gaussfb} is illustrated in Figure~\ref{fig:gaussfb1}.
\begin{figure}[tbp]
  \begin{center}
    \input{figures/gaussfb1.tex_t}
  \end{center}
  \caption{Geometrical interpretation of the feedback communication scheme of
  Example~\ref{ex:gaussfb}. The estimate $\Eh_0$ of the first transmission is
  the \emph{projection} of $S = E_0$ onto $Y_1$, hence the estimation error
  $E_1$ is orthogonal to~$Y_1$. Note in particular that $X_2$ is \emph{not}
  independent of $X_1$ (as would be a condition for optimality in the case
  without feedback).}
  \label{fig:gaussfb1}
\end{figure}
The figure shows that while $X_1$ and $X_2$ are not independent, $X_2$ is
independent of $Y_1$. Since all future rounds of communication involve only
linear combinations of~$E_1$ and the noise components $Z_2$, $Z_3$, \dots, all
future communication takes place in a subspace orthogonal to~$Y_1$. This
communication strategy thus produces a projection of $S$ onto the subspace
spanned by the orthogonal vectors $Y_1$, \ldots, $Y_n$.

%Since the estimate $\Eh_i$ is the projection of $E_i$ onto $Y_{i+1}$, 
%\begin{equation*}
%  \Eh_i = \frac{\sp{E_i}{Y_{i+1}}}{\|Y_{i+1}\|^2} Y_{i+1}.
%\end{equation*}
%Writing $Y_{i+1} = \alpha E_i + Z_{i+1}$, with $\alpha = \sqrt{P}/\|E_i\|$,
%yields
%\begin{equation*}
%  \|E_{i+1}\|^2 = \|\Eh_i - E_i\|^2 = \frac{\|E_i\|^2}{1 + P/\|Z_{i+1}\|^2},
%\end{equation*}
%which is nothing else than Equation~\ref{eq:gaussvardec} expressed in the inner
%product space formalism.



\subsection{Other Results}

\subsubsection{Noisy Feedback}

\subsubsection{Minimal-Delay Codes for Arbitrary Rates}


\begin{subappendices}
  \section{Asymptotic Notation}
  \label{app:asymptotic}

  \begin{definition}
    \label{def:bigo}
    Let $f(x)$ and $g(x)$ be two functions defined on~$\R$. The set $O(g(x))$ is
    defined as
    \begin{equation*}
      f(x) \in O(g(x))
    \end{equation*}
    if and only if there exists an $x_0$ and a constant~$c$ such that
    \begin{equation*}
      f(x) \le c g(x)
    \end{equation*}
    for all $x > x_0$. 
    Similarly, $f(x) \in \Omega(g(x))$ if $\le$ is replaced by $\ge$ in
    the above definition. Finally, $\Theta(g(x)) \deq O(g(x)) \cap
    \Omega(g(x))$.
  \end{definition}

\end{subappendices}
