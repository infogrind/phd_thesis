\chapter{Source-Channel Coding with Feedback}
\label{ch:feedback}

In this chapter we will revise the case where the encoder has noiseless and
strictly causal feedback available from the receiver. While feedback does not
increase capacity, it often leads to a great reduction in complexity compared
with the case without feedback. 

If a Gaussian source $S$ of variance~$\ssq$ is to be transmitted across an AWGN
channel with noise variance~$\szq$, where the channel can be used $n$~times per
source symbol, it is well known that the average mean squared error~$D$ and the
average input power~$P$ are related by
\begin{equation}
  \label{eq:shannonlimit1}
  R(D) \le nC(P),
\end{equation}
where $R(D) = 0.5 \log(\ssq/D)$ and $C(P) = 0.5\log(1+P/\szq)$ are the rate
distortion function of the source and the capacity-cost function of the channel,
respectively. The smallest distortion $\Dmin$ for a fixed power is therefore
\begin{equation}
  \label{eq:mindist}
  \Dmin = \frac{\ssq}{(1 + P/\szq)^n}.
\end{equation}
Using separate source and channel codes of sufficiently large blocklengths, one
can achieve any distortion $D > \Dmin$, whether or not feedback is present.
In the case $n=1$, moreover, $\Dmin$ can be achieved exactly by simply
transmitting a scaled version of the source and performing another scaling
operation at the decoder. 

For $n > 1$, a simple feedback strategy, due to Schalkwijk and
Kailath~\cite{SchalkwijkK1966}, is given by the following example.
\textbf{[Schalkwijk\slash Kailath is a scheme for \emph{channel coding}, perhaps
it should be made clear here what the difference to source coding is.]}

\begin{example}
  \label{ex:gaussfb}
  Define $E_0 = S$. In the $i^{\text{th}}$ channel use, transmit
  \begin{equation*}
    X_i = \sqrt{\frac{P}{\Var E_{i-1}}} E_{i-1}.
  \end{equation*}
  Both the receiver and the sender now compute the minimum mean-squared
  error (MMSE) estimator $\Eh_{i-1}$ of $E_{i-1}$ given $Y_i$. The sender
  computes $E_i = \Eh_{i-1} - E_{i-1}$ and proceeds to the next round.

  After $n$~rounds of transmission, the receiver has $n$~estimates $\Eh_0$
  to~$\Eh_{n-1}$. Using these, it computes the final estimate~$\Sh$ as
  \begin{equation}
    \label{eq:shdecom1}
    \Sh = \Eh_0 - \Eh_1 + \Eh_2 - \cdots \pm \Eh_{n-1}.
  \end{equation}
  (The sign of the last term is $+$ if $n$~is even and $-$ if $n$~is odd.)

  To compute the overall distortion $\mse$, note that by definition $\Eh_{i-1} =
  E_{i-1} + E_i$, so \eqref{eq:shdecom1}~can be written as
  \begin{align*}
    \Sh &= E_0 + E_1 - E_1 - E_2 + E_2 + E_3 - \cdots \pm E_{n-1} \pm E_n \\
    &= E_0 \pm E_n,
  \end{align*}
  and thus $\mse = \E[E_n^2]$, where $E_n$ is the remaining error after the last
  round of transmission.

  To compute $\E[E_n^2]$, note that since $\Eh_i$ is the MMSE estimator of
  $E_i$, the estimation error variance is given by (see
  \eg~\cite[Section~8.3]{Scharf1990})
  \begin{equation*}
    \E[(\Eh_i - E_i)^2] = \frac{\E[E_i^2]}{1 + P/\szq}.
  \end{equation*}
  Using $\E[E_0^2] = \E[S^2] = \ssq$ and recursively applying the above, we find
  that
  \begin{equation*}
    \E[E_n^2] = \E[(\Eh_{n-1} - E_{n-1})^2] = \frac{\ssq}{(1 + P/\szq)^n},
  \end{equation*}
  which is indeed the smallest possible distortion according
  to~\eqref{eq:mindist}.
\end{example}

The case of a Gaussian source and channel with squared error distortion measure
and average input power constraint is somewhat special because it naturally
fulfills certain general conditions for optimality, as the following section
shows. 


\section{Optimal Cost/Distortion Tradeoff}
\label{sec:fboptimality}

The goal of this section is to review the conditions for a communication system
to operate at an \emph{optimal cost-distortion tradeoff}. The derivation is a
slightly expanded version of that found in~\cite[Section~3.5]{GastparThesis}.

\begin{theorem}
  \label{thm:feedbackconv}
  Consider a memoryless source $S$ with distortion measure $d(\cdot,\cdot)$ and
  a memoryless channel $p_{Y|X}$ with input cost measure $\rho(\cdot)$. Suppose
  the source is transmitted using a code consisting of an encoder that encodes
  $k$ source symbols into $n$ channel inputs, and a decoder that produces $k$
  source estimates from $n$~channel outputs. Then the average distortion~$D$ and
  the average cost~$P$ are related by
  \begin{equation}
    \label{eq:shannonlimit}
    kR(D) \le nC(P),
  \end{equation}
  with equality if and only if the following five conditions are satisfied.
  \begin{enumerate}
    \item The conditional distribution of the source symbols given the estimates
      can be factored as~$p(s^k|\sh^k) = \prod_{i=1}^k p(s_i|\sh_i)$ and each
      $p(s_i|\sh_i)$ achieves the rate distortion function of the source at the
      same average distortion~$D$. 
    \item The estimates $\Sh^k$ form a sufficient statistic for $S^k$ given the
      outputs $Y^n$. 
    \item The encoder is information lossless.
    \item The channel output sequence $Y^k$ consists of independent random
      variables. 
    \item The marginal distributions $p(x_i)$ of the channel inputs all achieve
      the capacity at the same average cost~$P$.
  \end{enumerate}
\end{theorem}

\begin{proof}
  In the proof we use the notion of \emph{directed
  information}~\cite{Massey1990,Kramer1998}, defined as
  \begin{equation*}
    I(X^n \ra Y^n) = \sum_{i=1}^n I(X^i; Y_i | Y^{i-1}).
  \end{equation*}
  Consider the following series of inequalities.
  \begin{align*}
    kR(D) &\stackrel{(a)}{\le} I(S^k; \Sh^k) \\
    &\stackrel{(b)}{\le} I(S^k; Y^n) \\
    &\stackrel{(c)}{\le} I(X^n \ra Y^n) \\
    &\stackrel{(d)}{\le} \sum_{i=1}^n I(X_i; Y_i) \\
    &\stackrel{(e)}{\le} nC(P).
  \end{align*}
  Inequality~(a) follows from the definition of $R(D)$ and becomes an equality
  if and only if $p(s^k|\sh^k)=\prod_i p(s_i|\sh_i)$ and each $p(s_i|\sh_i)$
  achieves $R(D)$ at the same average distortion. (b) is the data processing
  inequality; it becomes an equality if $S^k - \Sh^k - Y^n$ forms a Markov chain
  or, equivalently, $\Sh^k$ is a sufficient statistic for $S^k$ given~$Y^n$.
  To prove (c), write
  \begin{align*}
    I(S^k; Y^n)
    % &= \sum_{i=1}^n I(Y_i; S^k | Y^{i-1}) \\
    &= \sum_{i=1}^n H(Y_i|Y^{i-1}) - H(Y_i | S^k Y^{i-1}) \\
    &\le \sum_{i=1}^n H(Y_i|Y^{i-1}) - H(Y_i | S^k X^i Y^{i-1}) \\
    &= \sum_{i=1}^n H(Y_i|Y^{i-1}) - H(Y_i | X^i Y^{i-1}) \\
    &= I(X^n \ra Y^n),
  \end{align*}
  where the inequality is because conditioning does not increase the entropy and
  the third equality follows from the Markov chain $(S^k, Y^{i-1}) - X^i - Y_i$.
  The inequality becomes an inequality if the encoder is information lossless in
  the sense that $X^i - (S^k, Y^{i-1}) - Y_i$ forms a Markov chain (see
  Remark~\ref{rem:inflosslessenc} below). Next, (c) is Theorem~2
  in~\cite{Massey1990} and becomes an equality if the $Y_1$, \dots, $Y_n$ are
  mutually independent. Finally (e) follows from the definition of~$C(P)$ and
  becomes an equality if all the $p(x_i)$ achieve capacity at the same~$P$. 
\end{proof}

\begin{remark}
  \label{rem:inflosslessenc}
  According to the above proof, a sufficient condition for an encoder to be
  ``information lossless'' is that is deterministic. While this condition is not
  necessary, any performance achievable using a stochastic encoder is
  also achievable with an alternative deterministic encoder. Indeed, suppose
  $X_i$ is distributed according to $p(x_i | s^k y^{i-1})$ and that $X^i - (S^k,
  Y^{i-1}) - Y_i$ is a Markov chain. Since the distribution of $Y_i$ given
  $(S^k, Y^{i-1})$ is independent of the particular value of $X_i$, $p(x_i | s^k
  y^{i-1})$ may as well be replaced by a distribution with all its mass on a
  particular $x_i$ with $p(x_i|s^k y^{i-1}) > 0$. For all practical purposes,
  therefore, it can be assumed that an optimal system uses a deterministic
  encoder.
%
%  The information losslessness of the encoder can thus also be stated like this:
%  if for a given $(S^k, Y^{i-1})$ there is a set of possible inputs that all
%  have positive probability, then the channel output distribution must be
%  invariant with respect to this set of inputs. 
\end{remark}

Let us now revisit Example~\ref{ex:gaussfb} and see why it achieves the optimal
distortion. We will go through the conditions of Theorem~\ref{thm:feedbackconv}
in reverse order, starting at condition~5. Since the source and the noise are
jointly Gaussian and the encoder and decoder are linear, all channel inputs
$X_i$ are Gaussian; since they are scaled to have variance~$P$ they all achieve
the capacity at the same average cost. Next, because the estimation error of an
MMSE estimator is uncorrelated with the observation and because in the Gaussian
case uncorrelated implies independence, $E_i$ is independent of $Y_{i-1}$ and
thus so are~$X_i$ and~$Y_i$, satisfying condition~4. Condition~3 is satisfied
because the encoder is deterministic. For conditions~1 and~2, observe that the
final estimate $\Sh$ is such that $S = \Sh + E_n$, where $E_n$ is independent of
$\Sh$ and of $Y^n$. Given $\Sh$, $S$ is therefore independent of~$Y^n$, which
makes $\Sh$ a sufficient statistic, satisfying condition~2. Moreover, the
relationship between $S$ and $\Sh$ is exactly the one leading to the
distribution that achieves the rate distortion function (see
\eg~\cite{CoverT1991}), fulfilling condition~1.

It appears as though this example works only because of the particular
properties of the Gaussian distribution: preservation of distribution under
linear transformation, linear MMSE decoder, equivalence of uncorrelatedness and
independence, and so on. One would assume, therefore, that this example does not
give any indication about how to use the feedback for general sources and
channels. As the next section shows, though, conditions~3--5 of
Theorem~\ref{thm:feedbackconv} can be achieved with minimal complexity for any
channel if perfect feedback is available.


\section{Posterior Matching}

We have seen in Example~\ref{ex:gaussfb} how a simple transmission scheme can
achieve the optimal distortion using noiseless feedback. In
Section~\ref{sec:fboptimality} we saw how the particular properties of the
Gaussian distribution helped in achieving this. The present section shows that
conditions~3 to~5 of Theorem~\ref{thm:feedbackconv} can be satisfied for
arbitrary channels. The underlying idea, called \emph{posterior matching}, was
used by Shayevitz and Feder in~2007~\cite{ShayevitzF2007,ShayevitzF2008} to
generalize the capacity achieving channel coding schemes of Schalkwijk and
Kailath~\cite{SchalkwijkK1966} and Horstein~\cite{Horstein1963} to arbitrary
channels with feedback.


\subsection{Achieving Channel Capacity}

Consider a channel $p(y|x)$ and let $\pi(x)$ be the capacity achieving
distribution at average cost~$P$, \ie, 
\begin{equation*}
  \pi(x) = \arg\max_{p(x): \E[\rho(X)] \le P} I(X;Y).
\end{equation*}
As before, we assume perfect feedback. The problem is to encode one source
symbol of an \emph{analog} source into~$n$ channel inputs. 

Let $\Fpi$ be the cumulative distribution function (\cdf) of the distribution
$\pi(x)$, and let $F_S(\x)$ be the \cdf\ of the source. In the first channel
use, the encoder produces
\begin{equation*}
  X_1 = \Fpi^{-1}(F_S(S)),
\end{equation*}
where $\Fpi^{-1}$ is the inverse function of $\Fpi$. The random variable
$F_S(S)$ has uniform distribution on $[0,1]$, and the application of $\Fpi^{-1}$
to a uniform random variable produces a random variable with \cdf\ $\Fpi$. Thus,
$X_1$ has the capacity achieving distribution~$\pi(x)$. 

After $i-1$ rounds of transmission, the encoder knows $y_1$, \ldots, $y_{i-1}$,
the values of the $i-1$ past received symbols, and can compute the conditional
\cdf\ $F_{S|y_1, \ldots, y_{i-1}}$. It then sends in the $i^{\text{th}}$ channel
use
\begin{equation}
  \label{eq:posteriorxi}
  X_i = \Fpi^{-1}(F_{S|y_1,\dots,y_{i-1}}(S)).
\end{equation}
For any particular received values $y_1$, \ldots, $y_{i-1}$, therefore,
\begin{equation*}
  p(x_i|y_1, \dots, y_{i-1}) = \pi(x)
\end{equation*}
and so $X_i$ is independent of $Y_1$, \ldots, $Y_{i-1}$. 

Using this strategy the encoder produces an \iid\ sequence of inputs $X_i$
with the capacity achieving distribution~$\pi(x)$, satisfying conditions~4 and~5
of Theorem~\ref{thm:feedbackconv}; condition~3 of the theorem is satisfied
trivially because the encoder is deterministic.


\subsection{Achieving the Rate Distortion Function?}

The previous section showed how, having knowledge of the past channel outputs,
the decoder could deterministically produce a new channel input that was
independent of the previous outputs and had the capacity achieving distribution.
As was explained in Section~\ref{sec:fboptimality}, a further necessary
condition for a system to be optimal in the sense of equality
in~\eqref{eq:shannonlimit} is
that the joint distribution of $S$ and $\Sh$ achieves the rate distortion
function. Is this possible using a similar distribution matching strategy?
Unfortunately it turns out that the answer is no in general; the Gaussian
distribution is a notable exception.

The rate distortion function is achieved if $p(\sh|s)$ minimizes the mutual
information $I(S;\Sh)$ under the average distortion constraint. It is therefore
appealing to apply a similar transformation at the channel output as done at the
channel input for the capacity case, in order to make the conditional
distribution of $\Sh$ given $S=s$ equal to the minimizing distribution
$\Phi_s(\sh)$. The resulting decoder would produce
\begin{equation}
  \label{eq:distmatchdec}
  \sh = g(y) = F_{\Phi_s}^{-1}(F_{Y|S=s}(y)).
\end{equation}
First, $y$ is transformed to be uniformly distributed given $S=s$, and then the
inverse \cdf\ of the desired conditional distribution is applied. It is
immediately clear, however, that such an approach cannot work -- both \cdf s
needed to implement this decoder depend on the actual value of~$s$, which
naturally is not known at the decoder. The notable fact about the Gaussian
case is that while both $F_{\Phi_s}$ and $F_{Y|S=s}$ individually depend on~$s$,
their concatenation $F_{\Phi_s} \circ F_{Y|S=s}$ does not, as the following
example shows. 

\begin{example}
  Let the source $S$ be distributed as $\N(0,1)$ and let the channel be AWGN
  with noise variance~$\szq$ and input constraint $\sum_i \E[X_i^2]/n \le P$.
  The distortion is the squared error. If a single channel use is allowed,  the
  smallest achievable distortion is
  \begin{equation}
    \label{eq:exmindist}
    D_{\min} = \frac{1}{1 + P/\szq}.
  \end{equation}
\end{example}
The capacity-achieving input distribution is $\N(0,P)$ and the conditional
distribution of $\Sh$ given $S=s$ that achieves the rate distortion function at
distortion~$D$ is $\N((1-D)s, D(1-D))$.

The encoder that makes $X$ capacity achieving is therefore~$X = \sqrt{P}S$. The
decoder from~\eqref{eq:distmatchdec} becomes
\begin{align*}
  g(y) &= F_{\Phi_s}^{-1} \circ F_{Y|S=s}(y) \\
  &= \sqrt{D(1-D)} F_{\N}^{-1} \circ F_{\N}\left( \frac{y-\sqrt{P}s}{\sz}
  \right) + (1-D)s \\
  &= \sqrt{D(1-D)} \left( \frac{y-\sqrt{P}s}{\sz} \right) + (1-D)s,
\end{align*}
where we used $F_{\N}$ to denote the \cdf\ of a $\N(0,1)$ distribution. Now this
expression still depends on~$s$; if we plug in the optimal distortion $D_{\min}$
from~\eqref{eq:exmindist}, however, the decoder becomes
\begin{align*}
  g(y) &= \frac{\sz \sqrt{P}}{P+\szq} \frac{y - \sqrt{P}s}{\sz} + \frac{P}{P +
  \szq}s \\ 
  &= \frac{\sqrt{P}y}{P + \szq},
\end{align*}
which no longer depends on~$s$. Furthermore, this decoder is the MMSE decoder.




\section{Geometric Interpretation}

This section is about the interpretation of random variables as a vector space,
and how this connects to the feedback problem. For this section we should also
look again at the book by Cramer and Leadbetter, ``Stationary and related
stochastic processes'', Ch.~5, Sections~5.6 and~5.7 as recommended by Burnashev.

Depending on what this section will contain exactly, it may be moved after the
next section.


\section{The Case $k$:$n$}

Some thoughts about the $k$:$n$ case as treated in my technical report.


\section{Noisy Feedback}

Possibly a section about the case with noisy feedback for the Gaussian case. We
could give the example that if the feedback noise vs.\ channel noise ratio is
constant then the distortion scales only as $\snr^{-1}$.


\section{Towards the Feedbackless Case}

Example~\ref{ex:gaussfb} showed that using feedback, the theoretically optimal
distortion when one source symbol is transmitted in $n$~channel uses can be
achieved exactly with a simple transmission scheme that encodes a single source
symbol at a time. The goal of the present section is to draw insights from this
example that can help in the case without feedback. 

Recall Example~\ref{ex:gaussfb}. In the first round the source was
transmitted uncoded and the receiver computed the LMMSE estimator~$\Eh_0$. In
each subsequent round, the error $E_i = \Eh_{i-1} - E_{i-1}$ was transmitted
uncoded and again estimated at the receiver using LMMSE. Let us now express the
source as
\begin{equation*}
  S = S + E_1 - E_1 - E_2 + E_2 + \cdots \pm E_{n-1} \mp E_{n-1}.
\end{equation*}
Using the definition of $E_i$ repeatedly, this can be written as
\begin{equation*}
  S = \Eh_0 - \Eh_1 + \Eh_2 - \cdots \pm \Eh_{n-1} \mp E_{n-1}.
\end{equation*}
This transmission scheme therefore implicitly decomposes $S$ into $n$
components. Moreover, the first $n-1$ components are known exactly by the
decoder, so the only error is due to the last transmission.

In the absence of feedback, a possible strategy is therefore to do the
following:
\begin{enumerate}
  \item Break up the source into the sum of $n$ components.
  \item Transmit these components such that the first $n-1$ components are
    decoded practically error free and transmit the last component uncoded.
\end{enumerate}
Furthermore, the $n$ components should be (approximately) independent.

The investigation and analysis of strategies based on this principle is the
subject of the next chapter. 

