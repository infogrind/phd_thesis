\chapter{Source-Channel Coding with Feedback}
\label{ch:feedback}

In this chapter we will revise the case where the encoder has noiseless and
strictly causal feedback available from the receiver. While feedback does not
increase capacity, it often leads to a great reduction in complexity compared
with the case without feedback. 


\section{A First Example}

If a Gaussian source $S$ of variance~$\ssq$ is to be transmitted across an AWGN
channel with noise variance~$\szq$, where the channel can be used $n$~times per
source symbol, it is well known that the average mean squared error~$D$ and the
average input power~$P$ are related by
\begin{equation}
  \label{eq:shannonlimit1}
  R(D) \le nC(P),
\end{equation}
where $R(D) = 0.5 \log(\ssq/D)$ and $C(P) = 0.5\log(1+P/\szq)$ are the rate
distortion function of the source and the capacity-cost function of the channel,
respectively. The smallest distortion $\Dmin$ for a fixed power is therefore
\begin{equation}
  \label{eq:mindist}
  \Dmin = \frac{\ssq}{(1 + P/\szq)^n}.
\end{equation}
Using separate source and channel codes of sufficiently large blocklengths, one
can achieve any distortion $D > \Dmin$, whether or not feedback is present.
In the case $n=1$, moreover, $\Dmin$ can be achieved exactly by simply
transmitting a scaled version of the source and performing another scaling
operation at the decoder. 

For $n > 1$, a simple feedback strategy, due to Schalkwijk and
Kailath~\cite{SchalkwijkK1966}, is given by the following example.
\textbf{[Schalkwijk\slash Kailath is a scheme for \emph{channel coding}, perhaps
it should be made clear here what the difference to source coding is.]}

\begin{example}
  \label{ex:gaussfb}
  Define $E_0 = S$. In the $i^{\text{th}}$ channel use, transmit
  \begin{equation}
    \label{eq:gaussfbxi}
    X_i = \sqrt{\frac{P}{\Var E_{i-1}}} E_{i-1}.
  \end{equation}
  Both the receiver and the sender now compute the minimum mean-squared
  error (MMSE) estimator $\Eh_{i-1}$ of $E_{i-1}$ given $Y_i$. The sender
  computes $E_i = \Eh_{i-1} - E_{i-1}$ and proceeds to the next round.

  After $n$~rounds of transmission, the receiver has $n$~estimates $\Eh_0$
  to~$\Eh_{n-1}$. Using these, it computes the final estimate~$\Sh$ as
  \begin{equation}
    \label{eq:shdecom1}
    \Sh = \Eh_0 - \Eh_1 + \Eh_2 - \cdots \pm \Eh_{n-1}.
  \end{equation}
  (The sign of the last term is $+$ if $n$~is even and $-$ if $n$~is odd.)

  To compute the overall distortion $\mse$, note that by definition $\Eh_{i-1} =
  E_{i-1} + E_i$, so \eqref{eq:shdecom1}~can be written as
  \begin{align*}
    \Sh &= E_0 + E_1 - E_1 - E_2 + E_2 + E_3 - \cdots \pm E_{n-1} \pm E_n \\
    &= E_0 \pm E_n,
  \end{align*}
  and since we have defined $E_0 = S$, we have $\mse = \E[E_n^2]$, where $E_n$
  is the remaining error after the last round of transmission.

  To compute $\E[E_n^2]$, note that since $\Eh_i$ is the MMSE estimator of
  $E_i$, the estimation error variance is given by (see
  \eg~\cite[Section~8.3]{Scharf1990})
  \begin{equation}
    \label{eq:gaussvardec}
    \E[(\Eh_i - E_i)^2] = \frac{\E[E_i^2]}{1 + P/\szq}.
  \end{equation}
  Using $\E[E_0^2] = \E[S^2] = \ssq$ and recursively applying the above, we find
  that
  \begin{equation*}
    \E[E_n^2] = \E[(\Eh_{n-1} - E_{n-1})^2] = \frac{\ssq}{(1 + P/\szq)^n},
  \end{equation*}
  which is indeed the smallest possible distortion according
  to~\eqref{eq:mindist}.
\end{example}

\begin{remark}
  \label{rem:snrscalingfeedback}
  The resulting distortion in Example~\ref{ex:gaussfb} does not depend on the
  distribution of the source. The transmission strategy therefore achieves a
  distortion of $\ssq/(1+P/\szq)^n$ for any source. 

  For an arbitrary analog source with variance~$\ssq$ and differential entropy
  $h(S) < \infty$, the rate distortion function under mean squared error
  distortion can be bounded as~\cite{Shannon1959}
  \begin{equation*}
    h(S) - \frac12 \log(2\pi e D) \le R(D).
  \end{equation*}
  Applying this bound to the inequality $R(D) \le nC(P)$, the mean squared error
  is therefore lower bounded by
  \begin{equation*}
    D \ge \frac{2^{2h(S)}}{2\pi e} (1 + P/\szq)^{-n},
  \end{equation*}
  where $2^{2h(S)}/2\pi e \le \ssq$, and so the strategy of
  Example~\ref{ex:gaussfb} yields a distortion that is at most a constant
  multiple of the theoretically minimal distortion.
\end{remark}

The case of a Gaussian source and channel with squared error distortion measure
and average input power constraint is somewhat special because it naturally
fulfills certain general conditions for optimality, as the following section
shows. 


\section{Optimal Cost/Distortion Tradeoff}
\label{sec:fboptimality}

The goal of this section is to review the conditions for a communication system
to operate at an \emph{optimal cost-distortion tradeoff}. The derivation is
essentially that found in~\cite[Section~3.5]{GastparThesis}, with a slightly
more expanded discussion on the ``information losslessness'' of the code.

\begin{theorem}
  \label{thm:feedbackconv}
  Consider a memoryless source $S$ with distortion measure $d(\cdot,\cdot)$ and
  a memoryless channel $p_{Y|X}$ with input cost measure $\rho(\cdot)$. Suppose
  the source is transmitted using a code consisting of an encoder that encodes
  $k$ source symbols into $n$ channel inputs, and a decoder that produces $k$
  source estimates from $n$~channel outputs. Then the average distortion~$D$ and
  the average cost~$P$ are related by
  \begin{equation}
    \label{eq:shannonlimit}
    kR(D) \le nC(P),
  \end{equation}
  with equality if and only if the following five conditions are satisfied.
  \begin{enumerate}
    \item The conditional distribution of the source symbols given the estimates
      can be factored as~$p(s^k|\sh^k) = \prod_{i=1}^k p(s_i|\sh_i)$ and each
      $p(s_i|\sh_i)$ achieves the rate distortion function of the source at the
      same average distortion~$D$. 
    \item The estimates $\Sh^k$ form a sufficient statistic for $S^k$ given the
      outputs $Y^n$. 
    \item The encoder is information lossless.
    \item The channel output sequence $Y^k$ consists of mutually independent
      random variables. 
    \item The marginal distributions $p(x_i)$ of the channel inputs all achieve
      the capacity at the same average cost~$P$.
  \end{enumerate}
\end{theorem}

\begin{proof}
  In the proof we use the notion of \emph{directed
  information}~\cite{Massey1990,Kramer1998}, defined as
  \begin{equation*}
    I(X^n \ra Y^n) = \sum_{i=1}^n I(X^i; Y_i | Y^{i-1}).
  \end{equation*}
  Consider the following series of inequalities.
  \begin{align*}
    kR(D) &\stackrel{(a)}{\le} I(S^k; \Sh^k) \\
    &\stackrel{(b)}{\le} I(S^k; Y^n) \\
    &\stackrel{(c)}{\le} I(X^n \ra Y^n) \\
    &\stackrel{(d)}{\le} \sum_{i=1}^n I(X_i; Y_i) \\
    &\stackrel{(e)}{\le} nC(P).
  \end{align*}
  Inequality~(a) follows from the definition of $R(D)$ and becomes an equality
  if and only if Condition~1 of the theorem is satisfied. (b) is the data
  processing inequality; it becomes an equality if $S^k - \Sh^k - Y^n$ forms a
  Markov chain, which is another way of saying that $\Sh^k$ is a sufficient
  statistic for $S^k$ given~$Y^n$.  Inequality~(c) is the equivalent of the data
  processing inequality for channels with feedback. One way to prove it is as
  follows.
  \begin{align*}
    I(S^k; Y^n)
    % &= \sum_{i=1}^n I(Y_i; S^k | Y^{i-1}) \\
    &= \sum_{i=1}^n I(S^k; Y_i | Y^{i-1}) \\
    &\le \sum_{i=1}^n I(S^k; Y_i | Y^{i-1}) + I(X^i;Y_i | S^k Y^{i-1}) \\
    &= \sum_{i=1}^n I(S^k X^i; Y_i | Y^{i-1}) \\
    &= \sum_{i=1}^n I(X^i; Y_i | Y^{i-1}) + I(S^k; Y_i | X^i Y^{i-1}) \\
    &= \sum_{i=1}^n I(X^i; Y_i | Y^{i-1}) \\
    &= I(X^n \ra Y^n),
  \end{align*}
  where the fifth line is because the channel is memoryless.
  The inequality becomes an inequality if the encoder is information lossless in
  the sense that $X^i - (S^k, Y^{i-1}) - Y_i$ forms a Markov chain (see
  Remark~\ref{rem:inflosslessenc} below). Next, (d) is Theorem~2
  in~\cite{Massey1990} and becomes an equality if the $Y_1$, \dots, $Y_n$ are
  mutually independent. Finally (e) follows from the definition of~$C(P)$ and
  becomes an equality if all the $p(x_i)$ achieve capacity at the same~$P$. 
\end{proof}

\begin{remark}
  \label{rem:inflosslessenc}
  According to the above proof, a sufficient condition for an encoder to be
  ``information lossless'' is that is deterministic. While this condition is not
  necessary, any performance achievable using a stochastic encoder is
  also achievable with an alternative deterministic encoder. Indeed, suppose
  $X_i$ is distributed according to $p(x_i | s^k y^{i-1})$ and that $X^i - (S^k,
  Y^{i-1}) - Y_i$ is a Markov chain. Since the distribution of $Y_i$ given
  $(S^k, Y^{i-1})$ is independent of the particular value of $X_i$, $p(x_i | s^k
  y^{i-1})$ may as well be replaced by a distribution with all its mass on a
  particular $x_i$ with $p(x_i|s^k y^{i-1}) > 0$. For all practical purposes,
  therefore, it can be assumed that an optimal system uses a deterministic
  encoder.
%
%  The information losslessness of the encoder can thus also be stated like this:
%  if for a given $(S^k, Y^{i-1})$ there is a set of possible inputs that all
%  have positive probability, then the channel output distribution must be
%  invariant with respect to this set of inputs. 
\end{remark}

Let us now revisit Example~\ref{ex:gaussfb} and see why it achieves the optimal
distortion. We will go through the conditions of Theorem~\ref{thm:feedbackconv}
in reverse order, starting at condition~5. Since the source and the noise are
jointly Gaussian and the encoder and decoder are linear, all channel inputs
$X_i$ are Gaussian; since they are scaled to have variance~$P$ they all achieve
the capacity at the same average cost. Next, because the estimation error of an
MMSE estimator is uncorrelated with the observation and because in the Gaussian
case uncorrelated implies independence, $E_i$ is independent of $Y_{i-1}$ and
thus so are~$X_i$ and~$Y_i$, satisfying condition~4. Condition~3 is satisfied
because the encoder is deterministic. For conditions~1 and~2, observe that the
final estimate $\Sh$ is such that $S = \Sh + E_n$, where $E_n$ is independent of
$\Sh$ and of $Y^n$. Given $\Sh$, $S$ is therefore independent of~$Y^n$, which
makes $\Sh$ a sufficient statistic, satisfying condition~2. Moreover, the
relationship between $S$ and $\Sh$ is exactly the one leading to the
distribution that achieves the rate distortion function (see
\eg~\cite{CoverT1991}), fulfilling condition~1.

It appears as though this example works only because of the particular
properties of the Gaussian distribution: preservation of distribution under
linear transformation, linear MMSE decoder, equivalence of uncorrelatedness and
independence, and so on. One would assume, therefore, that this example does not
give any indication about how to use the feedback for general sources and
channels. As the next section shows, though, conditions~3--5 of
Theorem~\ref{thm:feedbackconv} can be achieved with minimal complexity for any
channel if perfect feedback is available.


\section{Posterior Matching}

We have seen in Example~\ref{ex:gaussfb} how a simple transmission scheme can
achieve the optimal distortion using noiseless feedback. In
Section~\ref{sec:fboptimality} we saw how the particular properties of the
Gaussian distribution helped in achieving this. The present section shows that
conditions~3 to~5 of Theorem~\ref{thm:feedbackconv} can be satisfied for
arbitrary channels. The underlying idea, called \emph{posterior matching}, was
used by Shayevitz and Feder in~2007~\cite{ShayevitzF2007,ShayevitzF2008} to
generalize the capacity achieving channel coding schemes of Schalkwijk and
Kailath~\cite{SchalkwijkK1966} and Horstein~\cite{Horstein1963} to arbitrary
channels with feedback.

Before continuing we prove some properties of cumulative distribution functions
(\cdf s).

\begin{lemma}
  \label{lem:cdfunif}
  Let $X$ be a continuous random variable with density $f(x)$ and \cdf\ $F_X$,
  \ie,
  \begin{equation*}
    F_X(x) = \Pr[X \le x].
  \end{equation*}
  Then the random variable $F_X(X)$ is uniformly distributed on~$[0,1]$.
\end{lemma}

\begin{proof}
  Let $Y = F_X(X)$. Then
  \begin{align*}
    \Pr[Y \le y] &= \Pr[F_X(X) \le y] \\
    &= \Pr[X \le F_X^{-1}(y)] \\
    &= \int_{-\infty}^{F_X^{-1}(y)} f(x) dx \\
    &= F_X(F_X^{-1}(y)) = y,
  \end{align*}
  which is the \cdf\ of a uniform random variable on $[0,1]$.
\end{proof}


\begin{lemma}
  \label{lem:invcdf}
  Let $U$ be a uniform random variable on~$[0,1]$ and let $F$ be the \cdf\ of an
  arbitrary random variable~$X$. Let $F^{-1}$ be the ``inverse'' of~$F$, in the
  sense
  \begin{equation}
    \label{eq:invcdf}
    F^{-1}(y) = \sup \{x : F(x) \le y\}.
  \end{equation}
  Then the random variable $F^{-1}(U)$ has the same distribution as~$X$.
\end{lemma}

\begin{proof}
  The definition of $F^{-1}$ according to~\eqref{eq:invcdf} has the property
  that $\Pr[F^{-1}(U) \le x] = \Pr[U \le F(x)]$. Thus,
  \begin{align*}
    \Pr[F^{-1}(U) \le x] &= \Pr[U \le F(x)] \\
    &= \int_0^{F(x)} d\xi = F(x).
  \end{align*}
\end{proof}

\subsection{Achieving Channel Capacity}

Consider a channel $p(y|x)$ and let $\pi(x)$ be the capacity achieving
distribution at average cost~$P$, \ie, 
\begin{equation*}
  \pi(x) = \arg\max_{p(x): \E[\rho(X)] \le P} I(X;Y).
\end{equation*}
As before, we assume perfect feedback. The problem is to encode one source
symbol of an \emph{analog} source into~$n$ channel inputs. 

Let $\Fpi$ be the cumulative distribution function (\cdf) of the distribution
$\pi(x)$, and let $F_S$ be the \cdf\ of the source. In the first channel
use, the encoder produces
\begin{equation}
  \label{eq:posteriorx1}
  X_1 = \Fpi^{-1}(F_S(S)),
\end{equation}
where $\Fpi^{-1}$ is the inverse of $\Fpi$ according to~\eqref{eq:invcdf}. By
Lemma~\ref{lem:cdfunif}, $F_S(S)$ has uniform distribution on $[0,1]$, and so by
Lemma~\ref{lem:invcdf}, $\Fpi^{-1}(F_S(S))$ to a uniform random variable
produces a random variable with \cdf\ $\Fpi$. Thus, $X_1$ has the capacity
achieving distribution~$\pi(x)$. 

After $i-1$ rounds of transmission, the encoder knows $y_1$, \ldots, $y_{i-1}$,
the values of the $i-1$ past received symbols, and can compute the conditional
\cdf\ $F_{S|y_1, \ldots, y_{i-1}}$. It then sends in the $i^{\text{th}}$ channel
use
\begin{equation}
  \label{eq:posteriorxi}
  X_i = \Fpi^{-1}(F_{S|y_1,\dots,y_{i-1}}(S)).
\end{equation}
For any particular received values $y_1$, \ldots, $y_{i-1}$, therefore,
\begin{equation*}
  p(x_i|s, y_1, \dots, y_{i-1}) = \pi(x)
\end{equation*}
and so $X_i$ is independent of $Y_1$, \ldots, $Y_{i-1}$. 

Using this strategy the encoder produces an \iid\ sequence of inputs $X_i$
with the capacity achieving distribution~$\pi(x)$, satisfying conditions~4 and~5
of Theorem~\ref{thm:feedbackconv}; condition~3 of the theorem is satisfied
trivially because the encoder is deterministic.

Let us look at Example~\ref{ex:gaussfb} once more, now from the perspective of
posterior matching.

\begin{example}
  \label{ex:gaussfbpost}
  Let $F_{\N(\mu, \sq)}$ be the \cdf\ of a Gaussian random variable of
  mean~$\mu$ and variance~$\sq$ and let $F_\N \deq F_{\N(0,1)}$. Then
  $F_{\N(\mu,\sq)}(x) = F_\N((x-\mu)/\sigma)$. Furthermore, the inverse \cdf\ is
  given by
  \begin{equation*}
    F_{\N(\mu,\sq)}^{-1}(y) = \sigma F_\N^{-1}(y) + \mu.
  \end{equation*}

  Since all involved random variables are jointly Gaussian, the source~$S$ can
  be written as
  \begin{equation*}
    S = \gamma_1 Y_1 + \dots + \gamma_{i-1} Y_{i-1} + W,
  \end{equation*}
  where $W$ is a Gaussian random variable of zero mean and variance $\swq$,
  independent of $Y_1$, \ldots, $Y_{i-1}$. Hence the distribution of~$S$ given
  $Y_1$, \ldots, $Y_{i-1}$ is Gaussian with mean $\E[S|Y_1, \dots, Y_{i-1}] =
  \gamma_1 Y_1 + \dots + \gamma_{i-1} Y_{i-1}$ and variance~$\swq$.  The
  posterior matching encoder~\eqref{eq:posteriorxi} therefore evaluates to
  \begin{align}
    X_i &= \sqrt{P} F_\N^{-1}\left( F_\N\left( \frac{S - \E[S|Y_1^{i-1}]}
    {\sqrt{\Var(S - \E[S|Y_1^{i-1}])}} \right) \right) \nonumber \\
    \label{eq:gausspmenc}
    &= \sqrt{P} \frac{S - \E[S|Y_1^{i-1}]}
    {\sqrt{\Var(S - \E[S|Y_1^{i-1}])}} .
  \end{align}
  Write now
  \begin{align*}
    S &= E_0 + (E_1 - E_1) - (E_2 - E_2) + \dots \pm (E_{i-2} -
    E_{i-2}) \\
    &= (E_0 + E_1) - (E_1 + E_2) + (E_2 + E_3) - \dots - E_{i-2} \\
    &= \Eh_0 - \Eh_1 + \Eh_2 - \dots - E_{i-2}.
  \end{align*}
  Since $\E[\Eh_j | Y_1^{i-1}] = \Eh_j$ for $j = 1$, \dots,~$i-2$, and
  $\E[E_{i-2}|Y_1^{i-1}] = \Eh_{i-2}$, 
  \begin{equation*}
    \E[S|Y_1^{i-1}] = \Eh_0 - \Eh_1 + \dots  - \Eh_{i-2}
  \end{equation*}
  and so $S - \E[S|Y_1^{i-1}] = \Eh_{i-2} - E_{i-2} = E_{i-1}$. Plugging this
  into~\eqref{eq:gausspmenc} yields exactly the encoder~\eqref{eq:gaussfbxi} of
  Example~\ref{ex:gaussfb}.
\end{example}


\subsection{Achieving the Rate Distortion Function?}

The previous section showed how, having knowledge of the past channel outputs,
the decoder could deterministically produce a new channel input that was
independent of the previous outputs and had the capacity achieving distribution.
As was explained in Section~\ref{sec:fboptimality}, a further necessary
condition for a system to be optimal in the sense of equality
in~\eqref{eq:shannonlimit} is
that the conditional distribution of $\Sh$ given $S$ achieves the rate
distortion function of the source. Is this possible using a similar distribution
matching strategy?  Unfortunately it turns out that the answer is no in general;
the Gaussian distribution is a notable exception.

It is appealing to apply a similar transformation at the channel output as done
at the channel input for the capacity case, in order to make the conditional
distribution of $\Sh$ given $S=s$ equal to the distribution
$\Phi_s(\sh)$ that achieves the rate distortion function. Assume the source
symbol~$S$ is transmitted across a single channel use. Let the decoder be given
by
\begin{equation}
  \label{eq:distmatchdec}
  \Sh = g(Y) = F_{\Phi_s}^{-1}(F_{Y|S=s}(Y)).
\end{equation}
For any~$s$, therefore, $\Sh$ given~$s$ is distributed according to~$\Phi_s$,
and so the rate distortion function is achieved.

It is immediately clear, however, that this approach cannot work -- both \cdf s
needed to implement this decoder depend on the actual value of~$s$, which is
obviously not known at the decoder (there would not really be a communication
problem otherise). Interestingly, though, in the Gaussian case the dependence
on~$s$ of $F_{\Phi_s}$ and of $F_{Y|S=s}$ cancel each other out, and the
decoder~\eqref{eq:distmatchdec} yields again the MMSE decoder, as the following
example shows.

\begin{example}
  Let the source $S$ be distributed as $\N(0,1)$ and let the channel be AWGN
  with noise variance~$1$ and input constraint $\E[X^2] \le P$.
  The distortion is the squared error. The smallest achievable distortion is
  \begin{equation}
    \label{eq:exmindist}
    D_{\min} = \frac{1}{1 + P}.
  \end{equation}
  The capacity-achieving input distribution is $\N(0,P)$ and the conditional
  distribution of $\Sh$ given $S=s$ that achieves the rate distortion function
  at distortion~$D$ is $\N((1-D)s, D(1-D))$.
  
  The encoder that makes $X$ capacity achieving is therefore~$X = \sqrt{P}S$.
  The decoder from~\eqref{eq:distmatchdec} is
  \begin{align*}
    g(y) &= F_{\Phi_s}^{-1} \circ F_{Y|S=s}(y) \\
    &= \sqrt{D(1-D)} F_{\N}^{-1} \circ F_{\N}\left( y-\sqrt{P}s
    \right) + (1-D)s \\
    &= \sqrt{D(1-D)} \left( y-\sqrt{P}s \right) + (1-D)s,
  \end{align*}
  where we used $F_{\N}$ to denote the \cdf\ of a $\N(0,1)$ distribution.
  This expression still depends on~$s$; if we plug in the optimal distortion
  $D_{\min}$ from~\eqref{eq:exmindist}, however, the decoder becomes
  \begin{align*}
    g(y) &= \frac{\sqrt{P}}{P+1} (y - \sqrt{P}s) + \frac{P}{P +
    1}s \\ 
    &= \frac{\sqrt{P}y}{P + 1},
  \end{align*}
  which no longer depends on~$s$. Furthermore, this decoder is the MMSE decoder.
\end{example}


\begin{remark}
  \label{rem:increasingdec}
  Suppose a communication system achieves $R(D) = C(P)$ with a single letter
  encoder and decoder. One might think that the encoder and the decoder can
  always be written in the form~\eqref{eq:posteriorx1}
  and~\eqref{eq:distmatchdec}, respectively. This is not the case, as a simple
  argument shows. Note that both~\eqref{eq:posteriorx1}
  and~\eqref{eq:distmatchdec} are increasing functions. Using the results
  of~\cite{GastparThesis}, on the other hand, one can select arbitrary encoding
  and decoding functions that are not increasing and then choose the cost and
  distortion measures such that $R(D) = C(P)$.
\end{remark}


\section{Gaussian Case: Geometrical Interpretation}

If the channel is an AWGN channel, the distortion measure is the squared error,
and the decoder computes the LMMSE estimator, there is a nice geometrical
interpretation for the feedback transmission problem. (The derivation of the
inner product space follows Cramér and
Leadbetter~\cite[Section~5.6]{CramerL1967}.)

\begin{definition}
  An \emph{inner product space} $H$ is a set of elements (points, vectors) $x$,
  $y$, \dots satisfying the following properties.
  \begin{enumerate}
    \item There is an operation of \emph{addition}, assigning to each two
      elements $x, y \in H$ a unique element $z \in H$, denoted $x + y$.
      The unique element $y$ satisfying $x + y = z$ is denoted $y = z - x$. The
      element $0 = x-x$ is unique. 
    \item For every $c \in \R$, the operation of \emph{scalar multiplication}
      maps each $x \in H$ to $cx \in H$. 
    \item To every two elements $x,y \in H$ corresponds a unique scalar $\sp x
      y$ called the \emph{inner product} of $x$ and $y$ with the properties that
      for all $c \in \R$ and $x,y, z\in H$, $\sp{cx+y}{z} = c\sp xz + \sp yz$,
      and $\sp xx \ge 0$, with equality if and only if $x = 0$. The
      \emph{norm} of an element $x \in H$ is defined as $\|x\| = \sqrt{\sp xx}$.
      If $\sp xy = 0$ for some $x$ and $y$, then $x$ and $y$ are called
      \emph{orthogonal}.
  \end{enumerate}
\end{definition}

\begin{lemma}
  \label{lem:rvinprodsp}
  The set of zero-mean random variables with finite variance forms an inner
  product space under ordinary addition and with $\sp XY = \E[X Y]$. 
\end{lemma}

\begin{lemma}
  \label{lem:inprodspcond}
  Given a set of random variables in an inner product space~$H$ as defined
  above, the space of the random variables conditioned on $X \in H$ is
  equivalent to the projection of $H$ on the subspace orthogonal to~$X$.
\end{lemma}
\begin{proof}
  Conditioned on~$X$, we have $\sp YZ = \E[YZ|X]$. The covariance between $Y$
  and~$Z$ given~$X$ is
  \begin{align*}
    \Cov(Y, Z \mid X) &= \E[YZ|X] - \E[Y|X]\E[Z|X] \\
    &= \E[YZ|X] - \frac{\E[XY|X]\E[XZ|X]}{\E[X^2|X]} \\
    &= \sp YZ - \frac{\sp XY \sp XZ}{\|X\|^2} \\
    &= \left\bra Y - \frac{\sp XY}{\|X\|^2} X, Z - \frac{\sp XZ}{\|X\|^2} X
    \right\ket \\
    &= \sp{Y_{\perp X}}{Z_{\perp X}},
  \end{align*}
  where the subscript $\perp X$ denotes the projection onto the subspace
  orthogonal to~$X$.
\end{proof}


From the point of view of this inner product space, the first two rounds of
communication in Example~\ref{ex:gaussfb} can be illustrated as in
Figure~\ref{fig:gaussfb1}.
\begin{figure}[tbph]
  \begin{center}
    \texttt{fig:gaussfb1}
  \end{center}
  \caption{Geometrical interpretation of the first two rounds of
  Example~\ref{ex:gaussfb}. The estimate $\Eh_0$ of the first transmission is
  the \emph{projection} of $S$ onto $Y_1$, hence the estimation error $E_1$ is
  orthogonal to~$Y_1$.}
  \label{fig:gaussfb1}
\end{figure}
The figure shows that while $X_1$ and $X_2$ are not independent, $X_2$ is
independent of $Y_1$. Since all future rounds of communication involve only
linear combinations of~$E_1$ and the noise components $Z_2$, $Z_3$, \dots, all
future communication takes place in a subspace orthogonal to~$Y_1$. For
reference, \figref{gaussfb2} shows the next round of communication.
\begin{figure}[tbph]
  \begin{center}
    \texttt{fig:gaussfb2}
  \end{center}
  \caption{Second and third round of communication of \exref{gaussfb}.}
  \label{fig:gaussfb2}
\end{figure}

Since the estimate $\Eh_i$ is the projection of $E_i$ onto $Y_{i+1}$, 
\begin{equation*}
  \Eh_i = \frac{\sp{E_i}{Y_{i+1}}}{\|Y_{i+1}\|^2} Y_{i+1}.
\end{equation*}
Writing $Y_{i+1} = \alpha E_i + Z_{i+1}$, with $\alpha = \sqrt{P}/\|E_i\|$,
yields
\begin{equation*}
  \|E_{i+1}\|^2 = \|\Eh_i - E_i\|^2 = \frac{\|E_i\|^2}{1 + P/\|Z_{i+1}\|^2},
\end{equation*}
which is nothing else than Equation~\ref{eq:gaussvardec} expressed in the inner
product space formalism.


\section{Towards the Feedbackless Case}

Example~\ref{ex:gaussfb} showed that using feedback, the theoretically optimal
distortion when one source symbol is transmitted in $n$~channel uses can be
achieved exactly with a simple transmission scheme that encodes a single source
symbol at a time. The goal of the present section is to draw insights from this
example that can help in the case without feedback. 

In the first round of Example~\ref{ex:gaussfb} the source was
transmitted uncoded and the receiver computed the MMSE estimator~$\Eh_0$. In
each subsequent round, the error $E_i = \Eh_{i-1} - E_{i-1}$ was transmitted
uncoded and again estimated at the receiver using MMSE. Let us now express the
source as
\begin{equation*}
  S = S + E_1 - E_1 - E_2 + E_2 + \cdots \pm E_{n-1} \mp E_{n-1}.
\end{equation*}
Using the definition of $E_i$ repeatedly, this can be written as
\begin{equation*}
  S = \Eh_0 - \Eh_1 + \Eh_2 - \cdots \pm \Eh_{n-1} \mp E_{n-1}.
\end{equation*}
Since each $\Eh_i$ is a linear function of~$Y_i$ and the $Y_i$ are mutually
independent, this transmission scheme implicitly decomposes $S$ into
$n$ independent components. Moreover, the first $n-1$ components are known
exactly by the decoder, so the only error is due to the last transmission.

In the absence of feedback, a possible strategy is therefore to do the
following:
\begin{enumerate}
  \item Break up the source into the sum of $n$ (approximately) independent
    components.
  \item Transmit these components such that the first $n-1$ components are
    decoded (approximately) error free and transmit the last component uncoded.
\end{enumerate}

The investigation and analysis of strategies based on this principle is the
subject of the next chapter. 

